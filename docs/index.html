<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>qpcr API documentation</title>
<meta name="description" content="This module is designed to provide functions to analyse qPCR data.
It is designed for maximal user-friendliness and streamlined data-visualisation â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>qpcr</code></h1>
</header>
<section id="section-intro">
<p>This module is designed to provide functions to analyse qPCR data.
It is designed for maximal user-friendliness and streamlined data-visualisation.</p>
<h2 id="terminology">Terminology</h2>
<hr>
<p>Let's talk some terminology first. There are a number of important terms that you will meet when using the <code><a title="qpcr" href="#qpcr">qpcr</a></code> module that you may feel have nothing to do with qPCR.
Read on to learn what these terms mean and why they are important. </p>
<h3 id="qpcr-vs-qpcr"><code><a title="qpcr" href="#qpcr">qpcr</a></code> vs qPCR</h3>
<p>Throughout this documentation, we will refer to the python module as <code><a title="qpcr" href="#qpcr">qpcr</a></code> in all lowercase, while we will talk about the qPCR experiment itself
by using <code>qPCR</code> with uppercase. </p>
<h3 id="file-or-datafile">File (or "datafile")</h3>
<p>Let's start simple. A <code>file</code> is simply one of your input datafiles (shocker). For the <code><a title="qpcr" href="#qpcr">qpcr</a></code> module this means either a <code>csv</code> or an <code>excel</code> file.
Files are the at the very basis of our data pipeline.
However, <code><a title="qpcr" href="#qpcr">qpcr</a></code> does not strictly require that these files correspond to anything in particular from "qPCR vocabulary" <em>per se</em>
(although the basic assumption underlying the default settings
is that a single "regular" datafile contains exactly one single qPCR assay). </p>
<p>The main <code><a title="qpcr" href="#qpcr">qpcr</a></code> classes are designed to work with datafiles that contain Ct values and replicate identifiers (see below for "replicates").
In the very most basic case, a simple datafile contains extactly an <code>"id"</code> or <code>"Name"</code> or whatever column and a <code>"Ct"</code> column (can also be named differently),
that store values from exactly <em>one single qPCR assay</em>. </p>
<p>However, if your experimental setup looks differently, there are ways to adapt <code><a title="qpcr" href="#qpcr">qpcr</a></code> to handle different setups.
But if your data follows the above mentioned standard arrangements, you can think of a file as identical to a "qPCR assay".</p>
<h3 id="regular-vs-irregular-datafiles">"Regular" vs "irregular" datafiles</h3>
<p><code>Regular</code> datafiles only contain the <code>id</code> and <code>Ct</code> columns and nothing else, storing values of exactly one qPCR assay.
<code>Irregular</code> datafiles also have to have these two columns, but they allow for more irregular stuff
around these columns.
Irregular files may contain multiple datasets / assays (see below). Check out the documentation of the <code><a title="qpcr.Parsers" href="Parsers/index.html">qpcr.Parsers</a></code> and <code><a title="qpcr.Readers" href="Readers/index.html">qpcr.Readers</a></code> for
details on how to work with irregular and multi-assay datafiles.</p>
<h3 id="an-assay-and-a-dataset-assay">An <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> and a "dataset" / "assay"</h3>
<p>Let's start by the term "dataset". Well, a "dataset" is simply a collection or replicate identifiers and corresponding Ct values belonging together.
By default this usually corresponds to a single qPCR assay, hence you will often encounter the term "assay" used instead of "dataset" to be more
intuitive. However, the two terms are really interchangeable. The <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> class is used to store the Ct values and replicate identifiers
from a single dataset extracted from a datafile. </p>
<h3 id="replicates">Replicates</h3>
<p>As far as the <code><a title="qpcr" href="#qpcr">qpcr</a></code> module is concerned, each row within your datasets corresponds to one replicate.
Hence, a <em>replicate</em> is just a single pair of some identifier and a corresponding Ct value.
That means that <code><a title="qpcr" href="#qpcr">qpcr</a></code> does not terminologically distinguish between multiplets (i.e. "true replicates" as an experimentor would understand them) and unicates.
If you feel more comfortable with a term like "measurement" then you can also think of the replicates like that. </p>
<h3 id="groups-of-replicates">Groups (of Replicates)</h3>
<p>This is one of the most important terms. In the previous paragraphs we already started to talk about replicates "belonging together".
Groups of replicates are, as their name already implies, well, a number of replicates that somehow do belong together.
For most cases, if your datafiles contain one assay each, then the groups of replicates are most likely your different qPCR samples / experimental conditions.
We talk about <em>groups</em> of replicates instead of samples or conditions, primarily, because there might be different data setups so that these terms might not be always appropriate.
If your data follows default arrangements, however, then a <em>group of replicates</em> is just what you would think of as a "qPCR sample".
Groups are assigned a numeric index starting from 0, which is how they are identified by the classes.
However, they also come with a text label called the <code>group_name</code> (you can manually set and re-set the group names as you like).
Many classes such as the <code><a title="qpcr.DataReader" href="#qpcr.DataReader">DataReader</a></code> will actually just use the term <code>names</code> instead of the full <code>group_names</code>.
Whenever you see anything "names"-related it is (super-duper most likely) a reference to the <code>group_names</code>.</p>
<h3 id="specifying-groups-of-replicates">Specifying (Groups of) Replicates</h3>
<p>Here's the best part: usually, we don't necessarily need to do anything because <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> objects are able to infer the groups of replicates in your data
automatically from the replicate identifiers (yeah!). However, you will be asked to manually provide replicate settings in case this fails.
In case you want to / have to manually specify replicate settings, an <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> accepts an input <code>replicates</code> which is where you can specify this information. </p>
<p><code>replicates</code> can be either an <code>integer</code>, a <code>tuple</code>, or a <code>string</code>. Why's that? Well, normally we perform experiments as "triplicates", or "duplicates", or whatever multiplets.
Hence, if we always have the same number of replicates in each group (say all triplicates) we can simply specify this number as <code>replicates = 3</code>.
However, some samples might only be done in unicates (such as the diluent sample), while others are triplicates.</p>
<p>In these cases your dataset does not have uniformly sized groups of replicates and a single number will not do to describe the groups of replicates.
For these cases you can specify the number of replicates in each group separately as a <code>tuple</code> such as <code>replicates = (3,3,3,3,1)</code> or as a <code>string</code> "formula"
which allows you to avoid repeating the same number of replicates many times like <code>replicates = "3:4,1"</code>, which will translate into the same tuple as we specified manually.
Check out the documentation of <code><a title="qpcr.Assay.replicates" href="#qpcr.Assay.replicates">Assay.replicates()</a></code> for more information on this. </p>
<h3 id="delta-ct-vs-delta-delta-ct-vs-normalisation"><code>Delta-Ct</code> vs <code>Delta-Delta-Ct</code> vs <code>normalisation</code></h3>
<p>The default analysis workflow in $\Delta \Delta Ct$ analysis is to first calculate a $\Delta Ct$ using an intra-assay reference and then calculate the $\Delta \Delta Ct$ using a normaliser assay.
The first $\Delta Ct$ step is performed by a class called <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code> using its native method <code><a title="qpcr.Analyser.DeltaCt" href="#qpcr.Analyser.DeltaCt">Analyser.DeltaCt()</a></code>.
Why just <code>DeltaCt</code> and not <code>DeltaDeltaCt</code>? Well, we call this second <code>Delta</code>-step in <code>DeltaDeltaCt</code> differently.
We name it <code>normalisation</code>, and it is handled by a class called <code><a title="qpcr.Normaliser" href="#qpcr.Normaliser">Normaliser</a></code> using its native method <code><a title="qpcr.Normaliser.normalise" href="#qpcr.Normaliser.normalise">Normaliser.normalise()</a></code>.
So, as far as the <code><a title="qpcr" href="#qpcr">qpcr</a></code> module is concerned there is only <code><a title="qpcr.Analyser.DeltaCt" href="#qpcr.Analyser.DeltaCt">Analyser.DeltaCt()</a></code> which performs the first $\Delta Ct$, and <code><a title="qpcr.Normaliser.normalise" href="#qpcr.Normaliser.normalise">Normaliser.normalise()</a></code> which later handles the second "delta"-step to get to $\Delta \Delta Ct$.
Of course, this means that the <code><a title="qpcr.Normaliser" href="#qpcr.Normaliser">Normaliser</a></code> will need to have knowledge about which <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> objects contain actual assays-of-interest and which ones contain normaliser-assays (specifying that is easy, though, so don't worry about that).</p>
<h3 id="the-anchor-and-the-reference-group">The <code>anchor</code> and the "reference group"</h3>
<p>Next to the "groups of replicates", this is probably one of the most important terms. The <code>anchor</code> is simply the intra-dataset reference used by the <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code> to perform its first $\Delta Ct$.
If your datafiles contain one assay each, and your groups of replicates are your qPCR samples, then you will likely have some "wildtype", "untreated", or "control" sample, right?
Well, in <code><a title="qpcr" href="#qpcr">qpcr</a></code> terms that would be your <em>reference group</em>.
Usually your <code>anchor</code> is part of or generated from the Ct values of your <em>reference group</em> (like their <code>mean</code> for instance).
By default it is assumed that your reference group is the <em>very first</em> group of replicates. However, it's not a big problem if this is not the case, as you can specify different anchors easily.
So, again, the <code>anchor</code> is the dataset-internal reference value used for the first $\Delta Ct$.</p>
<h3 id="assays-vs-normalisers">"assays" vs "normalisers"</h3>
<p>You will likely encounter methods and/or arguments that speak of "assays" and "normalisers", especially with the <code><a title="qpcr.Normaliser" href="#qpcr.Normaliser">Normaliser</a></code>.
For all intents and purposes, an "assay" is simply one of your datasets (we know this already).
However, in practice "assays" are the short notation for specifically "assays-of-interest"
(or more formally "datasets-of-interest"), while "normalisers" refer to your normaliser-assays (from housekeeping genes like ActinB for instance).
But again, if your datafiles do not conform to standard data arrangements, do not be distracted from the terminology here.</p>
<p>You will also find that the term "assays" is used within the final results dataframe (when using the summary-statistics mode).
In this setting "assays" refers to the assay-of-interst whose data was analysed according to the provided normaliser-assays.
In fact, this is a new "hybrid" assay identifier taht includes the names of all the normaliser-assays used during computation (check out what the final results look like and it'll be immediately clear).</p>
<h3 id="samples">"Samples"</h3>
<p>You may find that there is also a term "sample" within <code><a title="qpcr" href="#qpcr">qpcr</a></code>'s vocabulary.
As far as the <code><a title="qpcr" href="#qpcr">qpcr</a></code> module is concerned, the term "sample" is not very important in itself and usually appears in the context of "sample assays".
In this setting it is used interchangeably with "assays-of-interest".
Actually, we try to phase out the term "sample" and it currently mainly appears in hidden auxiliary functions which have retained the term from earlier development versions.</p>
<h2 id="some-more-basics">Some more basics</h2>
<hr>
<h4 id="the-values-stored-by-qpcr">The values stored by <code><a title="qpcr" href="#qpcr">qpcr</a></code></h4>
<p>Please, note at this point that, as described in more detail in the documentation of the <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code>, Delta-Ct values are directly computed as
exponential values $ \Delta Ct' = 2^{ - \Delta Ct}$, while normalisation later performs $ \mathrm{norm. } \Delta\Delta Ct = \frac{
\Delta Ct'_s
}{
\Delta Ct'_n
}$, where $s$ is an assay of interest's Delta-Ct ($\Delta Ct'$) value of some replicate,
and $n$ is the corresponding value of the normaliser assay. This is based on the mathemathical equivalence of $n^{
a - b
} \equiv \frac{
n^{ a } } {
n^{ b } }$.
Hence, while the documentation will continuously use the terms $\Delta Ct$ and $\Delta\Delta Ct$, they are in fact the exponential deriviative of the conventional values.</p>
<h4 id="modes-of-normalisation">Modes of <code>normalisation</code></h4>
<p>The <code><a title="qpcr.Normaliser" href="#qpcr.Normaliser">Normaliser</a></code> supports custom functions for normalisation. However, it also comes with three built-in methods to normalise sample-assays against normalisers.
These are accessible via the <code>mode</code> argument of the <code><a title="qpcr.Normaliser.normalise" href="#qpcr.Normaliser.normalise">Normaliser.normalise()</a></code> method, which can be set to <code>"pair-wise"</code> (default), <code>"combinatoric"</code>, or <code>"permutative"</code>.
The default option <code>"pair-wise"</code> is computationally the fastest and will rigidly normalise replicates against their corresponding partner from the normaliser. I.e. first against first,
second against second, etc. This mode is appropriate for multiplex qPCR experiments. For qPCR reactions that were pipetted individually, there is not reason to strictly only pair first
with first, second with second etc. For these cases there are other two options <code>"combinatoric"</code> and <code>"permutative"</code>. <code>"combinatoric"</code> normalisation will calculate all possible group-wise
combinations of a sample-assay replicate with all available normaliser replicates of the same group. I.e. first against first, and against second, etc. This will generate $n^2$ values where
$n$ is the number of replicates within a group. This mode is appropriate for small-scale datasets but will substantially increase computation times for larger datasets. <br>
<code>"permutative"</code> on the other hand will reflect the equivalence of replicates within a group through random permutations wihtin the normaliser replicates. Hence,
first may be normalised against first, or second, etc. This normalisation method may by used iteratively to increase the accuracy. Replacement during permutations are allowed (although disabled by default).
If replacement is desired by the user, the probability of each replicate to be chosen will be weighted based on a fitted normal distribution. This method is appropriate for larger datasets for which combinatoric normalisation
is not desired. </p>
<h3 id="pipelines"><code>pipeline</code>s</h3>
<p>A <code>pipeline</code> is essentially any workflow that starts from one or multiple input datafiles and ultimately pops out some results table you are happy with.
Pipelines can be manually created by assembling the main <code><a title="qpcr" href="#qpcr">qpcr</a></code> classes, usually starting with a Reader, passing to an Analyser, to an Normaliser, and you're good to go.
When manually assembling your workflow you can extract your data at any point and perform your own computations on it as you like. However, if you wish to "just do some good ol' Delta-Delta-Ct"
there are pre-defined pipelines that will handle writing the workflow and only require a very basic setup. You can find these in the <code><a title="qpcr.Pipes" href="Pipes/index.html">qpcr.Pipes</a></code> submodule.</p>
<h3 id="results-my-final-results">"Results" = my final results?</h3>
<p>Yes and no. Anything that is computed through any of the <code><a title="qpcr" href="#qpcr">qpcr</a></code> classes is called a "result" of some kind.
In practice as soon as you pass your data through a <code><a title="qpcr.Normaliser" href="#qpcr.Normaliser">Normaliser</a></code> you generate "results" which are actually stored in a separate class called <code><a title="qpcr.Results" href="#qpcr.Results">Results</a></code> (that's not so important, though).
So, when using the term "result" we usually mean just anything that was explicitly computed by <code><a title="qpcr" href="#qpcr">qpcr</a></code>. </p>
<h3 id="getting-your-data"><code>get</code>ting your data</h3>
<p>Too many classes and objects? Well, no worries, the underlying data is stored as <code>pandas DataFrames</code>. To get your data from the clutches of the <code><a title="qpcr" href="#qpcr">qpcr</a></code> classes you can always use the <code>get()</code> method.
<code>get</code> is pretty universal in the <code><a title="qpcr" href="#qpcr">qpcr</a></code> module, so whenever you want to extract your data, there's a <code>get()</code> method to help you.</p>
<h3 id="link-vs-add-vs-pipe"><code>link</code> vs <code>add</code> vs <code>pipe</code></h3>
<p>Different classes have slightly different methods of adding data to them. Classes that only accept one single data input (such as a single <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object or a single filepath as <code>string</code>)
usually have a <code>link()</code> method that, well, links the data to them. After that the classes are ready to perform whatever actions they can perform (an <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code> would perform <code>DeltaCt()</code> for instance).
Some classes such as the <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code> have a wrapper that will call both their <code>link()</code> as well as their actual core-functional method together in one go. This wrapper is called <code>pipe()</code>.
So for the <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code> you could either manually use <code>link()</code> and then <code>DeltaCt()</code>, or simply call <code>pipe()</code> which does both for you. It is noteworthy that <code>pipe</code> methods actually <em>return</em> whatever
their output is, which is <em>not</em> normally the case otherwise (normally you'd use the <code>get()</code> method to extract your data, see above). Most <code><a title="qpcr.Readers" href="Readers/index.html">qpcr.Readers</a></code> and <code><a title="qpcr.Parsers" href="Parsers/index.html">qpcr.Parsers</a></code> are also equipped with <code>pipe</code> methods.
Alright, we know about <code>link</code> and <code>pipe</code> now, what about <code>add</code>?
Classes that accept multiple inputs have <code>add</code> methods, which tells the class where exactly to store the input data.
<code>add</code>-methods are especially implemented within the pre-defined analysis pipelines of the <code><a title="qpcr.Pipes" href="Pipes/index.html">qpcr.Pipes</a></code> submodule. You will probably often use the methods <code>add_assays()</code> and <code>add_normalisers()</code> if you plan on using these predefined pipelines.
However, these classes usually still have a <code>link()</code> method somewhere that you can use as well. For instance, the <code><a title="qpcr.Normaliser" href="#qpcr.Normaliser">Normaliser</a></code> uses a <code>link</code> method to add both assays-of-interest and normaliser-assays simultaneously.</p>
<h2 id="getting-started">Getting started</h2>
<hr>
<p>You can find useful tutorials and applied examples as <code>jupyter notebooks</code> <a href="https://github.com/NoahHenrikKleinschmidt/qpcr/tree/main/Examples">on GitHub</a>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This module is designed to provide functions to analyse qPCR data. 
It is designed for maximal user-friendliness and streamlined data-visualisation.

## Terminology
---

Let&#39;s talk some terminology first. There are a number of important terms that you will meet when using the `qpcr` module that you may feel have nothing to do with qPCR. 
Read on to learn what these terms mean and why they are important. 

### `qpcr` vs qPCR
Throughout this documentation, we will refer to the python module as `qpcr` in all lowercase, while we will talk about the qPCR experiment itself 
by using `qPCR` with uppercase. 

### File (or &#34;datafile&#34;)
Let&#39;s start simple. A `file` is simply one of your input datafiles (shocker). For the `qpcr` module this means either a `csv` or an `excel` file. 
Files are the at the very basis of our data pipeline. 
However, `qpcr` does not strictly require that these files correspond to anything in particular from &#34;qPCR vocabulary&#34; _per se_ 
(although the basic assumption underlying the default settings
is that a single &#34;regular&#34; datafile contains exactly one single qPCR assay). 

The main `qpcr` classes are designed to work with datafiles that contain Ct values and replicate identifiers (see below for &#34;replicates&#34;).
In the very most basic case, a simple datafile contains extactly an `&#34;id&#34;` or `&#34;Name&#34;` or whatever column and a `&#34;Ct&#34;` column (can also be named differently),
that store values from exactly _one single qPCR assay_. 

However, if your experimental setup looks differently, there are ways to adapt `qpcr` to handle different setups.
But if your data follows the above mentioned standard arrangements, you can think of a file as identical to a &#34;qPCR assay&#34;.

### &#34;Regular&#34; vs &#34;irregular&#34; datafiles
`Regular` datafiles only contain the `id` and `Ct` columns and nothing else, storing values of exactly one qPCR assay. 
`Irregular` datafiles also have to have these two columns, but they allow for more irregular stuff
around these columns. 
Irregular files may contain multiple datasets / assays (see below). Check out the documentation of the `qpcr.Parsers` and `qpcr.Readers` for 
details on how to work with irregular and multi-assay datafiles.

### An `qpcr.Assay` and a &#34;dataset&#34; / &#34;assay&#34;
Let&#39;s start by the term &#34;dataset&#34;. Well, a &#34;dataset&#34; is simply a collection or replicate identifiers and corresponding Ct values belonging together. 
By default this usually corresponds to a single qPCR assay, hence you will often encounter the term &#34;assay&#34; used instead of &#34;dataset&#34; to be more 
intuitive. However, the two terms are really interchangeable. The `qpcr.Assay` class is used to store the Ct values and replicate identifiers 
from a single dataset extracted from a datafile. 

### Replicates
As far as the `qpcr` module is concerned, each row within your datasets corresponds to one replicate. 
Hence, a _replicate_ is just a single pair of some identifier and a corresponding Ct value. 
That means that `qpcr` does not terminologically distinguish between multiplets (i.e. &#34;true replicates&#34; as an experimentor would understand them) and unicates. 
If you feel more comfortable with a term like &#34;measurement&#34; then you can also think of the replicates like that. 

### Groups (of Replicates)
This is one of the most important terms. In the previous paragraphs we already started to talk about replicates &#34;belonging together&#34;. 
Groups of replicates are, as their name already implies, well, a number of replicates that somehow do belong together.
For most cases, if your datafiles contain one assay each, then the groups of replicates are most likely your different qPCR samples / experimental conditions. 
We talk about _groups_ of replicates instead of samples or conditions, primarily, because there might be different data setups so that these terms might not be always appropriate.
If your data follows default arrangements, however, then a _group of replicates_ is just what you would think of as a &#34;qPCR sample&#34;. 
Groups are assigned a numeric index starting from 0, which is how they are identified by the classes. 
However, they also come with a text label called the `group_name` (you can manually set and re-set the group names as you like). 
Many classes such as the `qpcr.DataReader` will actually just use the term `names` instead of the full `group_names`. 
Whenever you see anything &#34;names&#34;-related it is (super-duper most likely) a reference to the `group_names`.

### Specifying (Groups of) Replicates
Here&#39;s the best part: usually, we don&#39;t necessarily need to do anything because `qpcr.Assay` objects are able to infer the groups of replicates in your data 
automatically from the replicate identifiers (yeah!). However, you will be asked to manually provide replicate settings in case this fails. 
In case you want to / have to manually specify replicate settings, an `qpcr.Assay` accepts an input `replicates` which is where you can specify this information. 

`replicates` can be either an `integer`, a `tuple`, or a `string`. Why&#39;s that? Well, normally we perform experiments as &#34;triplicates&#34;, or &#34;duplicates&#34;, or whatever multiplets.
Hence, if we always have the same number of replicates in each group (say all triplicates) we can simply specify this number as `replicates = 3`. 
However, some samples might only be done in unicates (such as the diluent sample), while others are triplicates.

In these cases your dataset does not have uniformly sized groups of replicates and a single number will not do to describe the groups of replicates. 
For these cases you can specify the number of replicates in each group separately as a `tuple` such as `replicates = (3,3,3,3,1)` or as a `string` &#34;formula&#34;
which allows you to avoid repeating the same number of replicates many times like `replicates = &#34;3:4,1&#34;`, which will translate into the same tuple as we specified manually. 
Check out the documentation of `qpcr.Assay.replicates` for more information on this. 

### `Delta-Ct` vs `Delta-Delta-Ct` vs `normalisation`
The default analysis workflow in $\Delta \Delta Ct$ analysis is to first calculate a $\Delta Ct$ using an intra-assay reference and then calculate the $\Delta \Delta Ct$ using a normaliser assay. 
The first $\Delta Ct$ step is performed by a class called `qpcr.Analyser` using its native method `qpcr.Analyser.DeltaCt`. 
Why just `DeltaCt` and not `DeltaDeltaCt`? Well, we call this second `Delta`-step in `DeltaDeltaCt` differently. 
We name it `normalisation`, and it is handled by a class called `qpcr.Normaliser` using its native method `qpcr.Normaliser.normalise`. 
So, as far as the `qpcr` module is concerned there is only `qpcr.Analyser.DeltaCt` which performs the first $\Delta Ct$, and `qpcr.Normaliser.normalise` which later handles the second &#34;delta&#34;-step to get to $\Delta \Delta Ct$. 
Of course, this means that the `qpcr.Normaliser` will need to have knowledge about which `qpcr.Assay` objects contain actual assays-of-interest and which ones contain normaliser-assays (specifying that is easy, though, so don&#39;t worry about that).


### The `anchor` and the &#34;reference group&#34;
Next to the &#34;groups of replicates&#34;, this is probably one of the most important terms. The `anchor` is simply the intra-dataset reference used by the `qpcr.Analyser` to perform its first $\Delta Ct$. 
If your datafiles contain one assay each, and your groups of replicates are your qPCR samples, then you will likely have some &#34;wildtype&#34;, &#34;untreated&#34;, or &#34;control&#34; sample, right? 
Well, in `qpcr` terms that would be your _reference group_.
Usually your `anchor` is part of or generated from the Ct values of your _reference group_ (like their `mean` for instance).
By default it is assumed that your reference group is the _very first_ group of replicates. However, it&#39;s not a big problem if this is not the case, as you can specify different anchors easily.
So, again, the `anchor` is the dataset-internal reference value used for the first $\Delta Ct$.

### &#34;assays&#34; vs &#34;normalisers&#34;
You will likely encounter methods and/or arguments that speak of &#34;assays&#34; and &#34;normalisers&#34;, especially with the `qpcr.Normaliser`. 
For all intents and purposes, an &#34;assay&#34; is simply one of your datasets (we know this already).
However, in practice &#34;assays&#34; are the short notation for specifically &#34;assays-of-interest&#34; 
(or more formally &#34;datasets-of-interest&#34;), while &#34;normalisers&#34; refer to your normaliser-assays (from housekeeping genes like ActinB for instance). 
But again, if your datafiles do not conform to standard data arrangements, do not be distracted from the terminology here.

You will also find that the term &#34;assays&#34; is used within the final results dataframe (when using the summary-statistics mode). 
In this setting &#34;assays&#34; refers to the assay-of-interst whose data was analysed according to the provided normaliser-assays. 
In fact, this is a new &#34;hybrid&#34; assay identifier taht includes the names of all the normaliser-assays used during computation (check out what the final results look like and it&#39;ll be immediately clear).

### &#34;Samples&#34;
You may find that there is also a term &#34;sample&#34; within `qpcr`&#39;s vocabulary. 
As far as the `qpcr` module is concerned, the term &#34;sample&#34; is not very important in itself and usually appears in the context of &#34;sample assays&#34;.
In this setting it is used interchangeably with &#34;assays-of-interest&#34;. 
Actually, we try to phase out the term &#34;sample&#34; and it currently mainly appears in hidden auxiliary functions which have retained the term from earlier development versions.


## Some more basics 
----

#### The values stored by `qpcr`
Please, note at this point that, as described in more detail in the documentation of the `qpcr.Analyser`, Delta-Ct values are directly computed as 
exponential values $ \Delta Ct&#39; = 2^{ - \Delta Ct}$, while normalisation later performs $ \mathrm{norm. } \Delta\Delta Ct = \\frac{  \Delta Ct&#39;_s  }{  \Delta Ct&#39;_n  }$, where $s$ is an assay of interest&#39;s Delta-Ct ($\Delta Ct&#39;$) value of some replicate, 
and $n$ is the corresponding value of the normaliser assay. This is based on the mathemathical equivalence of $n^{  a - b  } \equiv \\frac{  n^{ a } } {  n^{ b } }$. 
Hence, while the documentation will continuously use the terms $\Delta Ct$ and $\Delta\Delta Ct$, they are in fact the exponential deriviative of the conventional values.

#### Modes of `normalisation`
The `qpcr.Normaliser` supports custom functions for normalisation. However, it also comes with three built-in methods to normalise sample-assays against normalisers.
These are accessible via the `mode` argument of the `qpcr.Normaliser.normalise` method, which can be set to `&#34;pair-wise&#34;` (default), `&#34;combinatoric&#34;`, or `&#34;permutative&#34;`. 
The default option `&#34;pair-wise&#34;` is computationally the fastest and will rigidly normalise replicates against their corresponding partner from the normaliser. I.e. first against first, 
second against second, etc. This mode is appropriate for multiplex qPCR experiments. For qPCR reactions that were pipetted individually, there is not reason to strictly only pair first
with first, second with second etc. For these cases there are other two options `&#34;combinatoric&#34;` and `&#34;permutative&#34;`. `&#34;combinatoric&#34;` normalisation will calculate all possible group-wise 
combinations of a sample-assay replicate with all available normaliser replicates of the same group. I.e. first against first, and against second, etc. This will generate $n^2$ values where 
$n$ is the number of replicates within a group. This mode is appropriate for small-scale datasets but will substantially increase computation times for larger datasets.   
`&#34;permutative&#34;` on the other hand will reflect the equivalence of replicates within a group through random permutations wihtin the normaliser replicates. Hence, 
first may be normalised against first, or second, etc. This normalisation method may by used iteratively to increase the accuracy. Replacement during permutations are allowed (although disabled by default).
If replacement is desired by the user, the probability of each replicate to be chosen will be weighted based on a fitted normal distribution. This method is appropriate for larger datasets for which combinatoric normalisation
is not desired. 

### `pipeline`s 
A `pipeline` is essentially any workflow that starts from one or multiple input datafiles and ultimately pops out some results table you are happy with.
Pipelines can be manually created by assembling the main `qpcr` classes, usually starting with a Reader, passing to an Analyser, to an Normaliser, and you&#39;re good to go.
When manually assembling your workflow you can extract your data at any point and perform your own computations on it as you like. However, if you wish to &#34;just do some good ol&#39; Delta-Delta-Ct&#34;
there are pre-defined pipelines that will handle writing the workflow and only require a very basic setup. You can find these in the `qpcr.Pipes` submodule.

### &#34;Results&#34; = my final results?
Yes and no. Anything that is computed through any of the `qpcr` classes is called a &#34;result&#34; of some kind.
In practice as soon as you pass your data through a `qpcr.Normaliser` you generate &#34;results&#34; which are actually stored in a separate class called `qpcr.Results` (that&#39;s not so important, though).
So, when using the term &#34;result&#34; we usually mean just anything that was explicitly computed by `qpcr`. 

### `get`ting your data
Too many classes and objects? Well, no worries, the underlying data is stored as `pandas DataFrames`. To get your data from the clutches of the `qpcr` classes you can always use the `get()` method. 
`get` is pretty universal in the `qpcr` module, so whenever you want to extract your data, there&#39;s a `get()` method to help you.

### `link` vs `add` vs `pipe`
Different classes have slightly different methods of adding data to them. Classes that only accept one single data input (such as a single `qpcr.Assay` object or a single filepath as `string`)
usually have a `link()` method that, well, links the data to them. After that the classes are ready to perform whatever actions they can perform (an `qpcr.Analyser` would perform `DeltaCt()` for instance). 
Some classes such as the `qpcr.Analyser` have a wrapper that will call both their `link()` as well as their actual core-functional method together in one go. This wrapper is called `pipe()`. 
So for the `qpcr.Analyser` you could either manually use `link()` and then `DeltaCt()`, or simply call `pipe()` which does both for you. It is noteworthy that `pipe` methods actually _return_ whatever
their output is, which is *not* normally the case otherwise (normally you&#39;d use the `get()` method to extract your data, see above). Most `qpcr.Readers` and `qpcr.Parsers` are also equipped with `pipe` methods.
Alright, we know about `link` and `pipe` now, what about `add`?  Classes that accept multiple inputs have `add` methods, which tells the class where exactly to store the input data. 
`add`-methods are especially implemented within the pre-defined analysis pipelines of the `qpcr.Pipes` submodule. You will probably often use the methods `add_assays()` and `add_normalisers()` if you plan on using these predefined pipelines.
However, these classes usually still have a `link()` method somewhere that you can use as well. For instance, the `qpcr.Normaliser` uses a `link` method to add both assays-of-interest and normaliser-assays simultaneously.

## Getting started
----
You can find useful tutorials and applied examples as `jupyter notebooks` [on GitHub](https://github.com/NoahHenrikKleinschmidt/qpcr/tree/main/Examples).

&#34;&#34;&#34;

import pandas as pd
import qpcr._auxiliary as aux
from qpcr._auxiliary import warnings as aw
import qpcr._auxiliary.defaults as defaults
import qpcr.Parsers as Parsers
import qpcr.Readers as Readers
import qpcr.Plotters as Plotters
import os
import numpy as np 
import scipy.stats as stats
from copy import deepcopy 
from io import StringIO
import re
import json

__pdoc__ = {
    &#34;_CORE_Reader&#34; : True
}

# default column names for raw Ct data files (don&#39;t change this!)
raw_col_names = defaults.raw_col_names

supported_filetypes = defaults.supported_filetypes

default_group_name = defaults.default_group_name

# the default columns that are structurally part of an Assay object
ref_cols = [ raw_col_names[0], &#34;group&#34;, &#34;group_name&#34;, defaults.default_dataset_header ]


# At some future point we will remove the _CORE_Reader from the 
# __init__ as it&#39;s now part of the Readers submodule...
class _CORE_Reader(aux._ID):
    &#34;&#34;&#34;
    The class handling the core functions of the Reader class. 
    The standard qpcr.Reader inherits from this. 

    Note
    ----
    This implementation is deprecated. The _CORE_Reader has been moved
    (and further updated since) to the `qpcr.Readers` submodule. 

    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._src = None
        self._delimiter = None
        self._df = None

    def get(self):
        &#34;&#34;&#34;
        Returns
        -------
        data : pd.DataFrame
            The dataframe from the datafile.
        &#34;&#34;&#34;
        return self._df

    def n(self):
        &#34;&#34;&#34;
        Returns
        -------
        n : int
            The number of replicates (entries) in the dataframe.
        &#34;&#34;&#34;
        return len(self._df[raw_col_names[0]])


    def read(self, **kwargs):
        &#34;&#34;&#34;
        Reads the given data file.

        If the data file is an Excel file replicates and their Ct values will be 
        extracted from the first excel sheet of the file. Note, this assumes by default
        that the replicates are headed by the label `&#34;Name&#34;` and the corresponding Ct values
        are headed by the label `&#34;Ct&#34;`. Both labels have to be on the same row. 

        If these labels do not match your excel file, you may
        specify `name_label` and `Ct_label` as additional arguments.
        &#34;&#34;&#34;
        suffix = self._filesuffix()
        if suffix == &#34;csv&#34;:
            try: 
                self._csv_read()
            except:
                # setup parser
                parser = Parsers.CsvParser()
                # check it file should be read transposed
                transpose = aux.from_kwargs(&#34;transpose&#34;, False, kwargs, rm = True)
                if transpose:
                    parser.transpose()
                
                # setup patterns and store assay-of-interest
                assay_pattern = aux.from_kwargs(&#34;assay_pattern&#34;, &#34;Rotor-Gene&#34;, kwargs)
                assay_of_interest = aux.from_kwargs(&#34;assay&#34;, None, kwargs, rm=True)
                parser.assay_pattern(assay_pattern)
                
                # get data column labels
                id_label = aux.from_kwargs(&#34;id_label&#34;, &#34;Name&#34;, kwargs, rm = True)
                ct_label = aux.from_kwargs(&#34;ct_label&#34;, &#34;Ct&#34;, kwargs, rm = True)
                parser.labels(id_label,ct_label)
                
                # pipe the datafile through the parser
                parser.pipe(self._src, **kwargs)

                if len(parser.assays()) &gt; 1:
                    if assay_of_interest is None: 
                        aw.HardWarning(&#34;Reader:cannot_read_multifile&#34;, file = self._src, assays = parser.assays())
                    self._df = parser.get(assay_of_interest)
                    self.id(assay_of_interest)
                else:
                    assay_of_interest = parser.assays()[0]
                    self._df = parser.get(assay_of_interest)
                    self.id(assay_of_interest)

        elif suffix == &#34;xlsx&#34;:
            # setup parser
            parser = Parsers.ExcelParser()
            # check it file should be read transposed
            transpose = aux.from_kwargs(&#34;transpose&#34;, False, kwargs, rm = True)
            if transpose:
                parser.transpose()
            
            # check for sheet_name
            sheet_name = aux.from_kwargs(&#34;sheet_name&#34;, 0, kwargs, rm = True)

            # setup patterns and store assay-of-interest
            assay_pattern = aux.from_kwargs(&#34;assay_pattern&#34;, &#34;Rotor-Gene&#34;, kwargs)
            assay_of_interest = aux.from_kwargs(&#34;assay&#34;, None, kwargs, rm=True)
            parser.assay_pattern(assay_pattern)
            
            # get data column labels
            id_label = aux.from_kwargs(&#34;id_label&#34;, &#34;Name&#34;, kwargs, rm = True)
            ct_label = aux.from_kwargs(&#34;ct_label&#34;, &#34;Ct&#34;, kwargs, rm = True)
            parser.labels(id_label,ct_label)

            # pipe the datafile through the parser
            parser.read(self._src, sheet_name = sheet_name)
            parser.parse(**kwargs)

            if len(parser.assays()) &gt; 1:
                if assay_of_interest is None: 
                    aw.HardWarning(&#34;Reader:cannot_read_multifile&#34;, file = self._src, assays = parser.assays(), traceback = False)
                self._df = parser.get(assay_of_interest)
                self.id(assay_of_interest)
            else:
                assay_of_interest = parser.assays()[0]
                self._df = parser.get(assay_of_interest)
                self.id(assay_of_interest)
        
    def _csv_read(self, **kwargs):
        &#34;&#34;&#34;
        Reads the given data file if it&#39;s a csv file
        &#34;&#34;&#34;
        df = None
        try: 
            df = pd.read_csv(
                                self._src, 
                                sep = self._delimiter, 
                                header = aux.from_kwargs(&#34;header&#34;, 0, kwargs), 
                                names = raw_col_names
                            )
        except: 
            aw.HardWarning(&#34;Reader:cannot_read_csv&#34;, file = self._src)

        # check if a valid Ct column was found
        Ct = raw_col_names[1]
        full_valid_Ct_col = len(  df[ df[Ct] == df[Ct] ]  ) == len(df)
        if not full_valid_Ct_col:
            aw.HardWarning(&#34;Reader:cannot_read_csv&#34;, file = self._src)

        self._df = df

    def _filesuffix(self):
        &#34;&#34;&#34;
        Returns the file-suffix of the provided file
        &#34;&#34;&#34;
        try: 
            suffix = self._src.split(&#34;.&#34;)[-1]
        except: 
            pass
        return suffix

class Reader(_CORE_Reader):
    &#34;&#34;&#34;
    Reads qpcr raw data files in csv or excel format to get a single dataset. 

    Note
    -----
    This implementation of the default Reader is now deprecated and will be removed
    at some point in the future. The new impelentation is the `qpcr.Readers.SingleReader`.
    Please, use that one instead.

    Input Data Files
    ----------------
    Valid input files are either regular `csv` files, or  irregular `csv` or `excel` files, 
    that specify assays by one replicate identifier column and one Ct value column.

    Irregular input files may specify multiple assays as separate tables, 
    one assay has to be selected using the `assay` argument. 
    Separate assay tables may be either below one another (separated by blank lines!)
    or besides one another (requires `transpose = True`).

    Parameters
    ----------
    filename : str
        A filepath to a raw data file.
        If the file is a `csv` file, it has to have two named columns; one for replicate names, one for Ct values. 
        Both csv (`,` spearated) and csv2 (`;` separated) are accepted.
        If the file is an `excel` file it the relevant sections of the spreadsheet are identified automatically. 
        But they require identifying headers. By default `Name` and `Ct` are assumed but these can be changed using 
        the `name_label` and `Ct_label` arguments that can be passed as kwargs (they will be forwarded to the `.read()` method). 

    **kwargs
        Any additional keyword arguments that shall be passed to the `read()` method which is immediately called during init.
    &#34;&#34;&#34;
    def __init__(self, filename:str, **kwargs) -&gt; pd.DataFrame: 
        super().__init__()
        self._src = filename
        if self._filesuffix() == &#34;csv&#34;:
            self._delimiter = &#34;;&#34; if self._is_csv2() else &#34;,&#34;
        self.read(**kwargs)

    def _is_csv2(self):
        &#34;&#34;&#34;
        Tests if csv file is ; delimited (True) or common , (False)
        &#34;&#34;&#34;
        with open(self._src, &#34;r&#34;) as openfile: 
            content = openfile.read()
        if &#34;;&#34; in content: 
            return True
        return False

    def _has_header(self):
        &#34;&#34;&#34;
        Checks if column headers are provided in the data file
        It does so by checking if the second element in the first row is numeric
        if it is numeric (returns None &lt;&lt; False) no headers are presumed. Otherwise
        it returns 0 (as in first row has headers)...
        &#34;&#34;&#34;
        with open(self._src, &#34;r&#34;) as openfile: 
            content = openfile.read().split(&#34;\n&#34;)[0]
            content = content.split(self._delimiter)
        try: 
            second_col = content[1]
            second_col = float(second_col)
        except ValueError:
            return 0 # Headers in row 0
        return None  # no headers


# class _Qupid_Reader(_CORE_Reader):
#     &#34;&#34;&#34;
#     This Reader class works with streamlit&#39;s UploadedFile class.
    
#     Note
#     -------
#     We have to use a little hack to make the UploadedFile readable.
#     We convert to a string and then back to a StringIO which we can then pass to pandas...
    
#     Parameters
#     ----------
#     file
#         A streamlit UploadedFile object
#     &#34;&#34;&#34;
#     def __init__(self, file) -&gt; pd.DataFrame: 
#         super().__init__()
#         self._filename = file.name
#         if self._filesuffix() == &#34;csv&#34;:
#             self._content = file.read().decode()
#             self._src = StringIO(self._content)
#             self._delimiter = &#34;;&#34; if self._is_csv2() else &#34;,&#34;
#         else: 
#             self._src = file
#         self.read()

#     def _filesuffix(self):
#         &#34;&#34;&#34;
#         Returns the file-suffix of the provided file
#         &#34;&#34;&#34;
#         suffix = self._filename.split(&#34;.&#34;)[-1]
#         return suffix

#     def _is_csv2(self):
#         &#34;&#34;&#34;
#         Tests if csv file is ; delimited (True) or common , (False)
#         &#34;&#34;&#34;
#         if &#34;;&#34; in self._content: 
#             return True
#         return False

#     def _has_header(self):
#         &#34;&#34;&#34;
#         Checks if column headers are provided in the data file
#         It does so by checking if the second element in the first row is numeric
#         if it is numeric (returns None &lt;&lt; False) no headers are presumed. Otherwise
#         it returns 0 (as in first row has headers)...
#         &#34;&#34;&#34;
#         content = self._content.split(&#34;\n&#34;)[0]
#         content = content.split(self._delimiter)
#         try: 
#             second_col = content[1]
#             second_col = float(second_col)
#         except ValueError:
#             return 0 # Headers in row 0
#         return None  # no headers

class Assay(aux._ID):
    &#34;&#34;&#34;
    The central storing unit of single datasets that were read from datafiles.
    An `qpcr.Assay` stores the replicate identifiers and Ct values, and also 
    groups these according to the `replicates` information (which is automatically
    inferred by default). Groups of replicates can be arbitrarily renamed by the user.

    Note
    -------
    The new implementation of the `qpcr.Assay` works directly with a DataFrame
    that was generated by any one of the `qpcr.Readers` or `qpcr.Parsers` 
    instead of a (now depcrecated) `qpcr.Reader` object. 
    However, a `qpcr.Reader` can still be passed as `df` argument and will be read-in using `link`. 
    Support for this will be removed at some point in the future. 

    Parameters
    ----------
    df : pandas.DataFrame
        A DataFrame produces by one of the `qpcr.Readers` 
        containing an `id` column for the replicate identifiers 
        and a `Ct` value column. 
    id : str
        The identifer of the assays (the Assay name, essentially). 

    replicates : int or tuple or str
            Can be an `integer` (equal group sizes, e.g. `3` for triplicates), 
            or a `tuple` (uneven group sizes, e.g. `(3,2,3)` if the second group is only a duplicate). 
            Another method to achieve the same thing is to specify a &#34;formula&#34; as a string of how to create a replicate tuple.
            The allowed structure of such a formula is `n:m,` where `n` is the number of replicates in a group and `m` is the number of times
            this pattern is repeated (if no `:m` is specified `:1` is assumed). See `qpcr.Assay.replicates` for an example. 

    group_names : list
        A list of names to use for the replicates groups. If replicates of the same group share the same identifier, then the 
        group will be inferred automatically. Otherwise, default group names will be set if no `group_names` are provided. 
    &#34;&#34;&#34;
    def __init__(self, df : pd.DataFrame = None, id : str = None, replicates : (int or tuple or str) = None, group_names : list = None) -&gt; dict:
        super().__init__()
        
        # FUTURE DROP HERE
        # check if a qpcr.Reader was supplied instead of a dataframe
        # drop this at some point
        if isinstance(df, Reader):
            self.link(df)
        else: 
            self._df = df
            if id is not None: self._id = id

        # setup length of the found data        
        self._length = None if self._df is None else len(self._df)

        # get replicates
        self._replicates = replicates

        # store names 
        self._names = group_names

        # setup the amplification efficiency
        self._efficiency = 1.0 
        self._eff = 2 * self._efficiency

        # if we got data, try to read it 
        if self._df is not None: 
            try: 
                self.replicates(self._replicates)
                self.group()
            except Exception as e:
                aw.SoftWarning(&#34;Assay:setup_not_grouped&#34;)
            
            # and try to change names, provided that we could group yet...
            if self._names is not None and self.groups() is not None: 
                self.rename(self._names)

    def efficiency( self, eff : float = None ):
        &#34;&#34;&#34;
        Gets or sets the amplification efficiency of the Assay.

        Parameters
        -------
        eff : float
            A new efficiency to assign to the assay.

        Returns
        -------
        float 
            The currently assigned efficiency.
        &#34;&#34;&#34;
        if isinstance( eff, (float, int) ):
            self._efficiency = float( eff ) 
            self._eff = 2 * self._efficiency
        return self._efficiency 

    def save(self, filename : str):
        &#34;&#34;&#34;
        Saves the data from the `Assay` to a `csv` file.
        Parameters
        ----------
        filename : str
            The filename into which the assay should be stored.
            If this is a `directory`, then the assay `id` will automatically
            be used as filename. 
        &#34;&#34;&#34;
        if os.path.isdir(filename):
            filename = os.path.join(filename, f&#34;{self.id()}.csv&#34;)
        self.to_csv(filename, index = False)

    def get(self, copy : bool = False ):
        &#34;&#34;&#34;
        Parameters
        -------
        copy : bool
            If `True` returns a deepcopy of the stored dataframe.

        Returns
        -------
        data : pandas.DataFrame
            The stored dataframe
        &#34;&#34;&#34;
        if copy: 
            data = deepcopy( self._df )
        else: 
            data = self._df 
        return data

    def boxplot( self, mode : str = &#34;interactive&#34;, **kwargs ):
        &#34;&#34;&#34;
        A shortcut to call a `qpcr.Plotters.ReplicateBoxPlot` plotter
        to visualise the loaded replicates.

        Parameters
        -------
        mode : str
            The plotting mode. May be either &#34;static&#34; (matplotlib) or &#34;interactive&#34; (plotly).
        **kwargs
            Any additional keyword arguments to be passed to the plotter.

        Returns
        -------
        fig : plt.figure or plotly.figure
            The figure generated by `ReplicateBoxPlot`.
        &#34;&#34;&#34;
        plotter = Plotters.ReplicateBoxPlot( mode = mode )
        plotter.link( self )
        fig = plotter.plot( **kwargs )
        return fig 



    def tile(self, n : int = 1):
        &#34;&#34;&#34;
        Expands the dataframe to the square number of entries for each group.
        This is useful for combinatoric normalisation wherein each replicate is normalised
        against each replicate group-wise from the normaliser, instead of only its supposed partner value.
        
        Parameters
        -------
        n : int
            The number of tiles to produce. By default `1 tile` will effectively *square* the number of entries within the dataframe.
        &#34;&#34;&#34;
        df = self.get()
        groups = self.groups()

        new = None

        for group in groups: 
            subset = df.query(f&#34;group == {group}&#34;)
            length = len(subset) * n
            subset = pd.concat( [subset for i in range(length) ], ignore_index = True )
            if new is None:
                new = subset
            else:
                new = pd.concat( [new, subset], ignore_index = True )

        self.adopt( new, force = True)

    def stack(self, n : int = 2):
        &#34;&#34;&#34;
        Expands the dataframe entry-wise `n` times. 

        Parameters
        -------
        n : int
            The number of stacks to produce. `1 stack` will introduce one more copy of each replicate.
            Note, `n == 1` will keep the current entries!
        &#34;&#34;&#34;
        df = self.get()
        groups = self.groups()

        n = int(n)

        new = None
        if n &gt; 1:
            for group in groups: 
                subset = df.query(f&#34;group == {group}&#34;)
                length = n
                subset = pd.concat( [subset for i in range(length) ], ignore_index = True )
                if new is None:
                    new = subset
                else:
                    new = pd.concat( [new, subset], ignore_index = True )

            self.adopt( new, force = True)

    def Ct(self):
        &#34;&#34;&#34;
        Returns
        ------
        Ct : pandas.Series
            A pandas Series with the assay&#39;s Ct values. The column is renamed 
            from &#34;Ct&#34; to the assay&#39;s `id`.
        &#34;&#34;&#34;
        Ct = self._df[ raw_col_names[1] ]
        Ct.name = self.id()
        return Ct


    def dCt(self):
        &#34;&#34;&#34;
        Returns
        -------
        dCt : pandas.Series
            A pandas Series with the computed Delta-Ct values. The column is renamed 
            from &#34;dCt&#34; to the assay&#39;s `id`.
        &#34;&#34;&#34;
        dCt = self._df[&#34;dCt&#34;]
        dCt.name = self.id()
        return dCt

    def ddCt(self):
        &#34;&#34;&#34;
        Returns
        -------
        ddCt : pandas.DataFrame
            A pandas DataFrame with all Delta-Delta-Ct values that the Assay has stored. 
            All `&#34;rel_{}&#34;` columns are renamed to include the assay `id` to `&#34;{id}_rel_{}&#34;`.
        &#34;&#34;&#34;
        # get all ddCt columns
        ddCt = [ i for i in self._df.columns if &#34;rel_&#34; in i ]
        id = self._id
        # make new names and generate renaming dictionary 
        new_names = [ f&#34;{id}_{i}&#34; for i in ddCt ]
        new_names = {  old : new for new, old in zip(new_names, ddCt)  }

        # get the data and rename
        ddCt = self._df[ ddCt ]
        if not isinstance(ddCt, pd.DataFrame):
            ddCt = pd.DataFrame(ddCt)
        ddCt = ddCt.rename(columns = new_names)
        
        return ddCt

    # FUTURE FEATURE HERE
    # def fc(self):
        # some method to also return the fold change columns... 


    def rename_cols(self, cols:dict):
        &#34;&#34;&#34;
        Renames columns according to a dictionary as key -&gt; value.

        Parameters
        ----------
        cols : dict
            A dictionary specifying old column names (keys) and new colums names (values).
        &#34;&#34;&#34;
        self._df = self._df.rename(columns = cols)


    def link(self, Reader:Reader):
        &#34;&#34;&#34;
        Links a `qpcr.Reader` object to the Assay.

        Note
        ------
        This is deprecated since the `qpcr.Reader` has been 
        replaced by the `qpcr.Readers.SingleReader`. 
        This method will be removed in the future.  

        Parameters
        ----------
        Reader : qpcr.Reader
            A qpcr.Reader object.
        &#34;&#34;&#34;
        self._Reader = Reader
        self.adopt_id(Reader)
        df = self._Reader.get()
        self._df = df
        self._length = self._Reader.n()
        
    def n(self):
        &#34;&#34;&#34;
        Returns 
        ------

        int 
            The number of entries (individual replicates) within the Assay.
        &#34;&#34;&#34;
        return len( self._df )  # self._length

    def add_dCt(self, dCt : pd.Series): 
        &#34;&#34;&#34;
        Adds results from Delta-Ct (first Delta-Ct performed by a `qpcr.Analyser`).

        Parameters
        -----------
        dCt : pandas.Series
            A pandas Series of Delta-Ct values that will be stored in a column `&#34;dCt&#34;`.
            Note, that each `Assay` can, of course, only store one single Delta-Ct column. 
        &#34;&#34;&#34;
        self._df[&#34;dCt&#34;] = dCt
    
    def add_ddCt(self, normaliser_id : str, ddCt : pd.Series):
        &#34;&#34;&#34;
        Adds results from Delta-Delta-Ct (&#34;normalisation&#34; performed by a `qpcr.Normaliser`).
        These will be stored in a column named `&#34;rel_{normaliser_id}&#34;`. Hence, an Assay can store
        an arbitrary number of Delta-Delta-Ct columns against an arbitrary number of different normalisers. 
        
        Parameters
        ----------
        normaliser_id : str
            The id of the normaliser Assay used to compute the Delta-Delta-Ct values.
        ddCt : pandas.Series
            A pandas Series of Delta-Delta-Ct values.
        &#34;&#34;&#34;
        name = f&#34;rel_{normaliser_id}&#34;
        self._df[name] = ddCt

    # FUTURE FEATURE HERE
    # some method to add fc columns here...

    def adopt(self, df : pd.DataFrame, force = False):
        &#34;&#34;&#34;
        Adopts an externally computed dataframe as its own.
        This is supposed to be used when setting up new `qpcr.Assay` objects that do not 
        inherit data from one of the `qpcr.Readers`. If you wish to alter an existing `qpcr.Assay` use `force = True`.
        When doing this, please, make sure to retain the proper data structure!

        Parameters
        ----------
        df : pd.DataFrame
            A pandas DataFrame.
        force : bool
            If a dataframe is already stored the new dataframe will only be stored if `force = True`.
        &#34;&#34;&#34;
        if self._df is None: 
            self._df = df
        elif force:
            self._df = df
        else:
            aw.HardWarning(&#34;Assay:no_data_adopted&#34;)
        self._length = len(self._df)

    def names(self, as_set = True):
        &#34;&#34;&#34;
        Returns a set of the replicate group names (maintaing group order).

        Parameters
        ----------
        as_set : bool
            If `as_set = True` (default) it returns a set (as list without duplicates) 
            of assigned group names for replicate groups.
            If `as_set = False` it returns the full group_name column (including all repeated entries).
        
        Returns
        -------
        names : list or pd.Series
            The given group names of all replicate groups.
        &#34;&#34;&#34;
        if &#34;group_name&#34; in self._df.columns: 
            if as_set:
                return aux.sorted_set(list(self._df[&#34;group_name&#34;]))
            else: 
                return self._df[&#34;group_name&#34;]
        else: 
            aw.SoftWarning(&#34;Assay:no_groupnames&#34;)
            return None
    
    def groups(self, as_set = True):
        &#34;&#34;&#34;
        Returns a set of sample groups (numeric).

        Parameters
        ----------
        as_set : bool
            If `as_set = True` (default) it returns a set (as list without duplicates) 
            of assigned group names for replicate groups.
            If `as_set = False` it returns the full group_name column (including all repeated entries).
        
        Returns
        -------
        groups : list
            The given numeric group identifiers of all replicate groups.
        &#34;&#34;&#34;
        if &#34;group&#34; in self._df.columns:
            groups = sorted(list(set(self._df[&#34;group&#34;]))) if as_set else self._df[&#34;group&#34;]
            return groups
        else:
            aw.SoftWarning(&#34;Assay:setup_not_grouped&#34;)
            return None

    def replicates(self, replicates : (int or tuple or str) = None):
        &#34;&#34;&#34;
        Either sets or gets the replicates settings to be used for grouping
        Before they are assigned, replicates are vetted to ensure they cover all data entries.

        Parameters
        ----------
        replicates : int or tuple or str
            Can be an `integer` (equal group sizes, e.g. `3` for triplicates), 
            or a `tuple` (uneven group sizes, e.g. `(3,2,3)` if the second group is only a duplicate). 
            Another method to achieve the same thing is to specify a &#34;formula&#34; as a string of how to create a replicate tuple.
            The allowed structure of such a formula is `n:m,` where `n` is the number of replicates in a group and `m` is the number of times
            this pattern is repeated (if no `:m` is specified `:1` is assumed). 
            
            So, as an example, if there are 12 groups which are triplicates, but
            at the end there is one which only has a single replicate (like the commonly measured diluent qPCR sample), we could either specify the tuple
            individually as `replicates = (3,3,3,3,3,3,3,3,3,3,3,3,1)` or we use the formula to specify `replicates = &#34;3:12,1&#34;`. Of course, this works for
            any arbitrary setting such as `&#34;3:5,2:5,10,3:12&#34;` (which specifies five triplicates, followed by two duplicates, a single decaplicate, and twelve triplicates again â€“ truly a dataset from another dimension)...
        &#34;&#34;&#34;
        if replicates is not None and self._df is not None: 
            # convert a string formula to tuple if one was provided
            if isinstance(replicates, str): 
                replicates = self._reps_from_formula(replicates)
            # vet replicate coverage
            if self._vet_replicates(replicates):
                self._replicates = replicates
            else: 
                aw.HardWarning(&#34;Assay:reps_dont_cover&#34;, n_samples = self._length, reps = replicates)
        return self._replicates

    def group(self, infer_names = True):
        &#34;&#34;&#34;
        Groups the data according to replicates-settings specified.

        Parameters
        ----------
        infer_names : bool
            Try to infer names of replicate groups based on the individual replicate sample identifiers.
            Note that this only works if all replicates have an identical sample name!
        &#34;&#34;&#34;
        
        # generate group and group_names columns
        if isinstance(self._replicates, int):
            groups, group_names = self._make_equal_groups()            
        elif isinstance(self._replicates, tuple):
            groups, group_names = self._make_unequal_groups()
        else:
            if self._identically_named():
                groups = self._infer_replicates()
                group_names = [default_group_name.format(i) for i in groups]
            else: 
                aw.HardWarning(&#34;Assay:no_reps_inferred&#34;, assay = self.id())
        
        # add numeric group identifiers
        self._df[&#34;group&#34;] = groups
        self._df[&#34;group_name&#34;] = group_names
        
        if infer_names: #and self._names is None:
            # infer group names
            self._infer_names()
            

    def rename(self, names:(list or dict)):
        &#34;&#34;&#34;
        Replaces the current names of the replicate groups 
        (stored in the &#34;group_name&#34; column).

        Parameters
        ----------
        names : list or dict
            Either a `list` (new names without repetitions) or `dict` (key = old name, value = new name) specifying new group names. 
            Group names only need to be specified once, and are applied to all replicate entries.
        &#34;&#34;&#34;
        # get new group names based on list (index) or dict (key)
        if isinstance(names, (list, tuple, set)):
            new_names = self._rename_per_index(names)       
        elif isinstance(names, dict):
            new_names = self._rename_per_key(names)
        else:
            aw.SoftWarning(&#34;Assay:no_groupname_assignment&#34;, names = names)

        # update &#34;group_name&#34;
        self._df[&#34;group_name&#34;] = new_names
        self._renamed = True

    def ignore(self, entries:tuple, drop = False):
        &#34;&#34;&#34;
        Remove lines based on index from the dataframe.
        This is useful when removing corrupted data entries.

        Parameters
        ----------
        entries : tuple
            Tuple of row indices from the dataframe to drop.
        drop : bool 
            If True the provided entries will be entirely removed from the 
            dataset. If False, ignore entries will be set to NaN. 
        &#34;&#34;&#34;
        if drop:
            self._df = self._df.drop(index = list(entries))
        else: 
            Cts = self.Ct()
            Cts = np.array( Cts )
            Cts[ entries ] = np.nan
            self._df[&#34;Ct&#34;] = Cts
    
    def _reps_from_formula(self, replicates):
        &#34;&#34;&#34;
        Generates a replicate tuple from a string formula. 
        See the docstring of `replicates()` for more info on the formula.

        Example:
        &#34;3:4,1:4,2:3,9&#34; -&gt; (3, 3, 3, 3, 1, 1, 1, 1, 2, 2, 2, 9)
        &#34;&#34;&#34;

        # split the formula and adjust standard formatting
        replicates = replicates.split(&#34;,&#34;)
        replicates = [i + &#34;:1&#34; if &#34;:&#34; not in i else i for i in replicates]

        # convert to numeric values and extend
        replicates = [np.array(i.split(&#34;:&#34;), dtype = int) for i in replicates]
        replicates = [np.tile(i[0], i[1]) for i in replicates]
        
        # generate replicate tuple
        replicates = np.concatenate(replicates)
        replicates = tuple(replicates)
        
        return replicates

    def _infer_replicates(self):
        &#34;&#34;&#34;
        Infers the replicate groups based on the replicate ids in case all replicates of the same group have the same name.
        &#34;&#34;&#34;
        names = self._df[raw_col_names[0]]
        names_set = aux.sorted_set(names)
        groups = [i for i in range(len(names_set))]
        for name, group in zip(names_set, groups):
            names = names.replace(name, group)
        
        indices = np.array(names, dtype = int)
        return indices

    def _infer_names(self):
        &#34;&#34;&#34;
        Infers replicate group names from the given replicate identifier column
        &#34;&#34;&#34;
        if self._identically_named():
            self._df[&#34;group_name&#34;] = self._df[raw_col_names[0]]
        elif self._names is None: 
            aw.SoftWarning(&#34;Assay:groupnames_not_inferred&#34;)

    def _identically_named(self):
        &#34;&#34;&#34;
        Checks if all replicates in the same group have the same name / id
        It checks simply the first group, if that is identical then it&#39;s fine.
        &#34;&#34;&#34;
        if &#34;group&#34; not in self._df.columns:
            names = self._df[raw_col_names[0]]
            names_set = aux.sorted_set(names)
            first_name = names_set[0]
            group0 = self._df.query(f&#34;{raw_col_names[0]} == &#39;{first_name}&#39;&#34;)[raw_col_names[0]]
            entries = len(group0)
            all_identical = entries &gt; 1                
        else: 
            group0 = self._df.query(&#34;group == 0&#34;)[raw_col_names[0]]
            all_identical = all(group0 == group0[0])
        return all_identical

    def _rename_per_key(self, names):
        &#34;&#34;&#34;
        Generates new name list based on current names in &#34;group_name&#34; and uses string.replace()
        to update groupnames, based on key (old name) : value (new name) indexing. 
        Before applying it checks if all groups are covered by new names
        &#34;&#34;&#34;
        current_names = aux.sorted_set(self._df[&#34;group_name&#34;])
        all_groups_covered = len(names) == len(current_names)
        if all_groups_covered:
            current_names = list(self._df[&#34;group_name&#34;])
            new_names = &#34;$&#34;.join(current_names)
            for old_name, new_name in names.items():
                new_names = new_names.replace(old_name, new_name)
            new_names = new_names.split(&#34;$&#34;)       
            return new_names
        else:
            aw.HardWarning(&#34;Assay:groupnames_dont_colver&#34;, current_groups = current_names, new_received = names)

    def _rename_per_index(self, names):
        &#34;&#34;&#34;
        Generates new name list based on current names in &#34;group_names&#34; and uses string.replace()
        to update groupnames to new names based on index (using a the order 
        of groups as is currently present in &#34;group_name&#34;). 
        &#34;&#34;&#34;
        current_names_set = aux.sorted_set(self._df[&#34;group_name&#34;])
        all_groups_covered = len(names) == len(current_names_set)
        if all_groups_covered:
            current_names = list(self._df[&#34;group_name&#34;])
            new_names = &#34;$&#34;.join(current_names)
            names = list(names)
            for old_name, new_name in zip(current_names_set, names):
                new_names = new_names.replace(old_name, new_name)
            new_names = new_names.split(&#34;$&#34;)
            return new_names
        else:
            aw.HardWarning(&#34;Assay:groupnames_dont_colver&#34;, current_groups = current_names_set, new_received = names)

    def _make_unequal_groups(self):
        &#34;&#34;&#34;
        Returns two lists of [0,0,0,1,1,1] and 
        [Group0, Group0, Group0, Group1,...] 
        to cover all data entries.
        (this function works with a tuple for replicate group sizes)
        &#34;&#34;&#34;
        groups = []
        group_names = []
        for rep, idx in zip(self._replicates, range(len(self._replicates))): 
            groups.extend([idx] * rep)
            group_names.extend([ default_group_name.format(idx) ] * rep)
        return groups, group_names

    def _make_equal_groups(self):
        &#34;&#34;&#34;
        Returns two lists of [0,0,0,1,1,1] and 
        [Group0, Group0, Group0, Group1,...] 
        to cover all data entries.
        (this function works with an integer group size, 
        assuming all groups have the same size)
        &#34;&#34;&#34;
        assays = self._length
        groups = []
        group_names = []
        slices = range(int(assays / self._replicates))
        for i in slices:
            groups.extend([i] * self._replicates)
            group_names.extend([ default_group_name.format(i) ] * self._replicates)
        return groups, group_names

    def _vet_replicates(self, replicates : (int or tuple)):
        &#34;&#34;&#34;
        Checks if provided replicates will place all data entries into a group
        returns True if all replicates are covered, False if not...
        &#34;&#34;&#34;
        current_entries = self._length
        verdict = None

        # for INT -&gt; modulo will be 0 if all replicates are covered
        # for TUPLE -&gt; sum(replicates) should cover all replicates...

        if isinstance(replicates, int):
            verdict = True if current_entries % replicates == 0 else False
        elif isinstance(replicates, tuple): 
            verdict = True if sum(replicates) == current_entries else False
        
        if verdict is None: 
            aw.HardWarning(&#34;Assay:reps_could_not_vet&#34;, reps = replicates)

        return verdict

&#34;&#34;&#34;
## Setting up a `qpcr.Assay`
---

Here is a manual example of creating a `qpcr.Assay` object. You can use either the `qpcr.DataReader` or any one of `qpcr.Readers` directly to 
read in your data and generate a pandas DataFrame. Note, the `qpcr.Readers` are already equipped with `make_Assay(s)` methods that will handle
setting up `qpcr.Assay` objects for you. 

However, setting up `qpcr.Assay`s manually can be as simple as:

```python
# get the dataframe from one of the qpcr.Readers
mydata = some_reader.get()

assay = Assay( df = mydata, id = &#34;my_assay&#34; )

```

If your replicate identifiers are the same for all replicates within each group then the groups are automatically inferred. And your assay is 
ready at this point already to be passed to an `qpcr.Analyser`. If not, you can specify the replicates manually like: 

```python
# manually specify triplicates during setup
assay = Assay( df = mydata, id = &#34;my_assay&#34;, replicates = 3 )

# or you can change the replicates after initial setup like 
assay = Assay( df = mydata, id = &#34;my_assay&#34; )
assay.replicates(3)
assay.group()

```

&#34;&#34;&#34;

class SampleReader(Assay):
    &#34;&#34;&#34;
    Sets up a Reader+Assay pipeline that reads in a single datafile and handles the 
    extracted dataset in a pandas dataframe. 
    Its `read()` method directly returns a `qpcr.Assay` object that can be piped to Analyser. 
    
    Note
    ----
    This is now deprecated and will be removed in a future version! Please, use the `qpcr.DataReader` instead.
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        
        aw.SoftWarning(&#34;Versions:Deprecation&#34;, old = &#34;SampleReader&#34;, new = &#34;DataReader&#34;)

        self._replicates = None
        self._names = None
        self._Reader = None
        self._Assay = None

    # def __str__(self):
    #     header = f&#34;qpcr.SampleReader ({self._id})&#34;
    #     reps = f&#34;Replicate settings:\t{self._replicates}&#34;
    #     names = f&#34;Name settings:\t\t{self._names}&#34;

    #     header_line = max([len(i) for i in [header, reps, names]])
    #     header_line = &#34;-&#34; * header_line

    #     string = f&#34;{header_line}\n{header}\n{header_line}\n\n{reps}\n{names}\n\n{header_line}&#34;
    #     return string

    def replicates(self, replicates:(int or tuple)):
        &#34;&#34;&#34;
        Set the replicates specifics to use for grouping.

        Parameters
        ----------
        replicates : int or tuple
            Can be an `integer` (equal group sizes, e.g. `3` for triplicates), 
            or a `tuple` (uneven group sizes, e.g. `(3,2,3)` if the second group is only a duplicate). 
        &#34;&#34;&#34;
        self._replicates = replicates

    def names(self, names:(list or dict)):
        &#34;&#34;&#34;
        Set names for replicates groups.

        Parameters
        ----------
        names : list or dict
            Either a `list` (new names without repetitions) or `dict` (key = old name, value = new name) specifying new group names. 
            Group names only need to be specified once, and are applied to all replicate entries.
        &#34;&#34;&#34;
        self._names = names
        
    def read(self, filename, **kwargs):
        &#34;&#34;&#34;
        Reads one raw datafile (csv or excel format).

        Parameters
        ----------
        filename : str
            A filepath to a raw data file.
            If the file is a `csv` file, it has to have two named columns; one for replicate names, one for Ct values. 
            Both csv (`,` spearated) and csv2 (`;` separated) are accepted.
            If the file is an `excel` file it the relevant sections of the spreadsheet are identified automatically. 
            But they require identifying headers. By default `Name` and `Ct` are assumed but these can be changed using 
            the `name_label` and `Ct_label` arguments that can be passed to `read()` as kwargs.
        **kwargs
            Any additional keyword arguments that should be passed to the `qpcr.Reader`.

        Returns
        -------
        Assay : qpcr.Assay
            A `qpcr.Assay` object containing the grouped and renamed data.
        &#34;&#34;&#34;
        self._Reader = Reader(filename, **kwargs)
        self._Reader.id(aux.fileID(filename))

        self._Assay = Assay(self._Reader)
        self._Assay.adopt_id(self._Reader)

        if self._replicates is not None:
            self._Assay.replicates(self._replicates)
        else: 
            pass 
            # VITAL CHANGE HERE
            # since Assays can now infer replicates we 
            # don&#39;t raise an Error if no replicates are specified manually...
            # aw.HardWarning(&#34;SampleReader:no_reps_yet&#34;)

        if self._names is not None:
            self._Assay.group(infer_names = False)
            self._Assay.rename(self._names)
        else:
            self._Assay.group()
            
        return self._Assay

class DataReader(aux._ID):
    &#34;&#34;&#34;
    Handles reading a single file containing input data
    for `qpcr`. 
    
    Note
    -----
    This is a top-level class that is designed
    as the central port through which data is read into `qpcr.Assay` objects
    from both regular and irregular, single- and multi-assay files.
    This is the suggested way to read your data for most users, which should
    work in most cases.
    
    However, due to the automated setup of the inferred Readers there may be cases where you 
    will either have a hard time or be unable to read your datafiles using the `DataReader`. 
    In such cases, don&#39;t try too long to make it work with the DataReader, 
    just use one of the `qpcr.Readers` or even `qpcr.Parsers`directly.  

    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._replicates = None
        self._names = None
        self._Reader = None             # the functional core will be either a Reader
        self._Data = {}                 # the _Data attribute will store any output from the Reader as a dictionary with filename : data structure.
        self._tmp_data = None           # this will not attempt to distinguish between assays / normalisers, or anything. It&#39;s just an archive of whatever data we got.
                                        # by default data is not stored by the DataReader, it&#39;s read only pipes through, but data can be stored using the .store() method.

    def Reader(self, Reader = None):
        &#34;&#34;&#34;
        Gets or sets the functional core Reader    
        &#34;&#34;&#34;
        if Reader is not None:
            self._Reader = Reader
        return self._Reader

    def clear(self):
        &#34;&#34;&#34;
        Clears all data that was extracted
        &#34;&#34;&#34;
        self._Data = {}
    
    def reset(self):
        &#34;&#34;&#34;
        Resets the core Reader
        &#34;&#34;&#34;
        self._Reader = None
    
    def prune(self):
        &#34;&#34;&#34;
        Resets the DataReader completely
        &#34;&#34;&#34;
        self.__init__()

    def store(self):
        &#34;&#34;&#34;
        Will store the read data. 


        Note 
        -------
        `DataReader` does NOT have a specific data storage facility to distinguish between 
        assays / normalisers, data types, etc. It simply keeps a dictionary of `{filename : data}`
        that can be accessed. This is designed in case multiple files should be read using the same 
        `DataReader` to allow an easier access of the data in case the data outputs are of the same type.

        However, the main intended application of `DataReader` is to use the`read` method&#39;s returned data directly.
        &#34;&#34;&#34;
        self._Data.update(self._tmp_data)
    
    def get(self):
        &#34;&#34;&#34;
        Returns the stored data
        &#34;&#34;&#34;
        return self._Data

    def read(self, filename : str, multi_assay : bool = False, big_table : bool = False, decorator : (bool or str) = None, reset = False, **kwargs):
        &#34;&#34;&#34;
        Reads an input file and extracts available datasets using the
        specified `Reader` or by setting up an approproate `Reader`. 

        Parameters
        ----------
        filename : str
            A filepath to an input datafile.

        multi_assay : bool
            Set to `True` if the file contains multiple assays you wish to read.

        big_table : bool
            Set to `True` if the file is a &#34;Big Table&#34; file. 
            Check out the documentation of the `qpcr.Readers` for more 
            information on &#34;Big Table&#34; files.

        decorator : str or bool
            Set if the file is decorated. This can be set either to `True` for `multi_assay` and multi-sheet (excel) or `big_table` files,
            or it can be set to a valid `qpcr decorator` for single assay files or single-sheet files.
            Check out the documentation of the `qpcr.Parsers` for more information on decorators.

        reset : bool
            If multiple input files should be read but they do not all 
            adhere to the same filetype / datastructure, use `reset = True` 
            to set up a new Reader each time `read` is called.

        **kwargs
            Any additional keyword arguments to be passed to the core Reader.
            Note, while this tries to be utmost versatile there is a limitation
            to costumizibility through the kwargs. If you require streamlined datareading
            use dedicated `qpcr.Readers` and/or `qpcr.Parsers` directly.
        &#34;&#34;&#34;
        self._src = filename
        # vet filesuffix
        suffix = self._filesuffix()
        if suffix not in supported_filetypes:
            aw.HardWarning(&#34;MultiReader:unknown_datafile&#34;, file = self._src)
        
        if reset or self._Reader is None: 
            self.reset()
            self._setup_Reader(
                                multi_assay = multi_assay, 
                                big_table = big_table,
                                decorator = decorator,
                                **kwargs
                            )
        
        # read file and return data
        data = self._Reader._DataReader( 
                                            filename = self._src, 
                                            decorator = decorator, 
                                            **kwargs 
                                    )

        self._tmp_data = {self._src : data}
        return data

    def _filesuffix(self):
        &#34;&#34;&#34;
        Returns the filesuffix
        &#34;&#34;&#34;
        return self._src.split(&#34;.&#34;)[-1]

    def _setup_Reader(self, **kwargs):
        &#34;&#34;&#34;
        Sets up the core Reader 
        &#34;&#34;&#34;
        suffix = self._filesuffix()

        use_multi = aux.from_kwargs(&#34;multi_assay&#34;, False, kwargs, rm = True)
        is_bigtable = aux.from_kwargs(&#34;big_table&#34;, False, kwargs, rm = True)

        if suffix == &#34;csv&#34;:
            if not use_multi and not is_bigtable:
                reader = Readers.SingleReader()
            elif is_bigtable:
                reader = Readers.BigTableReader()
            else:
                reader = Readers.MultiReader()

        elif suffix == &#34;xlsx&#34;:
            
            # check if not only a single sheet should be read
            multi_sheet = &#34;sheet_name&#34; not in kwargs

            if use_multi and multi_sheet:
                reader = Readers.MultiSheetReader()
            elif use_multi and not multi_sheet:
                reader = Readers.MultiReader()
            elif is_bigtable:
                reader = Readers.BigTableReader()
            else:
                reader = Readers.SingleReader()
        self._Reader = reader


class Results(aux._ID):
    &#34;&#34;&#34;
    Handles a pandas dataframe for data and computed results from a `qpcr` class. 
    
    Note
    -----
    This is a central data collection that can inherit directly from `qpcr.Assay` objects and from 
    extrenally computed sources. Please, note that it will not perform extensive vetting on its data input, 
    so make sure to only provide proper data input when manually assembling your `qpcr.Results`!
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._df = None
        self._Assay = None
        self._stats_results = {
                                &#34;group&#34; : [], 
                                defaults.default_dataset_header : [], 
                                &#34;mean&#34; : [], &#34;stdev&#34; : [], &#34;median&#34; : []
                            }
        self._stats_df = None

    def adopt_names(self, Assay:Assay):
        &#34;&#34;&#34;
        Links an instance of Assay to be used as reference for group_names
        It copies the `group` and `group_name` columns to the results storing dataframe.
        This step can only be performed once!

        Parameters
        ----------
        Assay : qpcr.Assay
            A `qpcr.Assay` object whose group_name column will be copied.
        &#34;&#34;&#34;
        if self.is_empty():
            self._df = Assay.get()
            self._drop_setup_cols()
        else:
            named_identically = all( self._df[&#34;group_name&#34;] == Assay.get()[&#34;group_name&#34;] )
            if not named_identically:
                aw.SoftWarning(&#34;Results:cannot_link&#34;)


    def add_Ct(self, assay : Assay):
        &#34;&#34;&#34;
        Adds a `&#34;Ct&#34;` column with Delta-Ct values from an `qpcr.Assay`.
        It will store these as a new column using the Assay&#39;s `id` as header.

        Parameters
        -------
        assay : qpcr.Assay
            An `qpcr.Assay` object from which to import.
        &#34;&#34;&#34;
        id = assay.id()
        Ct = assay.get()[ raw_col_names[1] ]
        Ct.name = id
        self.add(Ct)

    def add_dCt(self, assay : Assay):
        &#34;&#34;&#34;
        Adds a `&#34;dCt&#34;` column with Delta-Ct values from an `qpcr.Assay`.
        It will store these as a new column using the Assay&#39;s `id` as header.

        Parameters
        -------
        assay : qpcr.Assay
            An `qpcr.Assay` object from which to import.
        &#34;&#34;&#34;
        dCt = assay.dCt()
        self.add(dCt)

    def add_ddCt(self, assay : Assay):
        &#34;&#34;&#34;
        Adds all `&#34;rel_{}&#34;` columns with Delta-Delta-Ct values from an `qpcr.Assay`.
        It will store these as new columns using the Assay&#39;s `id` + the `_rel_{}` composite id.

        Parameters
        -------
        assay : qpcr.Assay
            An `qpcr.Assay` object from which to import.
        &#34;&#34;&#34;
        df = assay.ddCt()
        # add data
        self.add(df)

    # FUTURE FEATURE HERE
    # this feature will be for fold-change columns of second normalisation
    # currently this is not yet supported but is supposed to be an implemented feature of the 
    # Normaliser. The Normaliser is currently perfectly able to perform second normalisation but
    # does not yet have an integrated method of properly storing the results and there is also not
    # # yet a method for pairing assays together for second normalisation.
    # def add_fc(self, assay : Assay):
    #     &#34;&#34;&#34;
    #     Adds all `&#34;fc_{}&#34;` columns with Delta-Delta-Ct values from an `qpcr.Assay`.
    #     It will store these as new columns using the Assay&#39;s `id` + the `_fc_{}` composite id.

    #     Parameters
    #     -------
    #     assay : qpcr.Assay
    #         An `qpcr.Assay` object from which to import.
    #     &#34;&#34;&#34;
    #     id = assay.id()
    #     df = assay.get()
    #     # get the ddCt containing columns
    #     rel_cols = [ i for i in df.columns if &#34;fc_&#34; in i ]
    #     # generate new composite ids 
    #     new_names = [ f&#34;{id}_{i}&#34; for i in rel_cols ]
    #     # get ddCt columns 
    #     df = df[rel_cols]
    #     # rename the columns to include the assay id
    #     df = df.rename( columns = { new : old for new, old in zip(new_names, rel_cols) } )

    #     # add data
    #     self.add(df)

    def names(self, as_set = False):
        &#34;&#34;&#34;
        Returns 
        -------
        names : list or None
            The adopted `group_names` 
            (only works if a `qpcr.Assay` has already been linked 
            using `adopt_names()`!)
        &#34;&#34;&#34;
        if self._df is not None:
            names = self._df[&#34;group_name&#34;]
            if as_set: names = aux.sorted_set(names)
            return names
        return None

    def get(self):
        &#34;&#34;&#34;
        Returns 
        -------
        data : pd.DataFrame
            The results dataframe
        &#34;&#34;&#34;
        return self._df

    def is_empty(self):
        &#34;&#34;&#34;
        Checks if any results have been stored so far.

        Returns
        -------
        bool
            `True` if NO data is yet stored, else `False`.
        &#34;&#34;&#34;
        return self._df is None

    def drop_groups(self, groups : (list or str)):
        &#34;&#34;&#34;
        Removes specific groups of replicates from the DataFrame.

        Parameters
        ----------
        groups : list
            Either the numeric group identifiers or the group names
            of the groups to be removed, or a `regex` pattern defining which groups
            should be dropped (this is useful for systematically removing RT- groups etc.)
        &#34;&#34;&#34;
        # check for regex pattern
        # and get corresponding group names 
        if isinstance(groups, str):
            groups = [i for i in self._df[&#34;group_name&#34;] if re.match(groups, i) is not None]
        
        # get the right reference column and query to use to be 
        # used (either group or group_name)
        ref_query = &#34;group != {group}&#34; if isinstance( groups[0], int ) else &#34;group_name != &#39;{group}&#39;&#34;
        
        # remove groups from dataset
        for group in groups: 
            self._df = self._df.query(ref_query.format(group = group))

            # also drop from stats df
            if self._stats_df is not None:
                self._stats_df = self._stats_df.query(ref_query.format(group = group))
    

    def add(self, column:pd.Series, replace : bool = False):
        &#34;&#34;&#34;
        Adds some new column of data.

        Note
        ----
        The `column` argument has to be named for this to work. However, there are 
        already implemented methods dedicated to adding specifically Delta-Ct, Delta-Delta-Ct or just
        Ct values to the Results.

        Parameters
        ----------
        column : pd.Series
            A named pandas Series or DataFrame that can be joined into the already
            stored dataframe.
        replace : bool
            In case results from a computation with the same identifiers are already stored
            no new data can be stored under that id. Either the new data must be renamed or
            `replace = True` must be set to overwrite the presently stored data. 
        &#34;&#34;&#34;
        if isinstance(column, pd.Series):
            if column.name in self._df.columns:
                if not replace:  
                    aw.SoftWarning(&#34;Results:name_overlap&#34;, name = column.name)
                else: 
                    self._df[column.name] = column
            else: 
                self._df = self._df.join(column)
        else: 
            for i in column.columns:
                if i in self._df.columns: 
                    if not replace: 
                        aw.SoftWarning(&#34;Results:name_overlap&#34;, name = i )
                col = column[i]
                self._df[i] = col

    def merge(self, *Results):
        &#34;&#34;&#34;
        Merges any number of other qpcr.Results objects into this one.
        The source id of the results is added as column-name suffix. 

        Parameters
        ----------
        *Results
            An arbitrary number of qpcr.Results objects.

        &#34;&#34;&#34;
        new_df = self._df
        for R in Results: 
            R_df = R.get()

            # get only the delta-delta-Ct columns
            cols = [i for i in R_df.columns if i not in ref_cols]
            R_df = R_df[cols]


            # we merge the dataframes first without adding 
            # some new id suffix, only do so if this fails
            try: 
                new_df = pd.merge(new_df, R_df, 
                                    right_index = True, left_index = True, 
                                )
            except: 
                new_df = pd.merge(new_df, R_df, 
                                right_index = True, left_index = True, 
                                suffixes = [f&#34;_{self.id()}&#34;, f&#34;_{R.id()}&#34;]
                            )
        self._df = new_df

    def drop_cols(self, *cols):
        &#34;&#34;&#34;
        Drops all specified columns from the dataframes
        this is used for normaliser pre-processing.

        Parameters
        ----------
        *cols
            Any column names (as `str`) to be dropped.
            If no names are specified any/all `deltaCt` data-containing columns are dropped!
            If this is the case then the only columns retained are: `&#34;group&#34;, &#34;group_name&#34;, &#34;id&#34;, &#34;assay&#34;`.
        &#34;&#34;&#34;
        if cols == ():
            _to_drop = [c for c in self._df.columns if c not in [ &#34;group&#34;, &#34;group_name&#34;, raw_col_names[0], defaults.default_dataset_header ]]
        else:
            _to_drop = [c for c in cols if c in list(self._df.columns)]
        self._df = self._df.drop(columns = _to_drop)
        
    def rename_cols(self, cols:dict):
        &#34;&#34;&#34;
        Renames columns according to a dictionary as key -&gt; value.

        Parameters
        ----------
        cols : dict
            A dictionary specifying old column names (keys) and new colums names (values).
        &#34;&#34;&#34;
        self._df = self._df.rename(columns = cols)


    def stats(self, recompute = False) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        Computes summary statistis about the replicate groups: 
        `Mean`, `Median`, and `StDev` of all replicate groups, for all datasets (assays).
        
        Parameters
        ----------
        recompute : bool
            Statistics will only be once unless recompute is set to `True`.

        Returns
        -------
        stats_df : pd.DataFrame
            A new dataframe containing the computed statistics for each replicate group.

        &#34;&#34;&#34;
        default_dataset_header = defaults.default_dataset_header
        # if stats_df is already present, return but sorted according to assays, not groups (nicer for user to inspect)
        if self._stats_df is not None and not recompute:
            return self._stats_df.sort_values(default_dataset_header)
        elif recompute: 
            self._stats_results = {&#34;group&#34; : [], default_dataset_header : [], &#34;mean&#34; : [], &#34;stdev&#34; : [], &#34;median&#34; : []}
            self._stats_df = None

        # get groups and corresponding assay columns 
        groups = aux.sorted_set(list(self._df[&#34;group&#34;]))
        assays = [c for c in self._df.columns if c not in ref_cols]
     
        # compute stats for all replicates per group
        for group in groups:
            group_subset = self._df.query(f&#34;group == {group}&#34;)
            
            median = self._stat_var(group_subset, np.nanmedian)
            mean = self._stat_var(group_subset, np.nanmean)
            stdv = self._stat_var(group_subset, np.nanstd)
            self._add_stats(assays, group, median, mean, stdv)
            
        # add group names
        self._add_stats_names(assays)

        self._stats_df = pd.DataFrame(self._stats_results)
        return self._stats_df.sort_values(default_dataset_header)

    def save(self, path, df = True, stats = True):
        &#34;&#34;&#34;
        Saves a csv file for each specified type of results.

        Parameters
        ----------
        path : str
            Path has to be a filepath if only one type of results shall be saved (i.e. either `df` or `stats`), 
            otherwise a path to the directory where both `df` and `stats` shall be saved.
        
        df : bool
            Save the results dataframe containing all replicate values (the full results).
            Default is `df = True`.
        
        stats : bool
            Save the results dataframe containing summary statistics for all replicate groups.
            Default is `stats = True`.
        
        &#34;&#34;&#34;
        if df and stats and not os.path.isdir(path):
            aw.HardWarning(&#34;Results:save_need_dir&#34;)

        if df:
            # in case of raw results export we don&#39;t need the &#34;assay&#34; column as all 
            # assays are stored as separate columns anyaway, so it doesn&#39;t store any useful data
            _df = self._df
            if &#34;assay&#34; in _df.columns: _df = self._df.drop( columns = [&#34;assay&#34;] )
            self._save_single(path, _df, &#34;_df&#34;)
        if stats:
            # compute stats if none have been computed yet...
            if self._stats_df is None:
                self.stats()
            self._save_single(path, self._stats_df, &#34;_stats&#34;)

    def drop_rel(self):
        &#34;&#34;&#34;
        Crops the `X_rel_Y` column-names of Delta-Delta-Ct results to just `X`.
        I.e. reduces back to the assay-of-interest name only.
        &#34;&#34;&#34;
        colnames = self._df.columns
        to_change = {i : i.split(&#34;_rel_&#34;)[0] for i in colnames if &#34;_rel_&#34; in i }
        self.rename_cols(to_change)

        # also recompute the stats df with new names...
        if self._stats_df is not None: 
            self.stats(recompute = True)

    def split(self, reset_names = False, drop_rel = True):
        &#34;&#34;&#34;
        Splits the stored results dataframe into separate qpcr.Results objects containing only a signle deltaCt column each.

        Parameters
        ----------
        reset_names : bool
            Resets the deltaCt column-name from `&#34;X_rel_Y&#34;` to just `&#34;dCt&#34;`.

        drop_rel : bool
            Crops `&#34;X_rel_Y&#34;` deltaCt column-names to just `&#34;X&#34;`. 

        Returns 
        -------
        objects : list
            A list of qpcr.Results objects containing only a single dCt column each (retaining group columns etc.)
        &#34;&#34;&#34;
        shared_columns = [i for i in self._df.columns if i in ref_cols]
        dct_columns = [i for i in self._df.columns if i not in ref_cols]
        
        dfs = [self._df[shared_columns + [i]] for i in dct_columns]
        objects = [Results() for i in dfs]

        for o, df, dct_col in zip(objects, dfs, dct_columns): 
            o._df = df
            if reset_names:
                o.rename_cols({dct_col : &#34;dCt&#34;})
            if drop_rel: 
                o.drop_rel()

            o.id(dct_col)
        
        return objects


    def preview( self, kind : str = &#34;AssayBars&#34;, mode : str = &#34;static&#34;, **kwargs ):
        &#34;&#34;&#34;
        A shortcut to call on a `qpcr.Plotters.PreviewResults` wrapper to visualise 
        the results.

        Parameters
        ----------
        kind : str
            The kind of Plotter to call. This can be any of the four wrapped 
            Plotters, e.g. `kind = &#34;GroupBars&#34;`.
        mode : str
            The plotting mode. May be either &#34;static&#34; (matplotlib) or &#34;interactive&#34; (plotly).

        Returns
        -------
        fig : plt.figure or plotly.figure
            The figure generated by `PreviewResults`.
        &#34;&#34;&#34;
        preview_results = Plotters.PreviewResults( mode = mode, kind = kind )
        preview_results.params( **kwargs )
        preview_results.link( self )
        fig = preview_results.plot()
        return fig 

    def _save_single(self, path, src, suffix=&#34;&#34;):
        &#34;&#34;&#34;
        Saves either self._df or self._stats_df to a csv file based on a path
        (path can be either filename or directory)
        &#34;&#34;&#34;
        filename = path if not os.path.isdir(path) else os.path.join(path, f&#34;rel_{self.id()}{suffix}.csv&#34;)
        src.to_csv(filename, index = False)
        
    def _drop_setup_cols(self):
        &#34;&#34;&#34;
        Removes unnnecessary columns from the df during self._df setup with link()
        &#34;&#34;&#34;
        # drop the Ct columns
        relcols = [i for i in self._df.columns if &#34;rel_&#34; in i]
        self.drop_cols(
                        raw_col_names[1], &#34;dCt&#34;, *relcols
                    )



    def _add_stats_names(self, samples):
        &#34;&#34;&#34;
        Adds a group_name column to self._stats_result with appropriate
        repetition of group_names for each group of replicates...
        &#34;&#34;&#34;
        self._stats_results[&#34;group_name&#34;] = []
        group_names = aux.sorted_set(list(self._df[&#34;group_name&#34;]))
        for group_name in group_names:
            self._stats_results[&#34;group_name&#34;].extend([group_name] * len(samples))

    def _add_stats(self, samples, group, median, mean, stdv):
        &#34;&#34;&#34;
        Adds new summary entries to self._stats_results
        &#34;&#34;&#34;
        self._stats_results[&#34;group&#34;].extend([group] * len(samples))
        self._stats_results[&#34;assay&#34;].extend(samples)
        self._stats_results[&#34;median&#34;].extend(median)
        self._stats_results[&#34;mean&#34;].extend(mean)
        self._stats_results[&#34;stdev&#34;].extend(stdv)


    def _stat_var(self, group_subset, func, **kwargs):
        &#34;&#34;&#34;
        Performs a function (like mean or stdv) over all rows
        and returns the result as list with a float for each column in the df
        any function can be passed as long as it works with an iterable
        &#34;&#34;&#34;
        # ignore group and group_name columns
        ignore = [raw_col_names[0], &#34;group&#34;, &#34;group_name&#34;, &#34;assay&#34;]
        all_cols = [g for g in group_subset.columns if g not in ignore]
        tmp = group_subset[all_cols]
        # compute stats based on func
        stats = []
        for col in tmp.columns:
            try: 
                stat = func(tmp[col], **kwargs)
            except: 
                stat = np.nan
            stats.append(stat)
        return stats
        

class Analyser(aux._ID):
    &#34;&#34;&#34;
    Performs Single Delta-Ct (first normalisation 
    within dataset against the `anchor`) 
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._Assay = None

        # default settings
        self._anchor = &#34;first&#34;
        self._ref_group = 0
        self._ref_group_col = &#34;group&#34; # used in case of &#34;mean&#34; anchor where the ref_group must be located either from a numeric (group) or string (group_name) id
        
        self._eff_src = self._Assay         # By default use the efficencies stored in the Assay directly 
                                            # this is set to self if self.efficiency() is called...
        self._efficiency = 1                # the formal effiency in percent
        self._eff = 2 * self._efficiency    # the actual doubplciation factor used for calculation
        self._deltaCt_function = self._get_deltaCt_function(exp = True)

    def get(self):
        &#34;&#34;&#34;
        Returns 
        -------
        Assay : qpcr.Assay
            The analysed `qpcr.Assay` object that contains now deltaCT values.
        &#34;&#34;&#34;
        return self._Assay

    def link(self, Assay:Assay):
        &#34;&#34;&#34;
        Links a `qpcr.Assay` object to the Analyser.
       
        Parameters
        ----------
        Assay : qpcr.Assay
            A `qpcr.Assay` object containing data.
        
        &#34;&#34;&#34;
        self._Assay = Assay

        # check if no efficiency was specifically set for the Analyser
        # and if not so, use the Assay&#39;s efficiency...
        if self._eff_src is not self: 
            self._eff_src = self._Assay

    def pipe(self, Assay:Assay, **kwargs) -&gt; Results:
        &#34;&#34;&#34;
        A quick one-step implementation of link + DeltaCt.

        Note
        ----
        This is the suggested application of the `qpcr.Analyser`.


        Parameters
        ----------
        Assay : qpcr.Assay
            A `qpcr.Assay` object to be linked to the Analyser for DeltaCt computation.
        
        **kwargs
            Any additional keyword arguments to be passed to the `DeltaCt()` method.

        Returns 
        -------
        assay : qpcr.Assay
            The same `qpcr.Assay` with computed Delta-Ct values. 

        &#34;&#34;&#34;
        self.link(Assay)
        self.DeltaCt(**kwargs)
        assay = self.get()
        return assay

    def efficiency(self, e:float = None):
        &#34;&#34;&#34;
        Sets an efficiency factor for externally calculated qPCR amplification efficiency.
        By default `efficiency = 1` (100%) is assumed.

        Note
        ----
        By default the efficiency is now (`qpcr 3.2.0`) handled by the `qpcr.Assay` objects directly.
        The Analyser will directly read the efficiency from the `qpcr.Assay`s. However,
        it is still possible to set the efficiency via the `qpcr.Analyser` in this fashion.

        Parameters
        ----------
        e : float
            An amplification efficiency factor. Default is `e = 1`, 
            which is then treated as `eff = 2 * e`, so `e = 1` corresponds to true duplication
            each cycle.

        Returns
        -------
        efficiency : float
            The current efficiency used.

        &#34;&#34;&#34;
        # deprecation warning
        aw.SoftWarning( &#34;blank&#34;, msg = &#34;The use of efficiency() from the Analyser is deprecated and will be dropped in a future version! Please, set efficiencies directly in the Assay.&#34; )
        if isinstance(e, (int, float)):
            self._efficiency = float( e )
            self._eff = 2 * self._efficiency
            # and update the efficiency source 
            # to self instead of the assay...
            self._eff_src = self 
        return self._efficiency

    def anchor(self, anchor : (str or float or function) = None, group : (int or str) = 0):
        &#34;&#34;&#34;
        Sets the anchor for DeltaCt for internal normalisation.

        Parameters
        ----------
        anchor : str or float or function
            The internal anchor for normalisation.
            This can be either `&#34;first&#34;` (default, the very first dataset entry),
            `&#34;mean&#34;` (mean of the reference group), 
            `&#34;grouped&#34;` (first entry for each replicate group), 
            any specified numeric value (as `float`), 
            or a `function` that will calculate the anchor and returns a single numeric value. 
            If you wish to use a function to compute the anchor, you can access the dataframe stored by the `qpcr.Assay` that is being analysed through the 
            `data` argument. `data` will be automatically forwarded to your custom anchor-function, unless you specify it directly. Please, make sure your function can handle
            `**kwargs` because any kwargs supplied during `DeltaCt()`-calling will be passed per default to both the anchor-function and DeltaCt-function. 
        group : int or str
            The reference group identifier. This can be either the numeric identifier or the `group_name`. This is only used for `anchor = &#34;mean&#34;`. 
            By default the first group is assumed. 
        
        Returns
        -------
        anchor 
            The currently selected `anchor`.
        ref_group
            The current reference group.
        &#34;&#34;&#34;
        if anchor is not None:
            self._anchor = anchor
        if group != self._ref_group:
            self._ref_group = group
        
        # update column where to search for ref_group identifier
        if isinstance(group, str):
            self._ref_group_col = &#34;group_name&#34;
        else: 
            self._ref_group_col = &#34;group&#34;
        
        return self._anchor, self._ref_group


    def func(self, f:(str or function)):
        &#34;&#34;&#34;
        Sets the function to be used for DeltaCt (optional)

        Parameters
        ----------
        f : str or function
            The function to be used for DeltaCt computation. Pre-defined functions are 
            either `&#34;exponential&#34;` (which uses  `dCt = eff ** ( -(s-r) )`, default), or `&#34;linear&#34;` 
            (uses `dCt = s-r`), where `s` is any replicate entry in the dataframe and `r` is the anchor. `eff = 2 * efficiency` is the 
            numeric duplication factor (default assumed `efficiency = 1`).
            It is also possible to assign any defined function that accepts one `float` Ct value `s` (1st!) and anchor `r` value (2nd!), 
            alongside any `kwargs` (which will be forwarded from DeltaCt()...). It must return also a single `float`. 
        &#34;&#34;&#34;
        if f in [&#34;exponential&#34;, &#34;linear&#34;]:
            f = True if f == &#34;exponential&#34; else False
            self._deltaCt_function = self._get_deltaCt_function(f)
        elif type(f) == type(aux.fileID):
            self._deltaCt_function = f
        else:
            aw.HardWarning(&#34;Analyser:cannot_set_func&#34;, func = f)

    def DeltaCt(self, **kwargs):
        &#34;&#34;&#34;
        Calculates Delta-Ct for all groups within the dataframe.
        Any specifics such as `anchor` or `func` must have already been 
        set using the respective methods prior to calling `DeltaCt()`!

        Parameters
        ----------
        **kwargs
            Any additional keyword arguments that a custom DeltaCt function may require.
        &#34;&#34;&#34;

        predefined = {
                        &#34;first&#34;     : self._DeltaCt_first_anchored,
                        &#34;grouped&#34;   : self._DeltaCt_grouped_anchored,
                        &#34;mean&#34;      : self._DeltaCt_mean_anchored,
                    }

        if self._anchor in predefined.keys():
        
            deltaCt_func = predefined[ self._anchor ]
            deltaCt_func(
                                            self._deltaCt_function, 
                                            **kwargs
                                    )
        
        elif isinstance(self._anchor, (float, int)): 
            self._DeltaCt_externally_anchored(
                                                self._anchor, 
                                                self._deltaCt_function, 
                                                **kwargs
                                            )
        elif type(self._anchor) == type(aux.fileID):
            self._DeltaCt_function_anchor(
                                            self._anchor, 
                                            self._deltaCt_function, 
                                            **kwargs
                                        )

    def _DeltaCt_function_anchor(self, anchor_function, deltaCt_function, **kwargs):
        &#34;&#34;&#34;
        Performs DeltaCt using a function as anchor
        &#34;&#34;&#34;
        df = self._Assay.get()
        # update a &#34;data&#34; argument into the 
        # kwargs for the anchor_function
        if &#34;data&#34; not in kwargs.keys(): 
            kwargs.update(dict(data = df))
        anchor = anchor_function(**kwargs)

        # apply deltaCt_function
        Ct = raw_col_names[1]
        dCt = df[Ct].apply(deltaCt_function, r = anchor, **kwargs)
        dCt.name = &#34;dCt&#34;
        # store results
        self._Assay.add_dCt(dCt)

    def _DeltaCt_mean_anchored(self, deltaCt_function, **kwargs):
        &#34;&#34;&#34;
        Performs DeltaCt using the mean of the reference group as anchor
        &#34;&#34;&#34;
        df = self._Assay.get()

        # get  reference group
        ref_query = &#34;{} == {}&#34; if isinstance(self._ref_group, int) else &#34;{} == &#39;{}&#39;&#34; 
        ref = df.query(
                        ref_query.format(  self._ref_group_col, self._ref_group  )
                    )
        # get Ct values from ref group and make anchor
        ref = ref[raw_col_names[1]]
        anchor = ref.mean()
        
        # apply DeltaCt function
        Ct = raw_col_names[1]
        dCt = df[Ct].apply(deltaCt_function, r = anchor, **kwargs)
        dCt.name = &#34;dCt&#34;
        # store results
        self._Assay.add_dCt(dCt)


    def _DeltaCt_externally_anchored(self, anchor:float, deltaCt_function, **kwargs):
        &#34;&#34;&#34;
        Performs DeltaCt using a specified anchor
        &#34;&#34;&#34;
        # get Ct column label 
        Ct = raw_col_names[1]
        df = self._Assay.get()

        # apply DeltaCt function
        dCt = df[Ct].apply(deltaCt_function, r = anchor, **kwargs)
        dCt.name = &#34;dCt&#34;
        # store results
        self._Assay.add_dCt(dCt)


    def _DeltaCt_grouped_anchored(self, deltaCt_function, **kwargs):
        &#34;&#34;&#34;
        Performs DeltaCt using the first entry of each group as anchor
        &#34;&#34;&#34;
        # get set of sample groups and dataset
        groups = self._Assay.groups()
        df = self._Assay.get()

        # get Ct column label
        Ct = raw_col_names[1]

        dCt = pd.Series()
        for group in groups: 
            group_subset = df.query(f&#34;group == {group}&#34;).reset_index(drop = True)
            anchor = group_subset[Ct][0]
            delta_cts = group_subset[Ct].apply(deltaCt_function, r = anchor, **kwargs)
            dCt = dCt.append(delta_cts).reset_index(drop = True)
        
        dCt.name = &#34;dCt&#34;

        # store results
        self._Assay.add_dCt(dCt)

    def _DeltaCt_first_anchored(self, deltaCt_function, **kwargs):
        &#34;&#34;&#34;
        Performs DeltaCt using the very first entry of the dataset as anchor
        &#34;&#34;&#34;
        # get Ct column label
        Ct = raw_col_names[1]
        
        # get first available entry
        # we do this instead of just 0 
        # because the truly first entry 
        # might have been filtered out 
        df = self._Assay.get()
        first = list(df.index)[0]

        # get anchor
        anchor = df[Ct][ first ]

        # apply DeltaCt function
        dCt = df[Ct].apply(deltaCt_function, r = anchor, **kwargs)
        dCt.name = &#34;dCt&#34;
        # store results
        self._Assay.add_dCt(dCt)

    def _exp_DCt(self, s, r, **kwargs):
        &#34;&#34;&#34;
        Calculates deltaCt exponentially
        &#34;&#34;&#34;
        factor = s - r 
        return self._eff_src._eff **(-factor)

    def _simple_DCt(self, s, r, **kwargs):
        &#34;&#34;&#34;
        Calculates deltaCt linearly
        &#34;&#34;&#34;
        return s - r

    def _get_deltaCt_function(self, exp):
        &#34;&#34;&#34;
        Returns the function to be used for DeltaCt based on 
        whether or not exponential shall be used.
        &#34;&#34;&#34;
        if exp == True:
            dCt = self._exp_DCt
        else:
            dCt = self._simple_DCt
        return dCt


class Normaliser(aux._ID):
    &#34;&#34;&#34;
    Handles the second step in Delta-Delta-Ct (normalisation against normaliser assays).
    
    Note
    -----
    This requires that all have been analysed in the same way before!
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()

        self._Normalisers = []
        self._Assays = []
        
        self._Results = Results()
        
        # the actually used normaliser 
        # which will be a pre-processed version
        # from all supplied normalsier assays
        # by default this will be the averaged version
        # of all normaliser assays (but this can be changed)  
        self._normaliser = None

        # setup defaults
        self._prep_func = self._preprocess_normalisers
        self._norm_func = self._divide_by_normaliser

        # store the state if a custom norm func was provided
        self._norm_func_is_set = False


    def clear(self):
        &#34;&#34;&#34;
        Will clear the presently stored results
        &#34;&#34;&#34;
        self._Results = Results()

    
    def prune(self, assays = True, normalisers = True, results = True):
        &#34;&#34;&#34;
        Will clear assays, normalisers, and/or results
        assays : bool
            Will clear any sample assays if True (default).
        
        results : bool
            Will clear any computed results if True (default).
        
        normalisers : bool
            Will clear any normalisers if True (default).
        &#34;&#34;&#34;
        if assays:
            self._Assays = []
        if normalisers: 
            self._Normalisers = []
        if results: 
            self.clear()


    def get(self, copy=False):
        &#34;&#34;&#34;
        Parameters
        ----------
        copy : bool
            Will return a deepcopy of the Results object if `copy = True` (default is `copy = False`).
        
        Returns
        -------
        Results : qpcr.Results
            A `qpcr.Results` object containing the normalised dataframe
        &#34;&#34;&#34;
        if copy: 
            return deepcopy(self._Results)
        return self._Results
    
    def link(self, assays:(list or tuple or Analyser) = None, normalisers:(list or tuple or Analyser) = None):
        &#34;&#34;&#34;
        Links either normalisers or assays-of-interest `qpcr.Assay` objects coming from the same `qpcr.Analyser`.

        Parameters
        ----------
        assays : list or tuple or qpcr.Analyser
            A list of `qpcr.Assay` objects coming from a `qpcr.Analyser` or the `qpcr.Analyser` itself. These assays will be normalised against a normaliser.
        
        normalisers : list or tuple or qpcr.Analyser
            A list of `qpcr.Assay` objects coming from a `qpcr.Analyser` or the `qpcr.Analyser` itself. These assays will be used as normalisers. These will be
            combined into one single pseudo-normaliser which will then be used to normalise the assays. The method of 
            combining the normalisers can be specified using the `qpcr.Normaliser.prep_func` method.
        &#34;&#34;&#34;
        # convert to lists (since the _link methods really want lists)
        if assays is not None and not isinstance(assays, (list, tuple)): 
            assays = [assays]
        self._link_assays(assays)

        if normalisers is not None and not isinstance(normalisers, (list, tuple)): 
            normalisers = [normalisers]
        self._link_normaliser(normalisers)
    
    def prep_func(self, f = None):
        &#34;&#34;&#34;
        Sets any defined function for combined normaliser pre-processing.
        If no `f` is provided, it returns the current `prep_func`.

        Parameters
        ----------
        f : function
            The function may accept one list of `qpcr.Assay` objects, and must return 
            either an `qpcr.Assay` object directly or a `pandas.Dataframe` (that will be migrated to an `qpcr.Assay`).
            The returned dataframe must contain a `&#34;dCt&#34;` column which stores the delta-Ct values ultimately used as 
            &#34;normaliser assay&#34;.  
        &#34;&#34;&#34;
        
        if aux.same_type(f, aux.fileID):
            self._prep_func = f
        elif f is None:
            return f
        else: 
            aw.HardWarning(&#34;Normaliser:cannot_set_prep_func&#34;, func = f)

    def norm_func(self, f = None):
        &#34;&#34;&#34;
        Sets any defined function to perform normalisation of assays against normalisers.
        If no `f` is provided, it returns the current `norm_func`.

        Parameters
        ----------
        f : function
            The function may accept two `qpcr.Assay` objects (named `assay` and `normaliser` which will be forwarded from the `qpcr.Normaliser`). 
            The function may also accept one `pandas.DataFrame` (named `df`) containing two numeric columns of delta-Ct values from a sample-assay (named &#34;s&#34;) and a normaliser-assay (named &#34;n&#34;),
            as well as a group identifier column (named &#34;group&#34;). 
            Whatever inputs it works with, it must return a named numeric `pandas.Series` of the same length as entries in the Assays&#39; dataframes. 
            
            ##### Note
            Support for the dataframe direct usage will be dropped at some point in the future. 

            By default `s/n` is used, where `s` is a column of sample-assay deltaCt values, 
            and `n` is the corresponding `&#34;dCt&#34;` column from the normaliser.

        &#34;&#34;&#34;
        if aux.same_type(f, aux.fileID):
            self._norm_func = f
            self._norm_func_is_set = True
        elif f is None:
            return f
        else: 
            aw.HardWarning(&#34;Normaliser:cannot_set_norm_func&#34;, func = f)

    def normalise(self, mode = &#34;pair-wise&#34;, **kwargs):
        &#34;&#34;&#34;
        Normalises all linked assays against the combined pseudo-normaliser 
        (by default, unless a custom `prep_func` has been specified), 
        and stores the results in a new Results object.

        Parameters
        ----------
        mode : str
            The normalisation mode to use. This can be either `pair-wise` (default), 
            or `combinatoric`, or `permutative`.
            `pair-wise` will normalise replicates only by their partner (i.e. first against first, 
            second by second, etc.). `combinatoric` will normalise all possible combinations of a replicate 
            with all partner replicates of the same group from a normaliser (i.e. first against first, then second, then third, etc.).
            This will generate `n^2` normalised Delta-Delta-Ct values, where `n` is the number of replicates in a group.
            `permutative` will scramble the normaliser replicates randomly and then normalise pair-wise. This mode supports 
            a parameter `k` which specifies the times this process should be repeated, thus generating `n * k` normalised Delta-Delta-Ct values.
            Also, through setting `replace = True` replacement may be allowed during normaliser scrambling.
            Note, this setting will be ignored if a custom `norm_func` is provided.

        **kwargs
            Any additional keyword arguments that may be passed to a custom 
            `norm_func` and `prep_func` (both will receive the kwargs!).
        &#34;&#34;&#34;
        if self._normaliser is None: 
            self._normaliser = self._prep_func(self._Normalisers, **kwargs)
            self._vet_normaliser()

        if self._Assays == [] or self._normaliser is None:
            aw.SoftWarning(&#34;Normaliser:no_data_yet&#34;)
            
        # check which kind of norm_func we should use
        if not self._norm_func_is_set:
            # pair-wise is already default so we don&#39;t check...
            if mode == &#34;combinatoric&#34;:
                self._norm_func = self._tile_normalise
                tiled = deepcopy(self._Assays[0])
                tiled.tile()
                self._Results.adopt_names( tiled )
                del tiled 
            elif mode == &#34;permutative&#34;:
                self._norm_func = self._permutate_normalise
                n = aux.from_kwargs(&#34;k&#34;, 1, kwargs)
                tiled = deepcopy(self._Assays[0])
                tiled.stack(n)
                self._Results.adopt_names( tiled )
                del tiled 
            elif mode == &#34;pair-wise&#34;:
                self._Results.adopt_names( self._Assays[0] )
        else:
            self._Results.adopt_names( self._Assays[0] )

        self._Results.drop_cols()

        # perform normalisation for each assay 
        for assay in self._Assays:

            # get data
            # assay_df = assay.get()

            # apply normalisation (delta-delta-Ct)
            normalised = self._norm_func_wrapper(
                                                    assay = assay, 
                                                    normaliser = self._normaliser, 
                                                    **kwargs
                                            )

            # # store results in _Results
            # self._store_to_Results(assay, normalised)

            # and store results also in the Assay itself
            assay.add_ddCt( self._normaliser.id(), normalised )

            # and store to results
            self._Results.add_ddCt(assay)

    def _vet_normaliser(self):
        &#34;&#34;&#34;
        Checks if the normaliser is already a qpcr.Assay object, and if not
        convert it to one. 
        &#34;&#34;&#34;
        if not isinstance(self._normaliser, Assay):
            tmp = Assay()
            tmp.adopt(self._normaliser)
            tmp.id(&#34;combined_normaliser&#34;)
            self._normaliser = tmp
            

    def _store_to_Results(self, assay, normalised):
        &#34;&#34;&#34;
        Stores computed Delta-delta-ct (normalisation)
        into the _Results object.
        &#34;&#34;&#34;
        column_name = f&#34;{assay.id()}_rel_{self._normaliser.id()}&#34;
        normalised = normalised.rename(column_name)
        self._Results.add(normalised)


    def _norm_func_wrapper(self, assay, normaliser, **kwargs):
        &#34;&#34;&#34;
        The wrapper that will apply the _norm_func to the sample and normaliser dataframes and return a pandas series of normalised values
        &#34;&#34;&#34;
        # for double normalised we want the same columns as dct and norm...

        # FUTURE DROP HERE
        # In the future we will not be creating the tmp_df 
        # directly anymore, but intead will only pass assay and normaliser. 

        sample_dCt = assay.dCt()
        groups = assay.groups( as_set = False )
        norm_dCt = normaliser.dCt()

        tmp_df = pd.DataFrame( dict( group = groups, s = sample_dCt, n = norm_dCt )  )

        results = self._norm_func(df = tmp_df, assay = assay, normaliser = normaliser, **kwargs)

        # this is the old call from before factoring out to Assays 
        # dCt_col, norm_col = self._prep_columns(sample_assay, dCt_col, norm_col)

        # tmp_df = normaliser.join(sample_assay, lsuffix=&#34;_s&#34;)
        # # tmp_df = sample_assay.join(normaliser, rsuffix = &#34;_n&#34;)
        # results = self._norm_func(tmp_df[[dCt_col, norm_col]], **kwargs)
        return results

    def _tile_normalise(self, assay, normaliser, **kwargs):
        &#34;&#34;&#34;
        Normalises assays and normalisers group wise, iteratively normalising
        each individual replicate against all replicates from the normaliser.
        This generates `n**2` normalised Delta-Delta-Ct values where `n` is the
        group size. 
        &#34;&#34;&#34;

        # get the untiled data
        adf = assay.get()
        groups = assay.groups()
        ndf = normaliser.get()
        
        # get the column to draw data from
        col = aux.from_kwargs(&#34;col&#34;, &#34;dCt&#34;, kwargs)

        # tile the assay
        assay.tile() 

        # generate results array
        ddCts = np.zeros( len( assay.get() ) )
       
        # now compute ddCt
        idx = 0
        for group in groups: 

            a_dCt = adf.query( f&#34;group == {group}&#34; )[ col ].to_numpy()
            n_dCt = ndf.query( f&#34;group == {group}&#34; )[ col ].to_numpy()
            
            for a in a_dCt:
                for n in n_dCt:

                    r = a / n
                    
                    try: 
                        ddCts[idx] = r 
                    except: 
                        break 

                    idx += 1

        ddCts = pd.Series( ddCts )       
        ddCts.name = &#34;ddCt&#34;
        return ddCts
        
    
    def _permutate_normalise(self, assay, normaliser, **kwargs):
        &#34;&#34;&#34;
        Scrambles randomly the normaliser&#39;s replicate values group-wise 
        &#34;&#34;&#34;
        # get the untiled data
        adf = assay.get()
        groups = assay.groups()
        ndf = normaliser.get()
        
        # get the column to draw data from
        col = aux.from_kwargs(&#34;col&#34;, &#34;dCt&#34;, kwargs)

        # stack the assay
        # get the number of permutations to perform
        n = aux.from_kwargs(&#34;k&#34;, 1, kwargs)  
        assay.stack(n) 

        # get replace argument for random choice
        replace = aux.from_kwargs(&#34;replace&#34;, False, kwargs)

        # generate results array
        ddCts = np.zeros( len( assay.get() ) )
       
        # now compute ddCt
        idx = 0
        for group in groups: 
            for i in range(n):
                a_dCt = adf.query( f&#34;group == {group}&#34; )[ col ].to_numpy()
                n_dCt = ndf.query( f&#34;group == {group}&#34; )[ col ].to_numpy()
                
                # randomly scramble the normaliser replicates
                np.random.seed( defaults.default_seed )
                if replace:
                    # in case of replacement, we generate a normal 
                    # distribution from the replicate values and weigh
                    # the random.choice with the given probabilities of the values 
                    # being chosen. Since the probabilities do not themselves correspond to 1
                    # we normalise against the limited available subset to generate a probabilities
                    # array that sums up to 1.
                    mu, sd = stats.norm.fit(n_dCt)
                    probs = stats.norm.pdf(n_dCt, loc = mu, scale = sd)
                    probs = probs / np.sum(probs)
                    n_dCt = np.random.choice( n_dCt, size = n_dCt.size, replace = replace, p = probs )
                else:
                    n_dCt = np.random.choice( n_dCt, size = n_dCt.size, replace = replace )
                length = a_dCt.size
                r = a_dCt / n_dCt
                        
                try: 
                    ddCts[idx : idx + length] = r 
                except: 
                    break 

                idx += length

        ddCts = pd.Series( ddCts )       
        ddCts.name = &#34;ddCt&#34;
        return ddCts

    def _divide_by_normaliser(self, df, **kwargs):
        &#34;&#34;&#34;
        Performs normalisation of sample s against normaliser n
        s and n are specified as two pandas dataframe columns
        Note, that the dataframe must ONLY contain these two columns, first the dCt sample, then the normaliser!
        (default _norm_func)
        &#34;&#34;&#34;
        s, n = df[&#34;s&#34;], df[&#34;n&#34;]
        # this is the old call from before factoring out to Assays
        # dCt_col, norm_col = df.columns
        # s, n = df[dCt_col], df[norm_col]
        return s / n

    def _link_assays(self, assays):
        &#34;&#34;&#34;
        Links any provided assays and checks their datatype in the process...
        &#34;&#34;&#34;
        if assays is not None:
            for sample in assays: 
                if aux.same_type(sample, Assay()):
                    self._Assays.append(sample)

                elif aux.same_type(sample, Analyser()):
                    self._Assays.append(sample.get())
                
                else: 
                    aw.SoftWarning(&#34;Normaliser:unknown_data&#34;, s = sample)
                
    def _link_normaliser(self, normalisers):
        &#34;&#34;&#34;
        Checks if normaliser is provided and has proper datatype to be added...
        &#34;&#34;&#34;
        if normalisers is not None:
            for normaliser in normalisers:

                if aux.same_type(normaliser, Assay()):
                    self._Normalisers.append(normaliser)

                elif aux.same_type(normaliser, Analyser()):
                    self._Normalisers.append(normaliser.get())

                else: 
                    aw.SoftWarning(&#34;Normaliser:norm_unknown_data&#34;, s = normaliser)

    def _preprocess_normalisers(self, *args, **kwargs):
        &#34;&#34;&#34;
        Averages the provided normalisers row-wise for all normalisers into a 
        single combined normaliser, that will be stored as a new Assay object.
        &#34;&#34;&#34;

        # initialise new Results to store the dCt values form all normalisers
        combined = Results()
        
        # setup names using the first normaliser
        combined.adopt_names(self._Normalisers[0])
        combined.drop_cols(&#34;dCt&#34;)
        combined.adopt_id(self._Normalisers[0])

        # now add all dCt columns from all normalisers
        for norm in self._Normalisers:
            combined.add_dCt(norm)
        
        # remove the non-dCt columns as they would interfere with
        # pre-processing. But we keep the &#34;group&#34; column because some
        # custom prep_func may want to use the group references.
        # i.e. we just replace any &#34;Ct&#34; columns that may have smuggled in if 
        # a non-default process is used for assembly
        combined.drop_cols( raw_col_names[1] )

        # now generate the combined normaliser
        combined_normaliser = self._average(combined)
        combined_normaliser = combined_normaliser.rename(&#34;dCt&#34;)
        combined.add(combined_normaliser)
        
        # now assemble the normaliser into a qpcr.Assay
        combined = combined.get()
        normaliser = Assay()
        normaliser.adopt(combined)
        normaliser.adopt_id(self._Normalisers[0])


        self._normaliser = normaliser  
        if len(self._Normalisers) &gt; 1:
            self._update_combined_id()
  
        # forward combined_id to self and _Results 
        self.adopt_id(self._normaliser)
        self._Results.adopt_id(self._normaliser)

        return self._normaliser

    def _update_combined_id(self):
        &#34;&#34;&#34;
        Generates a new id based on all normaliser ids,
        joining them as a+b+c,...
        &#34;&#34;&#34;
        ids = [N.id() for N in self._Normalisers]
        ids = &#34;+&#34;.join(ids)
        self._normaliser.id_reset()
        self._normaliser.id(ids)
        

    def _average(self, combined):
        &#34;&#34;&#34;
        Averages row-wise all Normaliser entries and 
        generates a series of their per-row means
        (default preprocess_normalisers function)
        &#34;&#34;&#34;
        tmp_df = combined.get()

        # drop group as it is a numeric column 
        # and would otherwise skew the average
        if &#34;group&#34; in tmp_df.columns:
            tmp_df = tmp_df.drop(columns = [&#34;group&#34;]) 
        tmp_df = tmp_df.mean(axis = 1, numeric_only = True)

        return tmp_df


# FUTURE MOVE HERE
# NOTE: This should be moved to the Curves submodule once it will be made in a future
#       version to work directly with absorption and fluorescense curves.
class _EfficiencyCurve(aux._ID):
    &#34;&#34;&#34;
    A helper class that will handle dilutions, ct values and the linreg model
    when newly computing efficiencies from assays.
    &#34;&#34;&#34;
    def __init__(self, dilutions, ct_values, model, efficiency):
        super().__init__()
        self._dilutions = dilutions
        self._ct_values = ct_values
        self._model = model
        self._efficiency = efficiency
    
    def values(self):
        &#34;&#34;&#34;
        Returns
        ------
        dilutions : np.ndarray
            The dilutions (x-values) used for efficiency calculation.
        ct_value : np.ndarray
            The underlying Ct values (y-values) used for efficiency calculation.
        &#34;&#34;&#34;
        return self._dilutions, self._ct_values
    
    def model(self):
        &#34;&#34;&#34;
        Returns
        -------
        model : stats.LinregressResult
            The linear regression model used for efficiency calculation.
        &#34;&#34;&#34;
        return self._model
    
    def efficiency(self):
        &#34;&#34;&#34;
        Returns
        -------
        eff : float
            The efficiency calculated from the stored data.
        &#34;&#34;&#34;
        return self._efficiency
    

class Calibrator(aux._ID):
    &#34;&#34;&#34;
    Calculates qPCR primer efficiency based on a dilution series.
    The dilution series may either be represented as an entire assay
    or as a subset of groups within an assay denoted as `calibrator : {some_name}`.
    In this mode, calibrator replicates will be removed after calibration is done.

    It is possible to specify the dilution steps directly in the groupnames as:
    `calibrator: {some_name}: dil` where `dil` is the inverse dilution step, e.g.
    `calibrator: my_sample: 2` for a `1 : 2` dilution or `calibrator: my_sample: 100`
    for a `1 : 100`. Note, this will have to be present in **each** groupname! 

    Alternatively, if no dilution is specified in the groupnames or they cannot be inferred
    for some other reason, it is possible to supply a dilution step via 
    the `qpcr.Calibrator.dilution` method. 
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._eff_dict = {}             # stores the efficiencies as assay_id : efficiency
        self._computed_values = {}      # stores newly computed efficiency data as:
                                        # assay_id :  _EfficiencyCurve(...)
                                        #             The _EfficiencyCurve object stores:
                                        #             - dilutions
                                        #             - Ct values
                                        #             - the Linreg model
        self._dilution_step = None      # the dilution step(s) used 
        self._manual_dilution_set = False 
        self._orig_dilution = None 
        
        self._loaded_file = None


    def save( self, filename : str = None , mode : str = &#34;write&#34; ):
        &#34;&#34;&#34;
        Saves the calculated efficiencies to a `csv` file.

        Parameters
        -------
        filename : str
            The filepath in which to store the efficiencies. If a file
            was already loaded then by default the same file will be 
            used to save values again.
        
        mode : str
            Can be either `&#34;write&#34;` to fully overwrite an existing file,
            with the newly computed data, or `&#34;append&#34;` to only add newly 
            computed efficiencies.
        &#34;&#34;&#34;

        if filename is None and self._loaded_file is not None: 
            filename = self._loaded_file

        if mode == &#34;write&#34;:
            self._save( filename, self._eff_dict )

        elif mode == &#34;append&#34;:
            current = self._load(filename)
            new = { **current, **self._eff_dict }
            self._save( filename, new )

        else: 
            aw.SoftWarning(&#34;Calibrator:unknown_savemode&#34;, mode = mode )

    def load(self, filename, merge : bool = True, supersede : bool = False ):
        &#34;&#34;&#34;
        Loads a `csv` file of previously computed efficiencies.

        Parameters
        -------
        filename : str
            The filepath to load efficiencies from.
        merge : bool
            In case efficiencies are already loaded, merge the
            new and existing ones. If `False` the current ones will
            be replaced completely.
        supersede : bool 
            In case efficiencies of the same assay are already loaded
            they will be overwritten by the newly incoming ones if `supersede = True`.
        &#34;&#34;&#34;
        try: 
            current = self._load(filename)
            if merge:
                current = { **self._eff_dict, **current } if supersede else { **current, **self._eff_dict }
            self.adopt( current )
            self._loaded_file = filename
            return current
        except: 
            aw.HardWarning(&#34;Calibrator:unknown_filetype&#34;, filename = filename )

    def get( self, which = &#34;efficiencies&#34; ):
        &#34;&#34;&#34;
        Returns
        -------
        dict
            Either the stored efficiencies (if 
            `which = &#34;efficiencies&#34;`) or 
            the computed values of newly computed
            efficiencies (if `which = &#34;values&#34;`).
        &#34;&#34;&#34;
        if which == &#34;efficiencies&#34;:
            return self.efficiencies()
        elif which == &#34;values&#34;:
            return self.computed_values()
       
    def efficiencies( self ):
        &#34;&#34;&#34;
        Returns
        ------
        dict
            The currently stored efficienies.
        &#34;&#34;&#34;
        return self._eff_dict
    
    def computed_values( self ):
        &#34;&#34;&#34;
        Returns
        ------
        dict
            The currently stored values from newly
            computed efficiencies.
        &#34;&#34;&#34; 
        return self._computed_values
    
    def merge( self, *filenames, outfile = None, adopt = True ):
        &#34;&#34;&#34;
        Merges multiple efficiency files together into a single one.

        Parameters
        -------
        filenames : iterable
            Filepaths to load data from which should be merged together.
        
        outfile : str 
            The filepath in which to store the merged efficiencies.
            Not saved if set to `None`.
        
        adopt : bool
            Will adopt the merged dictionary as its own if `True` (default).

        Returns
        -------
        all_effiencies : dict
            The merged dictionary of all efficiencies from all files.
        &#34;&#34;&#34;
        all_efficiencies = {}
        for filename in filenames: 
            new = self._load( filename )
            all_efficiencies = { **all_efficiencies, **new }

        if outfile is not None: 
            self._save( outfile, all_efficiencies )

        if adopt:
            self.adopt( all_efficiencies ) 

        return all_efficiencies
    
    def reset( self ):
        &#34;&#34;&#34;
        Resets the Calibrator to initial settings. This will 
        clear all stored efficiency values and computed data!
        &#34;&#34;&#34;
        self.__init__()

    def clear( self ):
        &#34;&#34;&#34;
        Will clear all stored efficiency values and computed data.
        &#34;&#34;&#34;
        self._eff_dict = {}
        self._computed_values = {}

    def adopt( self, effs : dict ):
        &#34;&#34;&#34;
        Adopts an externally generated dictionary of `assay : efficiency`
        structure as its own.

        Parameters
        -------
        effs : dict
            A dictionary where keys are Assay Ids (`str`) 
            and values are `float` efficiencies.
        &#34;&#34;&#34;
        if aux.same_type( effs, {} ):
            self._eff_dict = effs
        else: 
            aw.SoftWarning(&#34;Calibrator:cannot_adopt&#34;, effs = effs, eff_type = type(effs).__name__ )
    
    def dilution( self, step : float or np.ndarray or tuple = None ):
        &#34;&#34;&#34;
        Gets or sets the dilution steps used. This must be a `float` fraction
        e.g. `0.5` for a `1 : 2` dilution series or `0.1` for a `1 : 10` series etc.
        If there are multiple steps because there is a gap in the dilution series. It is 
        necessary to supply a step for each group individually e.g. `[1,0.5,0.25,0.0625,0.03125]`.
        if there are 5 dilution steps (originally six but 0.125 was discarded).

        Note, both of the above also work with the inverse dilutions e.g. `2` or `[1,2,4,16,32]`.
        
        By default the `qpcr.Calibrator` tries to infer the dilutions automatically.
        This only works, however, if the calibrator groupnames specify `calibrator: {some name} : dil` where
        `dil` is the inverse dilution step (e.g. `calibrator: my_sample: 2` for a `1 : 2` dilution). Note,
        it is important that the dilution step is given as the inverse (i.e. *not* as `1:2 or 1/2` or something else! )

        Parameters
        ----------
        step : float or np.ndarray
            The dilution step used.  

        Returns
        -------
        dilution : float or np.ndarray
            The currently used dilution step.
        &#34;&#34;&#34;
        
        dilution = self._dilution(step)
        self._orig_dilution = self._dilution_step

        # if the dilution is now set to a valid
        # value we set the _manual_dilution_set check to True
        if dilution is not None:
            self._manual_dilution_set = True 
        return dilution

        
    def pipe( self, assay : Assay, remove_calibrators : bool = True, ignore_uncalibrated : bool = False ):
        &#34;&#34;&#34;
        A wrapper for calibrate / assign.

        This method will first try to assign pre-computed efficiencies
        and if no matching ones are found it will try to calculate a new efficiency
        from the assay. 

        Parameters
        ----------
        assay : qpcr.Assay
            A `qpcr.Assay` object.
        remove_calibrators : bool
            If calibrators are present in the assay alongside other groups, 
            remove the calibrator replicates after assignment or efficiency calculation. 
        ignore_uncalibrated : bool
            If `True` assays that could neither be newly calibrated nor be assigned an existing
            efficiency will be ignored. Otherwise, and error will be raised. 

        Returns
        -------
        assay : qpcr.Assay
            The now calibrated `qpcr.Assay`.
        &#34;&#34;&#34;
        if self._eff_dict != {}:
            # first try to assign (will leave the assay unchanged if nothing is found)
            eff = self._get_efficiency( assay )
            if eff is not None: 
                assay = self.assign( assay, remove_calibrators = remove_calibrators )
            else: 
                try:
                    assay = self.calibrate( assay, remove_calibrators = remove_calibrators )
                except: 
                    if not ignore_uncalibrated:
                        aw.HardWarning(&#34;Calibrator:cannot_process_assay&#34;, id = assay.id() )
                    else: 
                        aw.SoftWarning(&#34;Calibrator:cannot_process_assay&#34;, id = assay.id() )
        else:
            try: 
                assay = self.calibrate( assay, remove_calibrators = remove_calibrators )
            except:
                if not ignore_uncalibrated:
                    aw.HardWarning(&#34;Calibrator:cannot_process_assay&#34;, id = assay.id() )
                else: 
                    aw.SoftWarning(&#34;Calibrator:cannot_process_assay&#34;, id = assay.id() )
        return assay

    def calibrate( self, assay : Assay, remove_calibrators : bool = True ):
        &#34;&#34;&#34;
        Computes an efficiency from an `qpcr.Assay` object.
    
        This method will try to compute a new efficiency. To do this, it will check autonomously if
        `calibrator : {}` replicates are present and use these for computation. If none are 
        found it will assume the entire assay is to be used as calibrator.

        Note
        ----
        Calibrators are searched for through the group `names` not the replicate ids!

        Parameters
        ----------
        assay : qpcr.Assay
            A `qpcr.Assay` object.

        remove_calibrators : bool
            If calibrators are present in the assay alongside other groups, 
            remove the calibrator replicates after efficiency calculation. 
        &#34;&#34;&#34;

        # get the assay&#39;s groupnames and check for the calibrator prefix.
        names = assay.names( as_set = False ).unique()


        # check if any groups are declared as calibrators
        calibrators = np.array( [ self._has_calibrator_prefix(i) for i in names ] )
        has_calibrators = any( calibrators )

        # now get the relevant dataframe for the computation
        # this will either be the entire df (if no calibrator groups are present)
        # or just the subset of calibrators
        df = assay.get()

        if has_calibrators:
            df = self._subset_calibrators(names, calibrators, df)

        # get Ct column name
        ct_name = defaults.raw_col_names[1]
        # drop NaN cols as they are incompatible with linregress anyway...
        df = df[ df[ ct_name] == df[ ct_name ] ]

        # now sort the dataframe by Ct values as they need to strictly
        # increase for dilution series.
        df = df.sort_values( ct_name ).reset_index()
        df = df.rename( columns = { &#34;index&#34; : &#34;orig_index&#34; } )

        # now generate dilution steps ( i.e. &#34;concentrations&#34; )
        # to do that we first need to check if dilutions have not
        # been supplied, and then try to infer them based on the groupnames
        # if we got an input for dilution() we use  that to generate a 
        # dilution steps array...

        # NOTE: The non-log-scaled dilutions are now stored in self._dilution_steps
        #       while the log-scaled versions are returned. Hence, the dilutions 
        #       variable below is the log-scaled version!
        if not self._manual_dilution_set: 
            dilutions = self._infer_dilution_steps(df)
        else:
            dilutions = self._generate_dilution_steps(df)
        
        
        # now interpolate a line through the log dilutions and the ct values
        cts = df[ ct_name ].to_numpy()

        regression_line = stats.linregress( x = dilutions, y = cts )
        # and now compute the efficiency from the regression line
        efficiency = self._compute_efficiency(regression_line)

        # and now assign the efficiency to the assay
        assay.efficiency( efficiency )

        # save the efficiency in self._eff_dict
        # self._eff_dict.update( { assay.id() : assay.efficiency() } )
        self._eff_dict[ assay.id() ] = assay.efficiency()

        # and, finally, save the computed values and the efficiency
        self._save_computation( assay, dilutions, cts, regression_line )

        # now remove the calibrators from the assay
        # but only do so in case there were calibrators found!
        # If the entire assay was used, we do not delete the entries...
        if has_calibrators and remove_calibrators: 
            self._remove_calibrators(assay, df)

        return assay 
    
    def assign( self, assay : Assay, remove_calibrators : bool = True ):
        &#34;&#34;&#34;
        Assigns an efficiency to an `qpcr.Assay` based on its Id.
        This requires that an efficiency corresponding to the Assay&#39;s Id
        is present in the currently loaded / computed effiencies.

        Parameters
        ----------
        assay : qpcr.Assay
            A `qpcr.Assay` object.

        remove_calibrators : bool
            If calibrators are present in the assay alongside other groups, 
            remove the calibrator replicates. 
        &#34;&#34;&#34;
        eff = self._get_efficiency( assay )
        if eff is not None:
            # set assay&#39;s efficiency
            assay.efficiency( eff )

            # check if any groups are declared as calibrators
            # that should be removed from the df
            names = assay.names( as_set = False ).unique()
            calibrators = np.array( [ self._has_calibrator_prefix(i) for i in names ] )
            has_calibrators = any( calibrators )
            if has_calibrators and remove_calibrators: 
                df = assay.get()
                df = self._subset_calibrators(names, calibrators, df)
                df = df.sort_values( raw_col_names[1] ).reset_index()
                df = df.rename( columns = { &#34;index&#34; : &#34;orig_index&#34; } )
                self._remove_calibrators(assay, df)
        else:
            aw.SoftWarning(&#34;Calibrator:could_not_assign&#34;, id = assay.id() )
        return assay

    def plot( self, mode : str = &#34;interactive&#34;, **kwargs ):
        &#34;&#34;&#34;
        A shortcut to call a `qpcr.Plotters.EfficiencyLines` plotter
        to visualise the regression lines from de novo efficiency computations.

        Parameters
        -------
        mode : str
            The plotting mode. May be either &#34;static&#34; (matplotlib) or &#34;interactive&#34; (plotly).
        **kwargs
            Any additional keyword arguments to be passed to the plotter.

        Returns
        -------
        fig : plt.figure or plotly.figure
            The figure generated by `EfficiencyLines`.
        &#34;&#34;&#34;
        plotter = Plotters.EfficiencyLines( mode = mode )
        plotter.link( self ) 
        fig = plotter.plot( **kwargs )
        return fig 

    def _dilution(self, step = None ):
        &#34;&#34;&#34;
        The functional core of self.dilution() the only difference is
        that self.dilution also sets a boolean attribute self._manual_dilution_set to True...
        Which signals that the manually supplied dilutions should be used rather than that 
        they should be inferred from the dataset...
        &#34;&#34;&#34;
        if step is not None: 
            unknown_datatype = not isinstance( step, (float, int, np.ndarray, pd.Series, tuple, list ) )
            if unknown_datatype:
                aw.HardWarning(&#34;Calibration:cannot_interpret_dilution&#34;, step = step, step_type = type(step).__name__ )
            
            # check if we need to convert to numpy array
            # because we have an iterable without math operations support.
            if isinstance( step, (tuple, list) ):
                step = np.array( step )

            # check for an ndarray and make sure to invert if 
            # the dilution steps are given as 2 4 instead of
            # 0.5 0.25 etc., also do the same for a single number...

            is_inverse_array = isinstance( step, (np.ndarray, pd.Series) ) and any( step &gt; 1 )
            is_inverse_number = isinstance( step, (float, int) ) and step &gt; 1
            need_inverse = is_inverse_array or is_inverse_number
            
            if need_inverse:
                step = 1 / step
            
            # and store new steps
            self._dilution_step = step

        return self._dilution_step

    def _remove_calibrators(self, assay, df):
        &#34;&#34;&#34;
        Drops all calibrator replicates from the dataframe of the Assay.

        Note
        -------
        This leaves the index unchanged! Possible we might wish to also reset the 
        index during this step...
        &#34;&#34;&#34;
        to_remove = df[&#34;orig_index&#34;].to_numpy()
        index = np.zeros( assay.n() )
        for i in to_remove: index[i] = 1 
        index = np.argwhere( index == 1 )
        index = np.squeeze( index )
        assay.ignore( index, drop = True )

    def _save_computation(self, assay, dilutions, ct_values, linreg):
        &#34;&#34;&#34;
        Creates a new entry in self._computed_values for the newly computed
        efficiency.
        &#34;&#34;&#34;
        self._computed_values[ assay.id() ] = _EfficiencyCurve( 
                                                                dilutions = dilutions, 
                                                                ct_values = ct_values, 
                                                                model = linreg,
                                                                efficiency = assay.efficiency() 
                                                            )
        # also add the Id of the Assay to the _EfficiencyCurve
        self._computed_values[ assay.id() ].id( assay.id() )


    def _compute_efficiency(self, regression_line):
        &#34;&#34;&#34;
        Calculates the efficiency from the regression line slope.
        &#34;&#34;&#34;
        slope = regression_line.slope
        efficiency = -1 / slope
        efficiency = np.exp( efficiency  )
        efficiency -= 1 
        efficiency = round( efficiency, 4 )
        return efficiency

    def _subset_calibrators(self, names, calibrators, df):
        &#34;&#34;&#34;
        Generates a dataframe subset containing only calibrator replicates.
        &#34;&#34;&#34;
        # generate a total query formula for all found calibrators
        q = names[ calibrators ]
        q = &#34;&#39; or group_name == &#39;&#34;.join(q)
        q = &#34;group_name == &#39;&#34; + q + &#34;&#39;&#34;
        df = df.query( q )
        return df

    def _infer_dilution_steps(self, df):
        &#34;&#34;&#34;
        Infers the dilution steps from the group names if they are specified 
        as `calibrator: some_name: dilution` e.g. `calibrator: mysample: 5`. 
        &#34;&#34;&#34;
        try: 
            # get dilution steps from groupnames in format calibrator: name : dil
            steps = df[&#34;group_name&#34;].apply(   lambda x: float( x.split(&#34;:&#34;)[2] )   ) 
            steps = steps.to_numpy()

            # preprocess to get proper format
            self._dilution( steps )    
            
            # get and transform to log-scale
            dilutions = self._dilution()
            dilutions = np.log(dilutions)
            self._reset_dilution()
            return dilutions 
        except: 
            aw.HardWarning(&#34;Calibrator:could_not_infer_dilution&#34;)

    def _generate_dilution_steps(self, df):
        &#34;&#34;&#34;
        Generates a numpy ndarray of log-scaled dilution steps
        for the calibrators.
        &#34;&#34;&#34;
        # generate steps range
        # we shall use the concept of (dilution)^m to generate 
        # the dilution steps. To get m we use a re-anchored df[&#34;group&#34;]
        # column

        self._reset_groups(df)
        steps = df[ &#34;group&#34; ]
        counts = df[&#34;group&#34;].value_counts( sort = False )
        repeats = steps.size if isinstance( self._dilution(), float ) else counts

        # repeat the dilution steps to match the group replicate numbers
        dilutions = np.repeat( self._dilution(), repeats ) 

        # if we only had a single value for the dilution we 
        # also need to now scale the dilutions to generate the
        # actual dilution scale. If we already got a dilution series
        # as input, we must not do this as we otherwise doubly scale...
        if isinstance( self._dilution(), float ):
            dilutions = dilutions ** steps
   
        # save dilutions
        self._dilution( dilutions )

        # and transform to log scale 
        dilutions = np.log( dilutions )

        # and reset the diltion back to the what was 
        # originally set (or None from init)
        self._reset_dilution()
        return dilutions

    def _reset_dilution( self ):
        &#34;&#34;&#34;
        Resets the to the default dilution step to ensure
        the same starting conditions are met for each assay 
        as it is passed through the Calibrator.
        &#34;&#34;&#34;
        self._dilution_step = self._orig_dilution


    def _reset_groups(self, df):
        &#34;&#34;&#34;
        Resets the numeric group identifiers to start continuously from 0.
        This method sets the initial group to 0 and then successively resets any
        gaps to match e.g. a 0,1,3 -&gt; 0,1,2...
        &#34;&#34;&#34;

        # get counts of each group_name in the df
        counts = df[&#34;group_name&#34;].value_counts( sort = False )

        # generate new numeric identifiers for each group
        new_groups = np.arange( len(counts) )

        # transform to match the right repeats
        new_groups = np.repeat( new_groups, counts )
        
        # and set new groups
        df[&#34;group&#34;] = new_groups

    def _has_calibrator_prefix( self, string ):
        &#34;&#34;&#34;
        Checks if the calibrator prefix is the start of a string
        The string is a replicate group name in this case...
        &#34;&#34;&#34;
        calibrator_prefix = defaults.calibrator_prefix
        return string.startswith( calibrator_prefix )

    def _get_efficiency( self, assay : Assay ):
        &#34;&#34;&#34;
        Returns the efficiency from the currently loaded effiencies
        that corresponds to the assay&#39;s Id. Returns None if no match is present.
        &#34;&#34;&#34;
        id = assay.id()
        effs = self.get()

        if id not in effs.keys():
            return None
        else:
            return effs[ id ]

    def _load(self, filename):
        &#34;&#34;&#34;
        Loads a csv file but does not adobt the data as its own yet.
        Returns a dictionary.
        &#34;&#34;&#34;
        # current = json.load( open(filename, &#34;r&#34;) )
        current = pd.read_csv( filename )
        current = current.to_dict( &#34;split&#34; )[ &#34;data&#34; ]
        current = { id : eff for id, eff in current }
        return current

    def _save(self, filename, dict_to_save ):
        &#34;&#34;&#34;
        Saves a dictionary to a csv file.
        &#34;&#34;&#34;
        # json.dump( dict_to_save , open(filename, &#34;w&#34;) )
        df = pd.DataFrame( dict_to_save, index = [&#34;eff&#34;] )
        df = df.transpose().reset_index()
        df.to_csv( filename, index = False )

if __name__ == &#34;__main__&#34;:
    
    files = [&#34;./Examples/Example Data/28S.csv&#34;, &#34;./Examples/Example Data/actin.csv&#34;, &#34;./Examples/Example Data/HNRNPL_nmd.csv&#34;, &#34;./Examples/Example Data/HNRNPL_prot.csv&#34;]
    # files = [&#34;Example Data 2/28S.csv&#34;, &#34;Example Data 2/actin.csv&#34;, &#34;Example Data 2/HNRNPL_nmd.csv&#34;, &#34;Example Data 2/HNRNPL_prot.csv&#34;]

    groupnames = [&#34;wt-&#34;, &#34;wt+&#34;, &#34;ko-&#34;, &#34;ko+&#34;]


    reader = DataReader()

    analyser = Analyser()
    analyser.anchor(&#34;first&#34;)

    normaliser = Normaliser()

    assays = []
    for file in files: 

        assay = reader.read(file, replicates = &#34;6,6,6,6&#34;, names = groupnames)
        assay = analyser.pipe(assay)
        assays.append(assay)

    # first to files are normalisers
    normaliser.link(normalisers = assays[:2])

    # last to files are assays
    normaliser.link(assays = assays[2:])

    # def some_prepfunc(normalisers):
    #     first = normalisers[0].get()
    #     first = first[&#34;dCt&#34;]
    #     return pd.DataFrame(dict(dCt_combined = first))
    
    # normaliser.prep_func(some_prepfunc)

    normaliser.normalise()

    print(assays[2].id())
    print(assays[2].get())
    
    result = normaliser.get()
    print(result.stats())

    result.preview()
    
    # alternatively we could link the analyser directly 
    # to get the Assay from there like

    # for file in files[2:]:

    #     assay = reader.read(file, replicates = 6)
    #     analyser.pipe(assay)
    #     normaliser1.link(assays = analyser)

    # for file in files[:2]:

    #     assay = reader.read(file, replicates = 6)
    #     analyser.pipe(assay)
    #     normaliser1.link(normalisers = analyser)

    # normaliser1.normalise()
    
    # result1 = normaliser1.get()

    # result1.drop_groups([3, 1])
    # print(result1.stats())

    # print(result.get() == result1.get())

# down here is some perliminary trial at making second
# normalisation... But this should be properly addressed at some point...

#     splitted = result.split(reset_names = False)

#     i, j = splitted
#     i.rename_cols({&#34;HNRNPL_nmd&#34;: &#34;HNRNPL&#34;})
#     j.rename_cols({&#34;HNRNPL_prot&#34;: &#34;HNRNPL&#34;})

# # print(i, j)
#     # print(&#34;-------&#34;)
#     # print(i.get()[&#34;HNRNPL&#34;] / j.get()[&#34;HNRNPL&#34;])
#     # print(&#34;-----&#34;)
    
    
    
#     sn = Normaliser()

#     sn.link(
#         assays = [splitted[0]], 
#         normalisers = [splitted[1]],
#     )
    
#     sn.normalise(dCt_col = &#34;named&#34;, norm_col = &#34;same&#34;)

#     print(sn.get().get())
#     print(sn.get().stats())

#     # # result.save(&#34;..&#34;)
    
#     # #result.add_names(samples)

#     # print(result.stats())


# if __name__ == &#34;__main__&#34;:
    
#     srcfile = &#34;__eff_data/Excel eff calculation.xlsx&#34;


#     reader = DataReader()

#     assays, _ = reader.read( srcfile, multi_assay = True, assay_pattern = &#34;Rotor-Gene&#34; )

#     from qpcr.Plotters import EfficiencyCurves

#     plotter = EfficiencyCurves( &#34;static&#34; )

#     calibrator = Calibrator()

#     calibrator.dilution( (1,2,4,8,16,32,64) )

#     assays = [ calibrator.calibrate(i) for i in assays ]

#     print( [ assay.efficiency() for assay in assays ])

#     plotter.link( calibrator )
#     plotter.plot() 
#     print( calibrator.get() )</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="qpcr.Filters" href="Filters/index.html">qpcr.Filters</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="qpcr.Parsers" href="Parsers/index.html">qpcr.Parsers</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="qpcr.Pipes" href="Pipes/index.html">qpcr.Pipes</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="qpcr.Plotters" href="Plotters/index.html">qpcr.Plotters</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="qpcr.Readers" href="Readers/index.html">qpcr.Readers</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="qpcr.Analyser"><code class="flex name class">
<span>class <span class="ident">Analyser</span></span>
</code></dt>
<dd>
<div class="desc"><p>Performs Single Delta-Ct (first normalisation
within dataset against the <code>anchor</code>)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Analyser(aux._ID):
    &#34;&#34;&#34;
    Performs Single Delta-Ct (first normalisation 
    within dataset against the `anchor`) 
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._Assay = None

        # default settings
        self._anchor = &#34;first&#34;
        self._ref_group = 0
        self._ref_group_col = &#34;group&#34; # used in case of &#34;mean&#34; anchor where the ref_group must be located either from a numeric (group) or string (group_name) id
        
        self._eff_src = self._Assay         # By default use the efficencies stored in the Assay directly 
                                            # this is set to self if self.efficiency() is called...
        self._efficiency = 1                # the formal effiency in percent
        self._eff = 2 * self._efficiency    # the actual doubplciation factor used for calculation
        self._deltaCt_function = self._get_deltaCt_function(exp = True)

    def get(self):
        &#34;&#34;&#34;
        Returns 
        -------
        Assay : qpcr.Assay
            The analysed `qpcr.Assay` object that contains now deltaCT values.
        &#34;&#34;&#34;
        return self._Assay

    def link(self, Assay:Assay):
        &#34;&#34;&#34;
        Links a `qpcr.Assay` object to the Analyser.
       
        Parameters
        ----------
        Assay : qpcr.Assay
            A `qpcr.Assay` object containing data.
        
        &#34;&#34;&#34;
        self._Assay = Assay

        # check if no efficiency was specifically set for the Analyser
        # and if not so, use the Assay&#39;s efficiency...
        if self._eff_src is not self: 
            self._eff_src = self._Assay

    def pipe(self, Assay:Assay, **kwargs) -&gt; Results:
        &#34;&#34;&#34;
        A quick one-step implementation of link + DeltaCt.

        Note
        ----
        This is the suggested application of the `qpcr.Analyser`.


        Parameters
        ----------
        Assay : qpcr.Assay
            A `qpcr.Assay` object to be linked to the Analyser for DeltaCt computation.
        
        **kwargs
            Any additional keyword arguments to be passed to the `DeltaCt()` method.

        Returns 
        -------
        assay : qpcr.Assay
            The same `qpcr.Assay` with computed Delta-Ct values. 

        &#34;&#34;&#34;
        self.link(Assay)
        self.DeltaCt(**kwargs)
        assay = self.get()
        return assay

    def efficiency(self, e:float = None):
        &#34;&#34;&#34;
        Sets an efficiency factor for externally calculated qPCR amplification efficiency.
        By default `efficiency = 1` (100%) is assumed.

        Note
        ----
        By default the efficiency is now (`qpcr 3.2.0`) handled by the `qpcr.Assay` objects directly.
        The Analyser will directly read the efficiency from the `qpcr.Assay`s. However,
        it is still possible to set the efficiency via the `qpcr.Analyser` in this fashion.

        Parameters
        ----------
        e : float
            An amplification efficiency factor. Default is `e = 1`, 
            which is then treated as `eff = 2 * e`, so `e = 1` corresponds to true duplication
            each cycle.

        Returns
        -------
        efficiency : float
            The current efficiency used.

        &#34;&#34;&#34;
        # deprecation warning
        aw.SoftWarning( &#34;blank&#34;, msg = &#34;The use of efficiency() from the Analyser is deprecated and will be dropped in a future version! Please, set efficiencies directly in the Assay.&#34; )
        if isinstance(e, (int, float)):
            self._efficiency = float( e )
            self._eff = 2 * self._efficiency
            # and update the efficiency source 
            # to self instead of the assay...
            self._eff_src = self 
        return self._efficiency

    def anchor(self, anchor : (str or float or function) = None, group : (int or str) = 0):
        &#34;&#34;&#34;
        Sets the anchor for DeltaCt for internal normalisation.

        Parameters
        ----------
        anchor : str or float or function
            The internal anchor for normalisation.
            This can be either `&#34;first&#34;` (default, the very first dataset entry),
            `&#34;mean&#34;` (mean of the reference group), 
            `&#34;grouped&#34;` (first entry for each replicate group), 
            any specified numeric value (as `float`), 
            or a `function` that will calculate the anchor and returns a single numeric value. 
            If you wish to use a function to compute the anchor, you can access the dataframe stored by the `qpcr.Assay` that is being analysed through the 
            `data` argument. `data` will be automatically forwarded to your custom anchor-function, unless you specify it directly. Please, make sure your function can handle
            `**kwargs` because any kwargs supplied during `DeltaCt()`-calling will be passed per default to both the anchor-function and DeltaCt-function. 
        group : int or str
            The reference group identifier. This can be either the numeric identifier or the `group_name`. This is only used for `anchor = &#34;mean&#34;`. 
            By default the first group is assumed. 
        
        Returns
        -------
        anchor 
            The currently selected `anchor`.
        ref_group
            The current reference group.
        &#34;&#34;&#34;
        if anchor is not None:
            self._anchor = anchor
        if group != self._ref_group:
            self._ref_group = group
        
        # update column where to search for ref_group identifier
        if isinstance(group, str):
            self._ref_group_col = &#34;group_name&#34;
        else: 
            self._ref_group_col = &#34;group&#34;
        
        return self._anchor, self._ref_group


    def func(self, f:(str or function)):
        &#34;&#34;&#34;
        Sets the function to be used for DeltaCt (optional)

        Parameters
        ----------
        f : str or function
            The function to be used for DeltaCt computation. Pre-defined functions are 
            either `&#34;exponential&#34;` (which uses  `dCt = eff ** ( -(s-r) )`, default), or `&#34;linear&#34;` 
            (uses `dCt = s-r`), where `s` is any replicate entry in the dataframe and `r` is the anchor. `eff = 2 * efficiency` is the 
            numeric duplication factor (default assumed `efficiency = 1`).
            It is also possible to assign any defined function that accepts one `float` Ct value `s` (1st!) and anchor `r` value (2nd!), 
            alongside any `kwargs` (which will be forwarded from DeltaCt()...). It must return also a single `float`. 
        &#34;&#34;&#34;
        if f in [&#34;exponential&#34;, &#34;linear&#34;]:
            f = True if f == &#34;exponential&#34; else False
            self._deltaCt_function = self._get_deltaCt_function(f)
        elif type(f) == type(aux.fileID):
            self._deltaCt_function = f
        else:
            aw.HardWarning(&#34;Analyser:cannot_set_func&#34;, func = f)

    def DeltaCt(self, **kwargs):
        &#34;&#34;&#34;
        Calculates Delta-Ct for all groups within the dataframe.
        Any specifics such as `anchor` or `func` must have already been 
        set using the respective methods prior to calling `DeltaCt()`!

        Parameters
        ----------
        **kwargs
            Any additional keyword arguments that a custom DeltaCt function may require.
        &#34;&#34;&#34;

        predefined = {
                        &#34;first&#34;     : self._DeltaCt_first_anchored,
                        &#34;grouped&#34;   : self._DeltaCt_grouped_anchored,
                        &#34;mean&#34;      : self._DeltaCt_mean_anchored,
                    }

        if self._anchor in predefined.keys():
        
            deltaCt_func = predefined[ self._anchor ]
            deltaCt_func(
                                            self._deltaCt_function, 
                                            **kwargs
                                    )
        
        elif isinstance(self._anchor, (float, int)): 
            self._DeltaCt_externally_anchored(
                                                self._anchor, 
                                                self._deltaCt_function, 
                                                **kwargs
                                            )
        elif type(self._anchor) == type(aux.fileID):
            self._DeltaCt_function_anchor(
                                            self._anchor, 
                                            self._deltaCt_function, 
                                            **kwargs
                                        )

    def _DeltaCt_function_anchor(self, anchor_function, deltaCt_function, **kwargs):
        &#34;&#34;&#34;
        Performs DeltaCt using a function as anchor
        &#34;&#34;&#34;
        df = self._Assay.get()
        # update a &#34;data&#34; argument into the 
        # kwargs for the anchor_function
        if &#34;data&#34; not in kwargs.keys(): 
            kwargs.update(dict(data = df))
        anchor = anchor_function(**kwargs)

        # apply deltaCt_function
        Ct = raw_col_names[1]
        dCt = df[Ct].apply(deltaCt_function, r = anchor, **kwargs)
        dCt.name = &#34;dCt&#34;
        # store results
        self._Assay.add_dCt(dCt)

    def _DeltaCt_mean_anchored(self, deltaCt_function, **kwargs):
        &#34;&#34;&#34;
        Performs DeltaCt using the mean of the reference group as anchor
        &#34;&#34;&#34;
        df = self._Assay.get()

        # get  reference group
        ref_query = &#34;{} == {}&#34; if isinstance(self._ref_group, int) else &#34;{} == &#39;{}&#39;&#34; 
        ref = df.query(
                        ref_query.format(  self._ref_group_col, self._ref_group  )
                    )
        # get Ct values from ref group and make anchor
        ref = ref[raw_col_names[1]]
        anchor = ref.mean()
        
        # apply DeltaCt function
        Ct = raw_col_names[1]
        dCt = df[Ct].apply(deltaCt_function, r = anchor, **kwargs)
        dCt.name = &#34;dCt&#34;
        # store results
        self._Assay.add_dCt(dCt)


    def _DeltaCt_externally_anchored(self, anchor:float, deltaCt_function, **kwargs):
        &#34;&#34;&#34;
        Performs DeltaCt using a specified anchor
        &#34;&#34;&#34;
        # get Ct column label 
        Ct = raw_col_names[1]
        df = self._Assay.get()

        # apply DeltaCt function
        dCt = df[Ct].apply(deltaCt_function, r = anchor, **kwargs)
        dCt.name = &#34;dCt&#34;
        # store results
        self._Assay.add_dCt(dCt)


    def _DeltaCt_grouped_anchored(self, deltaCt_function, **kwargs):
        &#34;&#34;&#34;
        Performs DeltaCt using the first entry of each group as anchor
        &#34;&#34;&#34;
        # get set of sample groups and dataset
        groups = self._Assay.groups()
        df = self._Assay.get()

        # get Ct column label
        Ct = raw_col_names[1]

        dCt = pd.Series()
        for group in groups: 
            group_subset = df.query(f&#34;group == {group}&#34;).reset_index(drop = True)
            anchor = group_subset[Ct][0]
            delta_cts = group_subset[Ct].apply(deltaCt_function, r = anchor, **kwargs)
            dCt = dCt.append(delta_cts).reset_index(drop = True)
        
        dCt.name = &#34;dCt&#34;

        # store results
        self._Assay.add_dCt(dCt)

    def _DeltaCt_first_anchored(self, deltaCt_function, **kwargs):
        &#34;&#34;&#34;
        Performs DeltaCt using the very first entry of the dataset as anchor
        &#34;&#34;&#34;
        # get Ct column label
        Ct = raw_col_names[1]
        
        # get first available entry
        # we do this instead of just 0 
        # because the truly first entry 
        # might have been filtered out 
        df = self._Assay.get()
        first = list(df.index)[0]

        # get anchor
        anchor = df[Ct][ first ]

        # apply DeltaCt function
        dCt = df[Ct].apply(deltaCt_function, r = anchor, **kwargs)
        dCt.name = &#34;dCt&#34;
        # store results
        self._Assay.add_dCt(dCt)

    def _exp_DCt(self, s, r, **kwargs):
        &#34;&#34;&#34;
        Calculates deltaCt exponentially
        &#34;&#34;&#34;
        factor = s - r 
        return self._eff_src._eff **(-factor)

    def _simple_DCt(self, s, r, **kwargs):
        &#34;&#34;&#34;
        Calculates deltaCt linearly
        &#34;&#34;&#34;
        return s - r

    def _get_deltaCt_function(self, exp):
        &#34;&#34;&#34;
        Returns the function to be used for DeltaCt based on 
        whether or not exponential shall be used.
        &#34;&#34;&#34;
        if exp == True:
            dCt = self._exp_DCt
        else:
            dCt = self._simple_DCt
        return dCt</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>qpcr._auxiliary._ID</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qpcr.Analyser.DeltaCt"><code class="name flex">
<span>def <span class="ident">DeltaCt</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates Delta-Ct for all groups within the dataframe.
Any specifics such as <code>anchor</code> or <code>func</code> must have already been
set using the respective methods prior to calling <code>DeltaCt()</code>!</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Any additional keyword arguments that a custom DeltaCt function may require.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def DeltaCt(self, **kwargs):
    &#34;&#34;&#34;
    Calculates Delta-Ct for all groups within the dataframe.
    Any specifics such as `anchor` or `func` must have already been 
    set using the respective methods prior to calling `DeltaCt()`!

    Parameters
    ----------
    **kwargs
        Any additional keyword arguments that a custom DeltaCt function may require.
    &#34;&#34;&#34;

    predefined = {
                    &#34;first&#34;     : self._DeltaCt_first_anchored,
                    &#34;grouped&#34;   : self._DeltaCt_grouped_anchored,
                    &#34;mean&#34;      : self._DeltaCt_mean_anchored,
                }

    if self._anchor in predefined.keys():
    
        deltaCt_func = predefined[ self._anchor ]
        deltaCt_func(
                                        self._deltaCt_function, 
                                        **kwargs
                                )
    
    elif isinstance(self._anchor, (float, int)): 
        self._DeltaCt_externally_anchored(
                                            self._anchor, 
                                            self._deltaCt_function, 
                                            **kwargs
                                        )
    elif type(self._anchor) == type(aux.fileID):
        self._DeltaCt_function_anchor(
                                        self._anchor, 
                                        self._deltaCt_function, 
                                        **kwargs
                                    )</code></pre>
</details>
</dd>
<dt id="qpcr.Analyser.anchor"><code class="name flex">
<span>def <span class="ident">anchor</span></span>(<span>self, anchor:Â strÂ =Â None, group:Â intÂ =Â 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the anchor for DeltaCt for internal normalisation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>anchor</code></strong> :&ensp;<code>str</code> or <code>float</code> or <code>function</code></dt>
<dd>The internal anchor for normalisation.
This can be either <code>"first"</code> (default, the very first dataset entry),
<code>"mean"</code> (mean of the reference group),
<code>"grouped"</code> (first entry for each replicate group),
any specified numeric value (as <code>float</code>),
or a <code>function</code> that will calculate the anchor and returns a single numeric value.
If you wish to use a function to compute the anchor, you can access the dataframe stored by the <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> that is being analysed through the
<code>data</code> argument. <code>data</code> will be automatically forwarded to your custom anchor-function, unless you specify it directly. Please, make sure your function can handle
<code>**kwargs</code> because any kwargs supplied during <code>DeltaCt()</code>-calling will be passed per default to both the anchor-function and DeltaCt-function.</dd>
<dt><strong><code>group</code></strong> :&ensp;<code>int</code> or <code>str</code></dt>
<dd>The reference group identifier. This can be either the numeric identifier or the <code>group_name</code>. This is only used for <code>anchor = "mean"</code>.
By default the first group is assumed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>anchor </code></dt>
<dd>The currently selected <code>anchor</code>.</dd>
<dt><code>ref_group</code></dt>
<dd>The current reference group.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def anchor(self, anchor : (str or float or function) = None, group : (int or str) = 0):
    &#34;&#34;&#34;
    Sets the anchor for DeltaCt for internal normalisation.

    Parameters
    ----------
    anchor : str or float or function
        The internal anchor for normalisation.
        This can be either `&#34;first&#34;` (default, the very first dataset entry),
        `&#34;mean&#34;` (mean of the reference group), 
        `&#34;grouped&#34;` (first entry for each replicate group), 
        any specified numeric value (as `float`), 
        or a `function` that will calculate the anchor and returns a single numeric value. 
        If you wish to use a function to compute the anchor, you can access the dataframe stored by the `qpcr.Assay` that is being analysed through the 
        `data` argument. `data` will be automatically forwarded to your custom anchor-function, unless you specify it directly. Please, make sure your function can handle
        `**kwargs` because any kwargs supplied during `DeltaCt()`-calling will be passed per default to both the anchor-function and DeltaCt-function. 
    group : int or str
        The reference group identifier. This can be either the numeric identifier or the `group_name`. This is only used for `anchor = &#34;mean&#34;`. 
        By default the first group is assumed. 
    
    Returns
    -------
    anchor 
        The currently selected `anchor`.
    ref_group
        The current reference group.
    &#34;&#34;&#34;
    if anchor is not None:
        self._anchor = anchor
    if group != self._ref_group:
        self._ref_group = group
    
    # update column where to search for ref_group identifier
    if isinstance(group, str):
        self._ref_group_col = &#34;group_name&#34;
    else: 
        self._ref_group_col = &#34;group&#34;
    
    return self._anchor, self._ref_group</code></pre>
</details>
</dd>
<dt id="qpcr.Analyser.efficiency"><code class="name flex">
<span>def <span class="ident">efficiency</span></span>(<span>self, e:Â floatÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets an efficiency factor for externally calculated qPCR amplification efficiency.
By default <code>efficiency = 1</code> (100%) is assumed.</p>
<h2 id="note">Note</h2>
<p>By default the efficiency is now (<code><a title="qpcr" href="#qpcr">qpcr</a> 3.2.0</code>) handled by the <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> objects directly.
The Analyser will directly read the efficiency from the <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code>s. However,
it is still possible to set the efficiency via the <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code> in this fashion.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>e</code></strong> :&ensp;<code>float</code></dt>
<dd>An amplification efficiency factor. Default is <code>e = 1</code>,
which is then treated as <code>eff = 2 * e</code>, so <code>e = 1</code> corresponds to true duplication
each cycle.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>efficiency</code></strong> :&ensp;<code>float</code></dt>
<dd>The current efficiency used.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def efficiency(self, e:float = None):
    &#34;&#34;&#34;
    Sets an efficiency factor for externally calculated qPCR amplification efficiency.
    By default `efficiency = 1` (100%) is assumed.

    Note
    ----
    By default the efficiency is now (`qpcr 3.2.0`) handled by the `qpcr.Assay` objects directly.
    The Analyser will directly read the efficiency from the `qpcr.Assay`s. However,
    it is still possible to set the efficiency via the `qpcr.Analyser` in this fashion.

    Parameters
    ----------
    e : float
        An amplification efficiency factor. Default is `e = 1`, 
        which is then treated as `eff = 2 * e`, so `e = 1` corresponds to true duplication
        each cycle.

    Returns
    -------
    efficiency : float
        The current efficiency used.

    &#34;&#34;&#34;
    # deprecation warning
    aw.SoftWarning( &#34;blank&#34;, msg = &#34;The use of efficiency() from the Analyser is deprecated and will be dropped in a future version! Please, set efficiencies directly in the Assay.&#34; )
    if isinstance(e, (int, float)):
        self._efficiency = float( e )
        self._eff = 2 * self._efficiency
        # and update the efficiency source 
        # to self instead of the assay...
        self._eff_src = self 
    return self._efficiency</code></pre>
</details>
</dd>
<dt id="qpcr.Analyser.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, f:Â str)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the function to be used for DeltaCt (optional)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>str</code> or <code>function</code></dt>
<dd>The function to be used for DeltaCt computation. Pre-defined functions are
either <code>"exponential"</code> (which uses
<code>dCt = eff ** ( -(s-r) )</code>, default), or <code>"linear"</code>
(uses <code>dCt = s-r</code>), where <code>s</code> is any replicate entry in the dataframe and <code>r</code> is the anchor. <code>eff = 2 * efficiency</code> is the
numeric duplication factor (default assumed <code>efficiency = 1</code>).
It is also possible to assign any defined function that accepts one <code>float</code> Ct value <code>s</code> (1st!) and anchor <code>r</code> value (2nd!),
alongside any <code>kwargs</code> (which will be forwarded from DeltaCt()&hellip;). It must return also a single <code>float</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, f:(str or function)):
    &#34;&#34;&#34;
    Sets the function to be used for DeltaCt (optional)

    Parameters
    ----------
    f : str or function
        The function to be used for DeltaCt computation. Pre-defined functions are 
        either `&#34;exponential&#34;` (which uses  `dCt = eff ** ( -(s-r) )`, default), or `&#34;linear&#34;` 
        (uses `dCt = s-r`), where `s` is any replicate entry in the dataframe and `r` is the anchor. `eff = 2 * efficiency` is the 
        numeric duplication factor (default assumed `efficiency = 1`).
        It is also possible to assign any defined function that accepts one `float` Ct value `s` (1st!) and anchor `r` value (2nd!), 
        alongside any `kwargs` (which will be forwarded from DeltaCt()...). It must return also a single `float`. 
    &#34;&#34;&#34;
    if f in [&#34;exponential&#34;, &#34;linear&#34;]:
        f = True if f == &#34;exponential&#34; else False
        self._deltaCt_function = self._get_deltaCt_function(f)
    elif type(f) == type(aux.fileID):
        self._deltaCt_function = f
    else:
        aw.HardWarning(&#34;Analyser:cannot_set_func&#34;, func = f)</code></pre>
</details>
</dd>
<dt id="qpcr.Analyser.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<p>Assay : qpcr.Assay
The analysed <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object that contains now deltaCT values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self):
    &#34;&#34;&#34;
    Returns 
    -------
    Assay : qpcr.Assay
        The analysed `qpcr.Assay` object that contains now deltaCT values.
    &#34;&#34;&#34;
    return self._Assay</code></pre>
</details>
</dd>
<dt id="qpcr.Analyser.link"><code class="name flex">
<span>def <span class="ident">link</span></span>(<span>self, Assay:Â <a title="qpcr.Assay" href="#qpcr.Assay">Assay</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Links a <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object to the Analyser.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>Assay</code></strong> :&ensp;<code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code></dt>
<dd>A <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object containing data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def link(self, Assay:Assay):
    &#34;&#34;&#34;
    Links a `qpcr.Assay` object to the Analyser.
   
    Parameters
    ----------
    Assay : qpcr.Assay
        A `qpcr.Assay` object containing data.
    
    &#34;&#34;&#34;
    self._Assay = Assay

    # check if no efficiency was specifically set for the Analyser
    # and if not so, use the Assay&#39;s efficiency...
    if self._eff_src is not self: 
        self._eff_src = self._Assay</code></pre>
</details>
</dd>
<dt id="qpcr.Analyser.pipe"><code class="name flex">
<span>def <span class="ident">pipe</span></span>(<span>self, Assay:Â <a title="qpcr.Assay" href="#qpcr.Assay">Assay</a>, **kwargs) â€‘>Â <a title="qpcr.Results" href="#qpcr.Results">Results</a></span>
</code></dt>
<dd>
<div class="desc"><p>A quick one-step implementation of link + DeltaCt.</p>
<h2 id="note">Note</h2>
<p>This is the suggested application of the <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>Assay</code></strong> :&ensp;<code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code></dt>
<dd>A <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object to be linked to the Analyser for DeltaCt computation.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Any additional keyword arguments to be passed to the <code>DeltaCt()</code> method.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>assay : qpcr.Assay
The same <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> with computed Delta-Ct values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pipe(self, Assay:Assay, **kwargs) -&gt; Results:
    &#34;&#34;&#34;
    A quick one-step implementation of link + DeltaCt.

    Note
    ----
    This is the suggested application of the `qpcr.Analyser`.


    Parameters
    ----------
    Assay : qpcr.Assay
        A `qpcr.Assay` object to be linked to the Analyser for DeltaCt computation.
    
    **kwargs
        Any additional keyword arguments to be passed to the `DeltaCt()` method.

    Returns 
    -------
    assay : qpcr.Assay
        The same `qpcr.Assay` with computed Delta-Ct values. 

    &#34;&#34;&#34;
    self.link(Assay)
    self.DeltaCt(**kwargs)
    assay = self.get()
    return assay</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="qpcr.Assay"><code class="flex name class">
<span>class <span class="ident">Assay</span></span>
<span>(</span><span>df:Â pandas.core.frame.DataFrameÂ =Â None, id:Â strÂ =Â None, replicates:Â intÂ =Â None, group_names:Â listÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>The central storing unit of single datasets that were read from datafiles.
An <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> stores the replicate identifiers and Ct values, and also
groups these according to the <code>replicates</code> information (which is automatically
inferred by default). Groups of replicates can be arbitrarily renamed by the user.</p>
<h2 id="note">Note</h2>
<p>The new implementation of the <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> works directly with a DataFrame
that was generated by any one of the <code><a title="qpcr.Readers" href="Readers/index.html">qpcr.Readers</a></code> or <code><a title="qpcr.Parsers" href="Parsers/index.html">qpcr.Parsers</a></code>
instead of a (now depcrecated) <code><a title="qpcr.Reader" href="#qpcr.Reader">Reader</a></code> object.
However, a <code><a title="qpcr.Reader" href="#qpcr.Reader">Reader</a></code> can still be passed as <code>df</code> argument and will be read-in using <code>link</code>.
Support for this will be removed at some point in the future. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>A DataFrame produces by one of the <code><a title="qpcr.Readers" href="Readers/index.html">qpcr.Readers</a></code>
containing an <code>id</code> column for the replicate identifiers
and a <code>Ct</code> value column.</dd>
<dt><strong><code>id</code></strong> :&ensp;<code>str</code></dt>
<dd>The identifer of the assays (the Assay name, essentially).</dd>
<dt><strong><code>replicates</code></strong> :&ensp;<code>int</code> or <code>tuple</code> or <code>str</code></dt>
<dd>Can be an <code>integer</code> (equal group sizes, e.g. <code>3</code> for triplicates),
or a <code>tuple</code> (uneven group sizes, e.g. <code>(3,2,3)</code> if the second group is only a duplicate).
Another method to achieve the same thing is to specify a "formula" as a string of how to create a replicate tuple.
The allowed structure of such a formula is <code>n:m,</code> where <code>n</code> is the number of replicates in a group and <code>m</code> is the number of times
this pattern is repeated (if no <code>:m</code> is specified <code>:1</code> is assumed). See <code><a title="qpcr.Assay.replicates" href="#qpcr.Assay.replicates">Assay.replicates()</a></code> for an example.</dd>
<dt><strong><code>group_names</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of names to use for the replicates groups. If replicates of the same group share the same identifier, then the
group will be inferred automatically. Otherwise, default group names will be set if no <code>group_names</code> are provided.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Assay(aux._ID):
    &#34;&#34;&#34;
    The central storing unit of single datasets that were read from datafiles.
    An `qpcr.Assay` stores the replicate identifiers and Ct values, and also 
    groups these according to the `replicates` information (which is automatically
    inferred by default). Groups of replicates can be arbitrarily renamed by the user.

    Note
    -------
    The new implementation of the `qpcr.Assay` works directly with a DataFrame
    that was generated by any one of the `qpcr.Readers` or `qpcr.Parsers` 
    instead of a (now depcrecated) `qpcr.Reader` object. 
    However, a `qpcr.Reader` can still be passed as `df` argument and will be read-in using `link`. 
    Support for this will be removed at some point in the future. 

    Parameters
    ----------
    df : pandas.DataFrame
        A DataFrame produces by one of the `qpcr.Readers` 
        containing an `id` column for the replicate identifiers 
        and a `Ct` value column. 
    id : str
        The identifer of the assays (the Assay name, essentially). 

    replicates : int or tuple or str
            Can be an `integer` (equal group sizes, e.g. `3` for triplicates), 
            or a `tuple` (uneven group sizes, e.g. `(3,2,3)` if the second group is only a duplicate). 
            Another method to achieve the same thing is to specify a &#34;formula&#34; as a string of how to create a replicate tuple.
            The allowed structure of such a formula is `n:m,` where `n` is the number of replicates in a group and `m` is the number of times
            this pattern is repeated (if no `:m` is specified `:1` is assumed). See `qpcr.Assay.replicates` for an example. 

    group_names : list
        A list of names to use for the replicates groups. If replicates of the same group share the same identifier, then the 
        group will be inferred automatically. Otherwise, default group names will be set if no `group_names` are provided. 
    &#34;&#34;&#34;
    def __init__(self, df : pd.DataFrame = None, id : str = None, replicates : (int or tuple or str) = None, group_names : list = None) -&gt; dict:
        super().__init__()
        
        # FUTURE DROP HERE
        # check if a qpcr.Reader was supplied instead of a dataframe
        # drop this at some point
        if isinstance(df, Reader):
            self.link(df)
        else: 
            self._df = df
            if id is not None: self._id = id

        # setup length of the found data        
        self._length = None if self._df is None else len(self._df)

        # get replicates
        self._replicates = replicates

        # store names 
        self._names = group_names

        # setup the amplification efficiency
        self._efficiency = 1.0 
        self._eff = 2 * self._efficiency

        # if we got data, try to read it 
        if self._df is not None: 
            try: 
                self.replicates(self._replicates)
                self.group()
            except Exception as e:
                aw.SoftWarning(&#34;Assay:setup_not_grouped&#34;)
            
            # and try to change names, provided that we could group yet...
            if self._names is not None and self.groups() is not None: 
                self.rename(self._names)

    def efficiency( self, eff : float = None ):
        &#34;&#34;&#34;
        Gets or sets the amplification efficiency of the Assay.

        Parameters
        -------
        eff : float
            A new efficiency to assign to the assay.

        Returns
        -------
        float 
            The currently assigned efficiency.
        &#34;&#34;&#34;
        if isinstance( eff, (float, int) ):
            self._efficiency = float( eff ) 
            self._eff = 2 * self._efficiency
        return self._efficiency 

    def save(self, filename : str):
        &#34;&#34;&#34;
        Saves the data from the `Assay` to a `csv` file.
        Parameters
        ----------
        filename : str
            The filename into which the assay should be stored.
            If this is a `directory`, then the assay `id` will automatically
            be used as filename. 
        &#34;&#34;&#34;
        if os.path.isdir(filename):
            filename = os.path.join(filename, f&#34;{self.id()}.csv&#34;)
        self.to_csv(filename, index = False)

    def get(self, copy : bool = False ):
        &#34;&#34;&#34;
        Parameters
        -------
        copy : bool
            If `True` returns a deepcopy of the stored dataframe.

        Returns
        -------
        data : pandas.DataFrame
            The stored dataframe
        &#34;&#34;&#34;
        if copy: 
            data = deepcopy( self._df )
        else: 
            data = self._df 
        return data

    def boxplot( self, mode : str = &#34;interactive&#34;, **kwargs ):
        &#34;&#34;&#34;
        A shortcut to call a `qpcr.Plotters.ReplicateBoxPlot` plotter
        to visualise the loaded replicates.

        Parameters
        -------
        mode : str
            The plotting mode. May be either &#34;static&#34; (matplotlib) or &#34;interactive&#34; (plotly).
        **kwargs
            Any additional keyword arguments to be passed to the plotter.

        Returns
        -------
        fig : plt.figure or plotly.figure
            The figure generated by `ReplicateBoxPlot`.
        &#34;&#34;&#34;
        plotter = Plotters.ReplicateBoxPlot( mode = mode )
        plotter.link( self )
        fig = plotter.plot( **kwargs )
        return fig 



    def tile(self, n : int = 1):
        &#34;&#34;&#34;
        Expands the dataframe to the square number of entries for each group.
        This is useful for combinatoric normalisation wherein each replicate is normalised
        against each replicate group-wise from the normaliser, instead of only its supposed partner value.
        
        Parameters
        -------
        n : int
            The number of tiles to produce. By default `1 tile` will effectively *square* the number of entries within the dataframe.
        &#34;&#34;&#34;
        df = self.get()
        groups = self.groups()

        new = None

        for group in groups: 
            subset = df.query(f&#34;group == {group}&#34;)
            length = len(subset) * n
            subset = pd.concat( [subset for i in range(length) ], ignore_index = True )
            if new is None:
                new = subset
            else:
                new = pd.concat( [new, subset], ignore_index = True )

        self.adopt( new, force = True)

    def stack(self, n : int = 2):
        &#34;&#34;&#34;
        Expands the dataframe entry-wise `n` times. 

        Parameters
        -------
        n : int
            The number of stacks to produce. `1 stack` will introduce one more copy of each replicate.
            Note, `n == 1` will keep the current entries!
        &#34;&#34;&#34;
        df = self.get()
        groups = self.groups()

        n = int(n)

        new = None
        if n &gt; 1:
            for group in groups: 
                subset = df.query(f&#34;group == {group}&#34;)
                length = n
                subset = pd.concat( [subset for i in range(length) ], ignore_index = True )
                if new is None:
                    new = subset
                else:
                    new = pd.concat( [new, subset], ignore_index = True )

            self.adopt( new, force = True)

    def Ct(self):
        &#34;&#34;&#34;
        Returns
        ------
        Ct : pandas.Series
            A pandas Series with the assay&#39;s Ct values. The column is renamed 
            from &#34;Ct&#34; to the assay&#39;s `id`.
        &#34;&#34;&#34;
        Ct = self._df[ raw_col_names[1] ]
        Ct.name = self.id()
        return Ct


    def dCt(self):
        &#34;&#34;&#34;
        Returns
        -------
        dCt : pandas.Series
            A pandas Series with the computed Delta-Ct values. The column is renamed 
            from &#34;dCt&#34; to the assay&#39;s `id`.
        &#34;&#34;&#34;
        dCt = self._df[&#34;dCt&#34;]
        dCt.name = self.id()
        return dCt

    def ddCt(self):
        &#34;&#34;&#34;
        Returns
        -------
        ddCt : pandas.DataFrame
            A pandas DataFrame with all Delta-Delta-Ct values that the Assay has stored. 
            All `&#34;rel_{}&#34;` columns are renamed to include the assay `id` to `&#34;{id}_rel_{}&#34;`.
        &#34;&#34;&#34;
        # get all ddCt columns
        ddCt = [ i for i in self._df.columns if &#34;rel_&#34; in i ]
        id = self._id
        # make new names and generate renaming dictionary 
        new_names = [ f&#34;{id}_{i}&#34; for i in ddCt ]
        new_names = {  old : new for new, old in zip(new_names, ddCt)  }

        # get the data and rename
        ddCt = self._df[ ddCt ]
        if not isinstance(ddCt, pd.DataFrame):
            ddCt = pd.DataFrame(ddCt)
        ddCt = ddCt.rename(columns = new_names)
        
        return ddCt

    # FUTURE FEATURE HERE
    # def fc(self):
        # some method to also return the fold change columns... 


    def rename_cols(self, cols:dict):
        &#34;&#34;&#34;
        Renames columns according to a dictionary as key -&gt; value.

        Parameters
        ----------
        cols : dict
            A dictionary specifying old column names (keys) and new colums names (values).
        &#34;&#34;&#34;
        self._df = self._df.rename(columns = cols)


    def link(self, Reader:Reader):
        &#34;&#34;&#34;
        Links a `qpcr.Reader` object to the Assay.

        Note
        ------
        This is deprecated since the `qpcr.Reader` has been 
        replaced by the `qpcr.Readers.SingleReader`. 
        This method will be removed in the future.  

        Parameters
        ----------
        Reader : qpcr.Reader
            A qpcr.Reader object.
        &#34;&#34;&#34;
        self._Reader = Reader
        self.adopt_id(Reader)
        df = self._Reader.get()
        self._df = df
        self._length = self._Reader.n()
        
    def n(self):
        &#34;&#34;&#34;
        Returns 
        ------

        int 
            The number of entries (individual replicates) within the Assay.
        &#34;&#34;&#34;
        return len( self._df )  # self._length

    def add_dCt(self, dCt : pd.Series): 
        &#34;&#34;&#34;
        Adds results from Delta-Ct (first Delta-Ct performed by a `qpcr.Analyser`).

        Parameters
        -----------
        dCt : pandas.Series
            A pandas Series of Delta-Ct values that will be stored in a column `&#34;dCt&#34;`.
            Note, that each `Assay` can, of course, only store one single Delta-Ct column. 
        &#34;&#34;&#34;
        self._df[&#34;dCt&#34;] = dCt
    
    def add_ddCt(self, normaliser_id : str, ddCt : pd.Series):
        &#34;&#34;&#34;
        Adds results from Delta-Delta-Ct (&#34;normalisation&#34; performed by a `qpcr.Normaliser`).
        These will be stored in a column named `&#34;rel_{normaliser_id}&#34;`. Hence, an Assay can store
        an arbitrary number of Delta-Delta-Ct columns against an arbitrary number of different normalisers. 
        
        Parameters
        ----------
        normaliser_id : str
            The id of the normaliser Assay used to compute the Delta-Delta-Ct values.
        ddCt : pandas.Series
            A pandas Series of Delta-Delta-Ct values.
        &#34;&#34;&#34;
        name = f&#34;rel_{normaliser_id}&#34;
        self._df[name] = ddCt

    # FUTURE FEATURE HERE
    # some method to add fc columns here...

    def adopt(self, df : pd.DataFrame, force = False):
        &#34;&#34;&#34;
        Adopts an externally computed dataframe as its own.
        This is supposed to be used when setting up new `qpcr.Assay` objects that do not 
        inherit data from one of the `qpcr.Readers`. If you wish to alter an existing `qpcr.Assay` use `force = True`.
        When doing this, please, make sure to retain the proper data structure!

        Parameters
        ----------
        df : pd.DataFrame
            A pandas DataFrame.
        force : bool
            If a dataframe is already stored the new dataframe will only be stored if `force = True`.
        &#34;&#34;&#34;
        if self._df is None: 
            self._df = df
        elif force:
            self._df = df
        else:
            aw.HardWarning(&#34;Assay:no_data_adopted&#34;)
        self._length = len(self._df)

    def names(self, as_set = True):
        &#34;&#34;&#34;
        Returns a set of the replicate group names (maintaing group order).

        Parameters
        ----------
        as_set : bool
            If `as_set = True` (default) it returns a set (as list without duplicates) 
            of assigned group names for replicate groups.
            If `as_set = False` it returns the full group_name column (including all repeated entries).
        
        Returns
        -------
        names : list or pd.Series
            The given group names of all replicate groups.
        &#34;&#34;&#34;
        if &#34;group_name&#34; in self._df.columns: 
            if as_set:
                return aux.sorted_set(list(self._df[&#34;group_name&#34;]))
            else: 
                return self._df[&#34;group_name&#34;]
        else: 
            aw.SoftWarning(&#34;Assay:no_groupnames&#34;)
            return None
    
    def groups(self, as_set = True):
        &#34;&#34;&#34;
        Returns a set of sample groups (numeric).

        Parameters
        ----------
        as_set : bool
            If `as_set = True` (default) it returns a set (as list without duplicates) 
            of assigned group names for replicate groups.
            If `as_set = False` it returns the full group_name column (including all repeated entries).
        
        Returns
        -------
        groups : list
            The given numeric group identifiers of all replicate groups.
        &#34;&#34;&#34;
        if &#34;group&#34; in self._df.columns:
            groups = sorted(list(set(self._df[&#34;group&#34;]))) if as_set else self._df[&#34;group&#34;]
            return groups
        else:
            aw.SoftWarning(&#34;Assay:setup_not_grouped&#34;)
            return None

    def replicates(self, replicates : (int or tuple or str) = None):
        &#34;&#34;&#34;
        Either sets or gets the replicates settings to be used for grouping
        Before they are assigned, replicates are vetted to ensure they cover all data entries.

        Parameters
        ----------
        replicates : int or tuple or str
            Can be an `integer` (equal group sizes, e.g. `3` for triplicates), 
            or a `tuple` (uneven group sizes, e.g. `(3,2,3)` if the second group is only a duplicate). 
            Another method to achieve the same thing is to specify a &#34;formula&#34; as a string of how to create a replicate tuple.
            The allowed structure of such a formula is `n:m,` where `n` is the number of replicates in a group and `m` is the number of times
            this pattern is repeated (if no `:m` is specified `:1` is assumed). 
            
            So, as an example, if there are 12 groups which are triplicates, but
            at the end there is one which only has a single replicate (like the commonly measured diluent qPCR sample), we could either specify the tuple
            individually as `replicates = (3,3,3,3,3,3,3,3,3,3,3,3,1)` or we use the formula to specify `replicates = &#34;3:12,1&#34;`. Of course, this works for
            any arbitrary setting such as `&#34;3:5,2:5,10,3:12&#34;` (which specifies five triplicates, followed by two duplicates, a single decaplicate, and twelve triplicates again â€“ truly a dataset from another dimension)...
        &#34;&#34;&#34;
        if replicates is not None and self._df is not None: 
            # convert a string formula to tuple if one was provided
            if isinstance(replicates, str): 
                replicates = self._reps_from_formula(replicates)
            # vet replicate coverage
            if self._vet_replicates(replicates):
                self._replicates = replicates
            else: 
                aw.HardWarning(&#34;Assay:reps_dont_cover&#34;, n_samples = self._length, reps = replicates)
        return self._replicates

    def group(self, infer_names = True):
        &#34;&#34;&#34;
        Groups the data according to replicates-settings specified.

        Parameters
        ----------
        infer_names : bool
            Try to infer names of replicate groups based on the individual replicate sample identifiers.
            Note that this only works if all replicates have an identical sample name!
        &#34;&#34;&#34;
        
        # generate group and group_names columns
        if isinstance(self._replicates, int):
            groups, group_names = self._make_equal_groups()            
        elif isinstance(self._replicates, tuple):
            groups, group_names = self._make_unequal_groups()
        else:
            if self._identically_named():
                groups = self._infer_replicates()
                group_names = [default_group_name.format(i) for i in groups]
            else: 
                aw.HardWarning(&#34;Assay:no_reps_inferred&#34;, assay = self.id())
        
        # add numeric group identifiers
        self._df[&#34;group&#34;] = groups
        self._df[&#34;group_name&#34;] = group_names
        
        if infer_names: #and self._names is None:
            # infer group names
            self._infer_names()
            

    def rename(self, names:(list or dict)):
        &#34;&#34;&#34;
        Replaces the current names of the replicate groups 
        (stored in the &#34;group_name&#34; column).

        Parameters
        ----------
        names : list or dict
            Either a `list` (new names without repetitions) or `dict` (key = old name, value = new name) specifying new group names. 
            Group names only need to be specified once, and are applied to all replicate entries.
        &#34;&#34;&#34;
        # get new group names based on list (index) or dict (key)
        if isinstance(names, (list, tuple, set)):
            new_names = self._rename_per_index(names)       
        elif isinstance(names, dict):
            new_names = self._rename_per_key(names)
        else:
            aw.SoftWarning(&#34;Assay:no_groupname_assignment&#34;, names = names)

        # update &#34;group_name&#34;
        self._df[&#34;group_name&#34;] = new_names
        self._renamed = True

    def ignore(self, entries:tuple, drop = False):
        &#34;&#34;&#34;
        Remove lines based on index from the dataframe.
        This is useful when removing corrupted data entries.

        Parameters
        ----------
        entries : tuple
            Tuple of row indices from the dataframe to drop.
        drop : bool 
            If True the provided entries will be entirely removed from the 
            dataset. If False, ignore entries will be set to NaN. 
        &#34;&#34;&#34;
        if drop:
            self._df = self._df.drop(index = list(entries))
        else: 
            Cts = self.Ct()
            Cts = np.array( Cts )
            Cts[ entries ] = np.nan
            self._df[&#34;Ct&#34;] = Cts
    
    def _reps_from_formula(self, replicates):
        &#34;&#34;&#34;
        Generates a replicate tuple from a string formula. 
        See the docstring of `replicates()` for more info on the formula.

        Example:
        &#34;3:4,1:4,2:3,9&#34; -&gt; (3, 3, 3, 3, 1, 1, 1, 1, 2, 2, 2, 9)
        &#34;&#34;&#34;

        # split the formula and adjust standard formatting
        replicates = replicates.split(&#34;,&#34;)
        replicates = [i + &#34;:1&#34; if &#34;:&#34; not in i else i for i in replicates]

        # convert to numeric values and extend
        replicates = [np.array(i.split(&#34;:&#34;), dtype = int) for i in replicates]
        replicates = [np.tile(i[0], i[1]) for i in replicates]
        
        # generate replicate tuple
        replicates = np.concatenate(replicates)
        replicates = tuple(replicates)
        
        return replicates

    def _infer_replicates(self):
        &#34;&#34;&#34;
        Infers the replicate groups based on the replicate ids in case all replicates of the same group have the same name.
        &#34;&#34;&#34;
        names = self._df[raw_col_names[0]]
        names_set = aux.sorted_set(names)
        groups = [i for i in range(len(names_set))]
        for name, group in zip(names_set, groups):
            names = names.replace(name, group)
        
        indices = np.array(names, dtype = int)
        return indices

    def _infer_names(self):
        &#34;&#34;&#34;
        Infers replicate group names from the given replicate identifier column
        &#34;&#34;&#34;
        if self._identically_named():
            self._df[&#34;group_name&#34;] = self._df[raw_col_names[0]]
        elif self._names is None: 
            aw.SoftWarning(&#34;Assay:groupnames_not_inferred&#34;)

    def _identically_named(self):
        &#34;&#34;&#34;
        Checks if all replicates in the same group have the same name / id
        It checks simply the first group, if that is identical then it&#39;s fine.
        &#34;&#34;&#34;
        if &#34;group&#34; not in self._df.columns:
            names = self._df[raw_col_names[0]]
            names_set = aux.sorted_set(names)
            first_name = names_set[0]
            group0 = self._df.query(f&#34;{raw_col_names[0]} == &#39;{first_name}&#39;&#34;)[raw_col_names[0]]
            entries = len(group0)
            all_identical = entries &gt; 1                
        else: 
            group0 = self._df.query(&#34;group == 0&#34;)[raw_col_names[0]]
            all_identical = all(group0 == group0[0])
        return all_identical

    def _rename_per_key(self, names):
        &#34;&#34;&#34;
        Generates new name list based on current names in &#34;group_name&#34; and uses string.replace()
        to update groupnames, based on key (old name) : value (new name) indexing. 
        Before applying it checks if all groups are covered by new names
        &#34;&#34;&#34;
        current_names = aux.sorted_set(self._df[&#34;group_name&#34;])
        all_groups_covered = len(names) == len(current_names)
        if all_groups_covered:
            current_names = list(self._df[&#34;group_name&#34;])
            new_names = &#34;$&#34;.join(current_names)
            for old_name, new_name in names.items():
                new_names = new_names.replace(old_name, new_name)
            new_names = new_names.split(&#34;$&#34;)       
            return new_names
        else:
            aw.HardWarning(&#34;Assay:groupnames_dont_colver&#34;, current_groups = current_names, new_received = names)

    def _rename_per_index(self, names):
        &#34;&#34;&#34;
        Generates new name list based on current names in &#34;group_names&#34; and uses string.replace()
        to update groupnames to new names based on index (using a the order 
        of groups as is currently present in &#34;group_name&#34;). 
        &#34;&#34;&#34;
        current_names_set = aux.sorted_set(self._df[&#34;group_name&#34;])
        all_groups_covered = len(names) == len(current_names_set)
        if all_groups_covered:
            current_names = list(self._df[&#34;group_name&#34;])
            new_names = &#34;$&#34;.join(current_names)
            names = list(names)
            for old_name, new_name in zip(current_names_set, names):
                new_names = new_names.replace(old_name, new_name)
            new_names = new_names.split(&#34;$&#34;)
            return new_names
        else:
            aw.HardWarning(&#34;Assay:groupnames_dont_colver&#34;, current_groups = current_names_set, new_received = names)

    def _make_unequal_groups(self):
        &#34;&#34;&#34;
        Returns two lists of [0,0,0,1,1,1] and 
        [Group0, Group0, Group0, Group1,...] 
        to cover all data entries.
        (this function works with a tuple for replicate group sizes)
        &#34;&#34;&#34;
        groups = []
        group_names = []
        for rep, idx in zip(self._replicates, range(len(self._replicates))): 
            groups.extend([idx] * rep)
            group_names.extend([ default_group_name.format(idx) ] * rep)
        return groups, group_names

    def _make_equal_groups(self):
        &#34;&#34;&#34;
        Returns two lists of [0,0,0,1,1,1] and 
        [Group0, Group0, Group0, Group1,...] 
        to cover all data entries.
        (this function works with an integer group size, 
        assuming all groups have the same size)
        &#34;&#34;&#34;
        assays = self._length
        groups = []
        group_names = []
        slices = range(int(assays / self._replicates))
        for i in slices:
            groups.extend([i] * self._replicates)
            group_names.extend([ default_group_name.format(i) ] * self._replicates)
        return groups, group_names

    def _vet_replicates(self, replicates : (int or tuple)):
        &#34;&#34;&#34;
        Checks if provided replicates will place all data entries into a group
        returns True if all replicates are covered, False if not...
        &#34;&#34;&#34;
        current_entries = self._length
        verdict = None

        # for INT -&gt; modulo will be 0 if all replicates are covered
        # for TUPLE -&gt; sum(replicates) should cover all replicates...

        if isinstance(replicates, int):
            verdict = True if current_entries % replicates == 0 else False
        elif isinstance(replicates, tuple): 
            verdict = True if sum(replicates) == current_entries else False
        
        if verdict is None: 
            aw.HardWarning(&#34;Assay:reps_could_not_vet&#34;, reps = replicates)

        return verdict</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>qpcr._auxiliary._ID</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="qpcr.SampleReader" href="#qpcr.SampleReader">SampleReader</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qpcr.Assay.Ct"><code class="name flex">
<span>def <span class="ident">Ct</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Ct</code></strong> :&ensp;<code>pandas.Series</code></dt>
<dd>A pandas Series with the assay's Ct values. The column is renamed
from "Ct" to the assay's <code>id</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Ct(self):
    &#34;&#34;&#34;
    Returns
    ------
    Ct : pandas.Series
        A pandas Series with the assay&#39;s Ct values. The column is renamed 
        from &#34;Ct&#34; to the assay&#39;s `id`.
    &#34;&#34;&#34;
    Ct = self._df[ raw_col_names[1] ]
    Ct.name = self.id()
    return Ct</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.add_dCt"><code class="name flex">
<span>def <span class="ident">add_dCt</span></span>(<span>self, dCt:Â pandas.core.series.Series)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds results from Delta-Ct (first Delta-Ct performed by a <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code>).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dCt</code></strong> :&ensp;<code>pandas.Series</code></dt>
<dd>A pandas Series of Delta-Ct values that will be stored in a column <code>"dCt"</code>.
Note, that each <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> can, of course, only store one single Delta-Ct column.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_dCt(self, dCt : pd.Series): 
    &#34;&#34;&#34;
    Adds results from Delta-Ct (first Delta-Ct performed by a `qpcr.Analyser`).

    Parameters
    -----------
    dCt : pandas.Series
        A pandas Series of Delta-Ct values that will be stored in a column `&#34;dCt&#34;`.
        Note, that each `Assay` can, of course, only store one single Delta-Ct column. 
    &#34;&#34;&#34;
    self._df[&#34;dCt&#34;] = dCt</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.add_ddCt"><code class="name flex">
<span>def <span class="ident">add_ddCt</span></span>(<span>self, normaliser_id:Â str, ddCt:Â pandas.core.series.Series)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds results from Delta-Delta-Ct ("normalisation" performed by a <code><a title="qpcr.Normaliser" href="#qpcr.Normaliser">Normaliser</a></code>).
These will be stored in a column named <code>"rel_{normaliser_id}"</code>. Hence, an Assay can store
an arbitrary number of Delta-Delta-Ct columns against an arbitrary number of different normalisers. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>normaliser_id</code></strong> :&ensp;<code>str</code></dt>
<dd>The id of the normaliser Assay used to compute the Delta-Delta-Ct values.</dd>
<dt><strong><code>ddCt</code></strong> :&ensp;<code>pandas.Series</code></dt>
<dd>A pandas Series of Delta-Delta-Ct values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_ddCt(self, normaliser_id : str, ddCt : pd.Series):
    &#34;&#34;&#34;
    Adds results from Delta-Delta-Ct (&#34;normalisation&#34; performed by a `qpcr.Normaliser`).
    These will be stored in a column named `&#34;rel_{normaliser_id}&#34;`. Hence, an Assay can store
    an arbitrary number of Delta-Delta-Ct columns against an arbitrary number of different normalisers. 
    
    Parameters
    ----------
    normaliser_id : str
        The id of the normaliser Assay used to compute the Delta-Delta-Ct values.
    ddCt : pandas.Series
        A pandas Series of Delta-Delta-Ct values.
    &#34;&#34;&#34;
    name = f&#34;rel_{normaliser_id}&#34;
    self._df[name] = ddCt</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.adopt"><code class="name flex">
<span>def <span class="ident">adopt</span></span>(<span>self, df:Â pandas.core.frame.DataFrame, force=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Adopts an externally computed dataframe as its own.
This is supposed to be used when setting up new <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> objects that do not
inherit data from one of the <code><a title="qpcr.Readers" href="Readers/index.html">qpcr.Readers</a></code>. If you wish to alter an existing <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> use <code>force = True</code>.
When doing this, please, make sure to retain the proper data structure!</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>A pandas DataFrame.</dd>
<dt><strong><code>force</code></strong> :&ensp;<code>bool</code></dt>
<dd>If a dataframe is already stored the new dataframe will only be stored if <code>force = True</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adopt(self, df : pd.DataFrame, force = False):
    &#34;&#34;&#34;
    Adopts an externally computed dataframe as its own.
    This is supposed to be used when setting up new `qpcr.Assay` objects that do not 
    inherit data from one of the `qpcr.Readers`. If you wish to alter an existing `qpcr.Assay` use `force = True`.
    When doing this, please, make sure to retain the proper data structure!

    Parameters
    ----------
    df : pd.DataFrame
        A pandas DataFrame.
    force : bool
        If a dataframe is already stored the new dataframe will only be stored if `force = True`.
    &#34;&#34;&#34;
    if self._df is None: 
        self._df = df
    elif force:
        self._df = df
    else:
        aw.HardWarning(&#34;Assay:no_data_adopted&#34;)
    self._length = len(self._df)</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.boxplot"><code class="name flex">
<span>def <span class="ident">boxplot</span></span>(<span>self, mode:Â strÂ =Â 'interactive', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A shortcut to call a <code>qpcr.Plotters.ReplicateBoxPlot</code> plotter
to visualise the loaded replicates.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>The plotting mode. May be either "static" (matplotlib) or "interactive" (plotly).</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Any additional keyword arguments to be passed to the plotter.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>fig</code></strong> :&ensp;<code>plt.figure</code> or <code>plotly.figure</code></dt>
<dd>The figure generated by <code>ReplicateBoxPlot</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def boxplot( self, mode : str = &#34;interactive&#34;, **kwargs ):
    &#34;&#34;&#34;
    A shortcut to call a `qpcr.Plotters.ReplicateBoxPlot` plotter
    to visualise the loaded replicates.

    Parameters
    -------
    mode : str
        The plotting mode. May be either &#34;static&#34; (matplotlib) or &#34;interactive&#34; (plotly).
    **kwargs
        Any additional keyword arguments to be passed to the plotter.

    Returns
    -------
    fig : plt.figure or plotly.figure
        The figure generated by `ReplicateBoxPlot`.
    &#34;&#34;&#34;
    plotter = Plotters.ReplicateBoxPlot( mode = mode )
    plotter.link( self )
    fig = plotter.plot( **kwargs )
    return fig </code></pre>
</details>
</dd>
<dt id="qpcr.Assay.dCt"><code class="name flex">
<span>def <span class="ident">dCt</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dCt</code></strong> :&ensp;<code>pandas.Series</code></dt>
<dd>A pandas Series with the computed Delta-Ct values. The column is renamed
from "dCt" to the assay's <code>id</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dCt(self):
    &#34;&#34;&#34;
    Returns
    -------
    dCt : pandas.Series
        A pandas Series with the computed Delta-Ct values. The column is renamed 
        from &#34;dCt&#34; to the assay&#39;s `id`.
    &#34;&#34;&#34;
    dCt = self._df[&#34;dCt&#34;]
    dCt.name = self.id()
    return dCt</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.ddCt"><code class="name flex">
<span>def <span class="ident">ddCt</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ddCt</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>A pandas DataFrame with all Delta-Delta-Ct values that the Assay has stored.
All <code>"rel_{}"</code> columns are renamed to include the assay <code>id</code> to <code>"{id}_rel_{}"</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ddCt(self):
    &#34;&#34;&#34;
    Returns
    -------
    ddCt : pandas.DataFrame
        A pandas DataFrame with all Delta-Delta-Ct values that the Assay has stored. 
        All `&#34;rel_{}&#34;` columns are renamed to include the assay `id` to `&#34;{id}_rel_{}&#34;`.
    &#34;&#34;&#34;
    # get all ddCt columns
    ddCt = [ i for i in self._df.columns if &#34;rel_&#34; in i ]
    id = self._id
    # make new names and generate renaming dictionary 
    new_names = [ f&#34;{id}_{i}&#34; for i in ddCt ]
    new_names = {  old : new for new, old in zip(new_names, ddCt)  }

    # get the data and rename
    ddCt = self._df[ ddCt ]
    if not isinstance(ddCt, pd.DataFrame):
        ddCt = pd.DataFrame(ddCt)
    ddCt = ddCt.rename(columns = new_names)
    
    return ddCt

# FUTURE FEATURE HERE
# def fc(self):
    # some method to also return the fold change columns... </code></pre>
</details>
</dd>
<dt id="qpcr.Assay.efficiency"><code class="name flex">
<span>def <span class="ident">efficiency</span></span>(<span>self, eff:Â floatÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets or sets the amplification efficiency of the Assay.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>eff</code></strong> :&ensp;<code>float</code></dt>
<dd>A new efficiency to assign to the assay.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float </code></dt>
<dd>The currently assigned efficiency.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def efficiency( self, eff : float = None ):
    &#34;&#34;&#34;
    Gets or sets the amplification efficiency of the Assay.

    Parameters
    -------
    eff : float
        A new efficiency to assign to the assay.

    Returns
    -------
    float 
        The currently assigned efficiency.
    &#34;&#34;&#34;
    if isinstance( eff, (float, int) ):
        self._efficiency = float( eff ) 
        self._eff = 2 * self._efficiency
    return self._efficiency </code></pre>
</details>
</dd>
<dt id="qpcr.Assay.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, copy:Â boolÂ =Â False)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>copy</code></strong> :&ensp;<code>bool</code></dt>
<dd>If <code>True</code> returns a deepcopy of the stored dataframe.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The stored dataframe</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self, copy : bool = False ):
    &#34;&#34;&#34;
    Parameters
    -------
    copy : bool
        If `True` returns a deepcopy of the stored dataframe.

    Returns
    -------
    data : pandas.DataFrame
        The stored dataframe
    &#34;&#34;&#34;
    if copy: 
        data = deepcopy( self._df )
    else: 
        data = self._df 
    return data</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.group"><code class="name flex">
<span>def <span class="ident">group</span></span>(<span>self, infer_names=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Groups the data according to replicates-settings specified.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>infer_names</code></strong> :&ensp;<code>bool</code></dt>
<dd>Try to infer names of replicate groups based on the individual replicate sample identifiers.
Note that this only works if all replicates have an identical sample name!</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def group(self, infer_names = True):
    &#34;&#34;&#34;
    Groups the data according to replicates-settings specified.

    Parameters
    ----------
    infer_names : bool
        Try to infer names of replicate groups based on the individual replicate sample identifiers.
        Note that this only works if all replicates have an identical sample name!
    &#34;&#34;&#34;
    
    # generate group and group_names columns
    if isinstance(self._replicates, int):
        groups, group_names = self._make_equal_groups()            
    elif isinstance(self._replicates, tuple):
        groups, group_names = self._make_unequal_groups()
    else:
        if self._identically_named():
            groups = self._infer_replicates()
            group_names = [default_group_name.format(i) for i in groups]
        else: 
            aw.HardWarning(&#34;Assay:no_reps_inferred&#34;, assay = self.id())
    
    # add numeric group identifiers
    self._df[&#34;group&#34;] = groups
    self._df[&#34;group_name&#34;] = group_names
    
    if infer_names: #and self._names is None:
        # infer group names
        self._infer_names()</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.groups"><code class="name flex">
<span>def <span class="ident">groups</span></span>(<span>self, as_set=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a set of sample groups (numeric).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>as_set</code></strong> :&ensp;<code>bool</code></dt>
<dd>If <code>as_set = True</code> (default) it returns a set (as list without duplicates)
of assigned group names for replicate groups.
If <code>as_set = False</code> it returns the full group_name column (including all repeated entries).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>groups</code></strong> :&ensp;<code>list</code></dt>
<dd>The given numeric group identifiers of all replicate groups.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def groups(self, as_set = True):
    &#34;&#34;&#34;
    Returns a set of sample groups (numeric).

    Parameters
    ----------
    as_set : bool
        If `as_set = True` (default) it returns a set (as list without duplicates) 
        of assigned group names for replicate groups.
        If `as_set = False` it returns the full group_name column (including all repeated entries).
    
    Returns
    -------
    groups : list
        The given numeric group identifiers of all replicate groups.
    &#34;&#34;&#34;
    if &#34;group&#34; in self._df.columns:
        groups = sorted(list(set(self._df[&#34;group&#34;]))) if as_set else self._df[&#34;group&#34;]
        return groups
    else:
        aw.SoftWarning(&#34;Assay:setup_not_grouped&#34;)
        return None</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.ignore"><code class="name flex">
<span>def <span class="ident">ignore</span></span>(<span>self, entries:Â tuple, drop=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Remove lines based on index from the dataframe.
This is useful when removing corrupted data entries.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>entries</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Tuple of row indices from the dataframe to drop.</dd>
<dt><strong><code>drop</code></strong> :&ensp;<code>bool </code></dt>
<dd>If True the provided entries will be entirely removed from the
dataset. If False, ignore entries will be set to NaN.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ignore(self, entries:tuple, drop = False):
    &#34;&#34;&#34;
    Remove lines based on index from the dataframe.
    This is useful when removing corrupted data entries.

    Parameters
    ----------
    entries : tuple
        Tuple of row indices from the dataframe to drop.
    drop : bool 
        If True the provided entries will be entirely removed from the 
        dataset. If False, ignore entries will be set to NaN. 
    &#34;&#34;&#34;
    if drop:
        self._df = self._df.drop(index = list(entries))
    else: 
        Cts = self.Ct()
        Cts = np.array( Cts )
        Cts[ entries ] = np.nan
        self._df[&#34;Ct&#34;] = Cts</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.link"><code class="name flex">
<span>def <span class="ident">link</span></span>(<span>self, Reader:Â <a title="qpcr.Reader" href="#qpcr.Reader">Reader</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Links a <code><a title="qpcr.Reader" href="#qpcr.Reader">Reader</a></code> object to the Assay.</p>
<h2 id="note">Note</h2>
<p>This is deprecated since the <code><a title="qpcr.Reader" href="#qpcr.Reader">Reader</a></code> has been
replaced by the <code>qpcr.Readers.SingleReader</code>.
This method will be removed in the future.
</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>Reader</code></strong> :&ensp;<code><a title="qpcr.Reader" href="#qpcr.Reader">Reader</a></code></dt>
<dd>A qpcr.Reader object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def link(self, Reader:Reader):
    &#34;&#34;&#34;
    Links a `qpcr.Reader` object to the Assay.

    Note
    ------
    This is deprecated since the `qpcr.Reader` has been 
    replaced by the `qpcr.Readers.SingleReader`. 
    This method will be removed in the future.  

    Parameters
    ----------
    Reader : qpcr.Reader
        A qpcr.Reader object.
    &#34;&#34;&#34;
    self._Reader = Reader
    self.adopt_id(Reader)
    df = self._Reader.get()
    self._df = df
    self._length = self._Reader.n()</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.n"><code class="name flex">
<span>def <span class="ident">n</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<p>int
The number of entries (individual replicates) within the Assay.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def n(self):
    &#34;&#34;&#34;
    Returns 
    ------

    int 
        The number of entries (individual replicates) within the Assay.
    &#34;&#34;&#34;
    return len( self._df )  # self._length</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.names"><code class="name flex">
<span>def <span class="ident">names</span></span>(<span>self, as_set=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a set of the replicate group names (maintaing group order).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>as_set</code></strong> :&ensp;<code>bool</code></dt>
<dd>If <code>as_set = True</code> (default) it returns a set (as list without duplicates)
of assigned group names for replicate groups.
If <code>as_set = False</code> it returns the full group_name column (including all repeated entries).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>names</code></strong> :&ensp;<code>list</code> or <code>pd.Series</code></dt>
<dd>The given group names of all replicate groups.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def names(self, as_set = True):
    &#34;&#34;&#34;
    Returns a set of the replicate group names (maintaing group order).

    Parameters
    ----------
    as_set : bool
        If `as_set = True` (default) it returns a set (as list without duplicates) 
        of assigned group names for replicate groups.
        If `as_set = False` it returns the full group_name column (including all repeated entries).
    
    Returns
    -------
    names : list or pd.Series
        The given group names of all replicate groups.
    &#34;&#34;&#34;
    if &#34;group_name&#34; in self._df.columns: 
        if as_set:
            return aux.sorted_set(list(self._df[&#34;group_name&#34;]))
        else: 
            return self._df[&#34;group_name&#34;]
    else: 
        aw.SoftWarning(&#34;Assay:no_groupnames&#34;)
        return None</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.rename"><code class="name flex">
<span>def <span class="ident">rename</span></span>(<span>self, names:Â list)</span>
</code></dt>
<dd>
<div class="desc"><p>Replaces the current names of the replicate groups
(stored in the "group_name" column).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>names</code></strong> :&ensp;<code>list</code> or <code>dict</code></dt>
<dd>Either a <code>list</code> (new names without repetitions) or <code>dict</code> (key = old name, value = new name) specifying new group names.
Group names only need to be specified once, and are applied to all replicate entries.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rename(self, names:(list or dict)):
    &#34;&#34;&#34;
    Replaces the current names of the replicate groups 
    (stored in the &#34;group_name&#34; column).

    Parameters
    ----------
    names : list or dict
        Either a `list` (new names without repetitions) or `dict` (key = old name, value = new name) specifying new group names. 
        Group names only need to be specified once, and are applied to all replicate entries.
    &#34;&#34;&#34;
    # get new group names based on list (index) or dict (key)
    if isinstance(names, (list, tuple, set)):
        new_names = self._rename_per_index(names)       
    elif isinstance(names, dict):
        new_names = self._rename_per_key(names)
    else:
        aw.SoftWarning(&#34;Assay:no_groupname_assignment&#34;, names = names)

    # update &#34;group_name&#34;
    self._df[&#34;group_name&#34;] = new_names
    self._renamed = True</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.rename_cols"><code class="name flex">
<span>def <span class="ident">rename_cols</span></span>(<span>self, cols:Â dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Renames columns according to a dictionary as key -&gt; value.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>cols</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary specifying old column names (keys) and new colums names (values).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rename_cols(self, cols:dict):
    &#34;&#34;&#34;
    Renames columns according to a dictionary as key -&gt; value.

    Parameters
    ----------
    cols : dict
        A dictionary specifying old column names (keys) and new colums names (values).
    &#34;&#34;&#34;
    self._df = self._df.rename(columns = cols)</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.replicates"><code class="name flex">
<span>def <span class="ident">replicates</span></span>(<span>self, replicates:Â intÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Either sets or gets the replicates settings to be used for grouping
Before they are assigned, replicates are vetted to ensure they cover all data entries.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>replicates</code></strong> :&ensp;<code>int</code> or <code>tuple</code> or <code>str</code></dt>
<dd>
<p>Can be an <code>integer</code> (equal group sizes, e.g. <code>3</code> for triplicates),
or a <code>tuple</code> (uneven group sizes, e.g. <code>(3,2,3)</code> if the second group is only a duplicate).
Another method to achieve the same thing is to specify a "formula" as a string of how to create a replicate tuple.
The allowed structure of such a formula is <code>n:m,</code> where <code>n</code> is the number of replicates in a group and <code>m</code> is the number of times
this pattern is repeated (if no <code>:m</code> is specified <code>:1</code> is assumed). </p>
<p>So, as an example, if there are 12 groups which are triplicates, but
at the end there is one which only has a single replicate (like the commonly measured diluent qPCR sample), we could either specify the tuple
individually as <code>replicates = (3,3,3,3,3,3,3,3,3,3,3,3,1)</code> or we use the formula to specify <code>replicates = "3:12,1"</code>. Of course, this works for
any arbitrary setting such as <code>"3:5,2:5,10,3:12"</code> (which specifies five triplicates, followed by two duplicates, a single decaplicate, and twelve triplicates again â€“ truly a dataset from another dimension)&hellip;</p>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def replicates(self, replicates : (int or tuple or str) = None):
    &#34;&#34;&#34;
    Either sets or gets the replicates settings to be used for grouping
    Before they are assigned, replicates are vetted to ensure they cover all data entries.

    Parameters
    ----------
    replicates : int or tuple or str
        Can be an `integer` (equal group sizes, e.g. `3` for triplicates), 
        or a `tuple` (uneven group sizes, e.g. `(3,2,3)` if the second group is only a duplicate). 
        Another method to achieve the same thing is to specify a &#34;formula&#34; as a string of how to create a replicate tuple.
        The allowed structure of such a formula is `n:m,` where `n` is the number of replicates in a group and `m` is the number of times
        this pattern is repeated (if no `:m` is specified `:1` is assumed). 
        
        So, as an example, if there are 12 groups which are triplicates, but
        at the end there is one which only has a single replicate (like the commonly measured diluent qPCR sample), we could either specify the tuple
        individually as `replicates = (3,3,3,3,3,3,3,3,3,3,3,3,1)` or we use the formula to specify `replicates = &#34;3:12,1&#34;`. Of course, this works for
        any arbitrary setting such as `&#34;3:5,2:5,10,3:12&#34;` (which specifies five triplicates, followed by two duplicates, a single decaplicate, and twelve triplicates again â€“ truly a dataset from another dimension)...
    &#34;&#34;&#34;
    if replicates is not None and self._df is not None: 
        # convert a string formula to tuple if one was provided
        if isinstance(replicates, str): 
            replicates = self._reps_from_formula(replicates)
        # vet replicate coverage
        if self._vet_replicates(replicates):
            self._replicates = replicates
        else: 
            aw.HardWarning(&#34;Assay:reps_dont_cover&#34;, n_samples = self._length, reps = replicates)
    return self._replicates</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, filename:Â str)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves the data from the <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> to a <code>csv</code> file.
Parameters</p>
<hr>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>The filename into which the assay should be stored.
If this is a <code>directory</code>, then the assay <code>id</code> will automatically
be used as filename.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, filename : str):
    &#34;&#34;&#34;
    Saves the data from the `Assay` to a `csv` file.
    Parameters
    ----------
    filename : str
        The filename into which the assay should be stored.
        If this is a `directory`, then the assay `id` will automatically
        be used as filename. 
    &#34;&#34;&#34;
    if os.path.isdir(filename):
        filename = os.path.join(filename, f&#34;{self.id()}.csv&#34;)
    self.to_csv(filename, index = False)</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.stack"><code class="name flex">
<span>def <span class="ident">stack</span></span>(<span>self, n:Â intÂ =Â 2)</span>
</code></dt>
<dd>
<div class="desc"><p>Expands the dataframe entry-wise <code>n</code> times. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of stacks to produce. <code>1 stack</code> will introduce one more copy of each replicate.
Note, <code>n == 1</code> will keep the current entries!</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stack(self, n : int = 2):
    &#34;&#34;&#34;
    Expands the dataframe entry-wise `n` times. 

    Parameters
    -------
    n : int
        The number of stacks to produce. `1 stack` will introduce one more copy of each replicate.
        Note, `n == 1` will keep the current entries!
    &#34;&#34;&#34;
    df = self.get()
    groups = self.groups()

    n = int(n)

    new = None
    if n &gt; 1:
        for group in groups: 
            subset = df.query(f&#34;group == {group}&#34;)
            length = n
            subset = pd.concat( [subset for i in range(length) ], ignore_index = True )
            if new is None:
                new = subset
            else:
                new = pd.concat( [new, subset], ignore_index = True )

        self.adopt( new, force = True)</code></pre>
</details>
</dd>
<dt id="qpcr.Assay.tile"><code class="name flex">
<span>def <span class="ident">tile</span></span>(<span>self, n:Â intÂ =Â 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Expands the dataframe to the square number of entries for each group.
This is useful for combinatoric normalisation wherein each replicate is normalised
against each replicate group-wise from the normaliser, instead of only its supposed partner value.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of tiles to produce. By default <code>1 tile</code> will effectively <em>square</em> the number of entries within the dataframe.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tile(self, n : int = 1):
    &#34;&#34;&#34;
    Expands the dataframe to the square number of entries for each group.
    This is useful for combinatoric normalisation wherein each replicate is normalised
    against each replicate group-wise from the normaliser, instead of only its supposed partner value.
    
    Parameters
    -------
    n : int
        The number of tiles to produce. By default `1 tile` will effectively *square* the number of entries within the dataframe.
    &#34;&#34;&#34;
    df = self.get()
    groups = self.groups()

    new = None

    for group in groups: 
        subset = df.query(f&#34;group == {group}&#34;)
        length = len(subset) * n
        subset = pd.concat( [subset for i in range(length) ], ignore_index = True )
        if new is None:
            new = subset
        else:
            new = pd.concat( [new, subset], ignore_index = True )

    self.adopt( new, force = True)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="qpcr.Calibrator"><code class="flex name class">
<span>class <span class="ident">Calibrator</span></span>
</code></dt>
<dd>
<div class="desc"><p>Calculates qPCR primer efficiency based on a dilution series.
The dilution series may either be represented as an entire assay
or as a subset of groups within an assay denoted as <code>calibrator : {some_name}</code>.
In this mode, calibrator replicates will be removed after calibration is done.</p>
<p>It is possible to specify the dilution steps directly in the groupnames as:
<code>calibrator: {some_name}: dil</code> where <code>dil</code> is the inverse dilution step, e.g.
<code>calibrator: my_sample: 2</code> for a <code>1 : 2</code> dilution or <code>calibrator: my_sample: 100</code>
for a <code>1 : 100</code>. Note, this will have to be present in <strong>each</strong> groupname! </p>
<p>Alternatively, if no dilution is specified in the groupnames or they cannot be inferred
for some other reason, it is possible to supply a dilution step via
the <code><a title="qpcr.Calibrator.dilution" href="#qpcr.Calibrator.dilution">Calibrator.dilution()</a></code> method.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Calibrator(aux._ID):
    &#34;&#34;&#34;
    Calculates qPCR primer efficiency based on a dilution series.
    The dilution series may either be represented as an entire assay
    or as a subset of groups within an assay denoted as `calibrator : {some_name}`.
    In this mode, calibrator replicates will be removed after calibration is done.

    It is possible to specify the dilution steps directly in the groupnames as:
    `calibrator: {some_name}: dil` where `dil` is the inverse dilution step, e.g.
    `calibrator: my_sample: 2` for a `1 : 2` dilution or `calibrator: my_sample: 100`
    for a `1 : 100`. Note, this will have to be present in **each** groupname! 

    Alternatively, if no dilution is specified in the groupnames or they cannot be inferred
    for some other reason, it is possible to supply a dilution step via 
    the `qpcr.Calibrator.dilution` method. 
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._eff_dict = {}             # stores the efficiencies as assay_id : efficiency
        self._computed_values = {}      # stores newly computed efficiency data as:
                                        # assay_id :  _EfficiencyCurve(...)
                                        #             The _EfficiencyCurve object stores:
                                        #             - dilutions
                                        #             - Ct values
                                        #             - the Linreg model
        self._dilution_step = None      # the dilution step(s) used 
        self._manual_dilution_set = False 
        self._orig_dilution = None 
        
        self._loaded_file = None


    def save( self, filename : str = None , mode : str = &#34;write&#34; ):
        &#34;&#34;&#34;
        Saves the calculated efficiencies to a `csv` file.

        Parameters
        -------
        filename : str
            The filepath in which to store the efficiencies. If a file
            was already loaded then by default the same file will be 
            used to save values again.
        
        mode : str
            Can be either `&#34;write&#34;` to fully overwrite an existing file,
            with the newly computed data, or `&#34;append&#34;` to only add newly 
            computed efficiencies.
        &#34;&#34;&#34;

        if filename is None and self._loaded_file is not None: 
            filename = self._loaded_file

        if mode == &#34;write&#34;:
            self._save( filename, self._eff_dict )

        elif mode == &#34;append&#34;:
            current = self._load(filename)
            new = { **current, **self._eff_dict }
            self._save( filename, new )

        else: 
            aw.SoftWarning(&#34;Calibrator:unknown_savemode&#34;, mode = mode )

    def load(self, filename, merge : bool = True, supersede : bool = False ):
        &#34;&#34;&#34;
        Loads a `csv` file of previously computed efficiencies.

        Parameters
        -------
        filename : str
            The filepath to load efficiencies from.
        merge : bool
            In case efficiencies are already loaded, merge the
            new and existing ones. If `False` the current ones will
            be replaced completely.
        supersede : bool 
            In case efficiencies of the same assay are already loaded
            they will be overwritten by the newly incoming ones if `supersede = True`.
        &#34;&#34;&#34;
        try: 
            current = self._load(filename)
            if merge:
                current = { **self._eff_dict, **current } if supersede else { **current, **self._eff_dict }
            self.adopt( current )
            self._loaded_file = filename
            return current
        except: 
            aw.HardWarning(&#34;Calibrator:unknown_filetype&#34;, filename = filename )

    def get( self, which = &#34;efficiencies&#34; ):
        &#34;&#34;&#34;
        Returns
        -------
        dict
            Either the stored efficiencies (if 
            `which = &#34;efficiencies&#34;`) or 
            the computed values of newly computed
            efficiencies (if `which = &#34;values&#34;`).
        &#34;&#34;&#34;
        if which == &#34;efficiencies&#34;:
            return self.efficiencies()
        elif which == &#34;values&#34;:
            return self.computed_values()
       
    def efficiencies( self ):
        &#34;&#34;&#34;
        Returns
        ------
        dict
            The currently stored efficienies.
        &#34;&#34;&#34;
        return self._eff_dict
    
    def computed_values( self ):
        &#34;&#34;&#34;
        Returns
        ------
        dict
            The currently stored values from newly
            computed efficiencies.
        &#34;&#34;&#34; 
        return self._computed_values
    
    def merge( self, *filenames, outfile = None, adopt = True ):
        &#34;&#34;&#34;
        Merges multiple efficiency files together into a single one.

        Parameters
        -------
        filenames : iterable
            Filepaths to load data from which should be merged together.
        
        outfile : str 
            The filepath in which to store the merged efficiencies.
            Not saved if set to `None`.
        
        adopt : bool
            Will adopt the merged dictionary as its own if `True` (default).

        Returns
        -------
        all_effiencies : dict
            The merged dictionary of all efficiencies from all files.
        &#34;&#34;&#34;
        all_efficiencies = {}
        for filename in filenames: 
            new = self._load( filename )
            all_efficiencies = { **all_efficiencies, **new }

        if outfile is not None: 
            self._save( outfile, all_efficiencies )

        if adopt:
            self.adopt( all_efficiencies ) 

        return all_efficiencies
    
    def reset( self ):
        &#34;&#34;&#34;
        Resets the Calibrator to initial settings. This will 
        clear all stored efficiency values and computed data!
        &#34;&#34;&#34;
        self.__init__()

    def clear( self ):
        &#34;&#34;&#34;
        Will clear all stored efficiency values and computed data.
        &#34;&#34;&#34;
        self._eff_dict = {}
        self._computed_values = {}

    def adopt( self, effs : dict ):
        &#34;&#34;&#34;
        Adopts an externally generated dictionary of `assay : efficiency`
        structure as its own.

        Parameters
        -------
        effs : dict
            A dictionary where keys are Assay Ids (`str`) 
            and values are `float` efficiencies.
        &#34;&#34;&#34;
        if aux.same_type( effs, {} ):
            self._eff_dict = effs
        else: 
            aw.SoftWarning(&#34;Calibrator:cannot_adopt&#34;, effs = effs, eff_type = type(effs).__name__ )
    
    def dilution( self, step : float or np.ndarray or tuple = None ):
        &#34;&#34;&#34;
        Gets or sets the dilution steps used. This must be a `float` fraction
        e.g. `0.5` for a `1 : 2` dilution series or `0.1` for a `1 : 10` series etc.
        If there are multiple steps because there is a gap in the dilution series. It is 
        necessary to supply a step for each group individually e.g. `[1,0.5,0.25,0.0625,0.03125]`.
        if there are 5 dilution steps (originally six but 0.125 was discarded).

        Note, both of the above also work with the inverse dilutions e.g. `2` or `[1,2,4,16,32]`.
        
        By default the `qpcr.Calibrator` tries to infer the dilutions automatically.
        This only works, however, if the calibrator groupnames specify `calibrator: {some name} : dil` where
        `dil` is the inverse dilution step (e.g. `calibrator: my_sample: 2` for a `1 : 2` dilution). Note,
        it is important that the dilution step is given as the inverse (i.e. *not* as `1:2 or 1/2` or something else! )

        Parameters
        ----------
        step : float or np.ndarray
            The dilution step used.  

        Returns
        -------
        dilution : float or np.ndarray
            The currently used dilution step.
        &#34;&#34;&#34;
        
        dilution = self._dilution(step)
        self._orig_dilution = self._dilution_step

        # if the dilution is now set to a valid
        # value we set the _manual_dilution_set check to True
        if dilution is not None:
            self._manual_dilution_set = True 
        return dilution

        
    def pipe( self, assay : Assay, remove_calibrators : bool = True, ignore_uncalibrated : bool = False ):
        &#34;&#34;&#34;
        A wrapper for calibrate / assign.

        This method will first try to assign pre-computed efficiencies
        and if no matching ones are found it will try to calculate a new efficiency
        from the assay. 

        Parameters
        ----------
        assay : qpcr.Assay
            A `qpcr.Assay` object.
        remove_calibrators : bool
            If calibrators are present in the assay alongside other groups, 
            remove the calibrator replicates after assignment or efficiency calculation. 
        ignore_uncalibrated : bool
            If `True` assays that could neither be newly calibrated nor be assigned an existing
            efficiency will be ignored. Otherwise, and error will be raised. 

        Returns
        -------
        assay : qpcr.Assay
            The now calibrated `qpcr.Assay`.
        &#34;&#34;&#34;
        if self._eff_dict != {}:
            # first try to assign (will leave the assay unchanged if nothing is found)
            eff = self._get_efficiency( assay )
            if eff is not None: 
                assay = self.assign( assay, remove_calibrators = remove_calibrators )
            else: 
                try:
                    assay = self.calibrate( assay, remove_calibrators = remove_calibrators )
                except: 
                    if not ignore_uncalibrated:
                        aw.HardWarning(&#34;Calibrator:cannot_process_assay&#34;, id = assay.id() )
                    else: 
                        aw.SoftWarning(&#34;Calibrator:cannot_process_assay&#34;, id = assay.id() )
        else:
            try: 
                assay = self.calibrate( assay, remove_calibrators = remove_calibrators )
            except:
                if not ignore_uncalibrated:
                    aw.HardWarning(&#34;Calibrator:cannot_process_assay&#34;, id = assay.id() )
                else: 
                    aw.SoftWarning(&#34;Calibrator:cannot_process_assay&#34;, id = assay.id() )
        return assay

    def calibrate( self, assay : Assay, remove_calibrators : bool = True ):
        &#34;&#34;&#34;
        Computes an efficiency from an `qpcr.Assay` object.
    
        This method will try to compute a new efficiency. To do this, it will check autonomously if
        `calibrator : {}` replicates are present and use these for computation. If none are 
        found it will assume the entire assay is to be used as calibrator.

        Note
        ----
        Calibrators are searched for through the group `names` not the replicate ids!

        Parameters
        ----------
        assay : qpcr.Assay
            A `qpcr.Assay` object.

        remove_calibrators : bool
            If calibrators are present in the assay alongside other groups, 
            remove the calibrator replicates after efficiency calculation. 
        &#34;&#34;&#34;

        # get the assay&#39;s groupnames and check for the calibrator prefix.
        names = assay.names( as_set = False ).unique()


        # check if any groups are declared as calibrators
        calibrators = np.array( [ self._has_calibrator_prefix(i) for i in names ] )
        has_calibrators = any( calibrators )

        # now get the relevant dataframe for the computation
        # this will either be the entire df (if no calibrator groups are present)
        # or just the subset of calibrators
        df = assay.get()

        if has_calibrators:
            df = self._subset_calibrators(names, calibrators, df)

        # get Ct column name
        ct_name = defaults.raw_col_names[1]
        # drop NaN cols as they are incompatible with linregress anyway...
        df = df[ df[ ct_name] == df[ ct_name ] ]

        # now sort the dataframe by Ct values as they need to strictly
        # increase for dilution series.
        df = df.sort_values( ct_name ).reset_index()
        df = df.rename( columns = { &#34;index&#34; : &#34;orig_index&#34; } )

        # now generate dilution steps ( i.e. &#34;concentrations&#34; )
        # to do that we first need to check if dilutions have not
        # been supplied, and then try to infer them based on the groupnames
        # if we got an input for dilution() we use  that to generate a 
        # dilution steps array...

        # NOTE: The non-log-scaled dilutions are now stored in self._dilution_steps
        #       while the log-scaled versions are returned. Hence, the dilutions 
        #       variable below is the log-scaled version!
        if not self._manual_dilution_set: 
            dilutions = self._infer_dilution_steps(df)
        else:
            dilutions = self._generate_dilution_steps(df)
        
        
        # now interpolate a line through the log dilutions and the ct values
        cts = df[ ct_name ].to_numpy()

        regression_line = stats.linregress( x = dilutions, y = cts )
        # and now compute the efficiency from the regression line
        efficiency = self._compute_efficiency(regression_line)

        # and now assign the efficiency to the assay
        assay.efficiency( efficiency )

        # save the efficiency in self._eff_dict
        # self._eff_dict.update( { assay.id() : assay.efficiency() } )
        self._eff_dict[ assay.id() ] = assay.efficiency()

        # and, finally, save the computed values and the efficiency
        self._save_computation( assay, dilutions, cts, regression_line )

        # now remove the calibrators from the assay
        # but only do so in case there were calibrators found!
        # If the entire assay was used, we do not delete the entries...
        if has_calibrators and remove_calibrators: 
            self._remove_calibrators(assay, df)

        return assay 
    
    def assign( self, assay : Assay, remove_calibrators : bool = True ):
        &#34;&#34;&#34;
        Assigns an efficiency to an `qpcr.Assay` based on its Id.
        This requires that an efficiency corresponding to the Assay&#39;s Id
        is present in the currently loaded / computed effiencies.

        Parameters
        ----------
        assay : qpcr.Assay
            A `qpcr.Assay` object.

        remove_calibrators : bool
            If calibrators are present in the assay alongside other groups, 
            remove the calibrator replicates. 
        &#34;&#34;&#34;
        eff = self._get_efficiency( assay )
        if eff is not None:
            # set assay&#39;s efficiency
            assay.efficiency( eff )

            # check if any groups are declared as calibrators
            # that should be removed from the df
            names = assay.names( as_set = False ).unique()
            calibrators = np.array( [ self._has_calibrator_prefix(i) for i in names ] )
            has_calibrators = any( calibrators )
            if has_calibrators and remove_calibrators: 
                df = assay.get()
                df = self._subset_calibrators(names, calibrators, df)
                df = df.sort_values( raw_col_names[1] ).reset_index()
                df = df.rename( columns = { &#34;index&#34; : &#34;orig_index&#34; } )
                self._remove_calibrators(assay, df)
        else:
            aw.SoftWarning(&#34;Calibrator:could_not_assign&#34;, id = assay.id() )
        return assay

    def plot( self, mode : str = &#34;interactive&#34;, **kwargs ):
        &#34;&#34;&#34;
        A shortcut to call a `qpcr.Plotters.EfficiencyLines` plotter
        to visualise the regression lines from de novo efficiency computations.

        Parameters
        -------
        mode : str
            The plotting mode. May be either &#34;static&#34; (matplotlib) or &#34;interactive&#34; (plotly).
        **kwargs
            Any additional keyword arguments to be passed to the plotter.

        Returns
        -------
        fig : plt.figure or plotly.figure
            The figure generated by `EfficiencyLines`.
        &#34;&#34;&#34;
        plotter = Plotters.EfficiencyLines( mode = mode )
        plotter.link( self ) 
        fig = plotter.plot( **kwargs )
        return fig 

    def _dilution(self, step = None ):
        &#34;&#34;&#34;
        The functional core of self.dilution() the only difference is
        that self.dilution also sets a boolean attribute self._manual_dilution_set to True...
        Which signals that the manually supplied dilutions should be used rather than that 
        they should be inferred from the dataset...
        &#34;&#34;&#34;
        if step is not None: 
            unknown_datatype = not isinstance( step, (float, int, np.ndarray, pd.Series, tuple, list ) )
            if unknown_datatype:
                aw.HardWarning(&#34;Calibration:cannot_interpret_dilution&#34;, step = step, step_type = type(step).__name__ )
            
            # check if we need to convert to numpy array
            # because we have an iterable without math operations support.
            if isinstance( step, (tuple, list) ):
                step = np.array( step )

            # check for an ndarray and make sure to invert if 
            # the dilution steps are given as 2 4 instead of
            # 0.5 0.25 etc., also do the same for a single number...

            is_inverse_array = isinstance( step, (np.ndarray, pd.Series) ) and any( step &gt; 1 )
            is_inverse_number = isinstance( step, (float, int) ) and step &gt; 1
            need_inverse = is_inverse_array or is_inverse_number
            
            if need_inverse:
                step = 1 / step
            
            # and store new steps
            self._dilution_step = step

        return self._dilution_step

    def _remove_calibrators(self, assay, df):
        &#34;&#34;&#34;
        Drops all calibrator replicates from the dataframe of the Assay.

        Note
        -------
        This leaves the index unchanged! Possible we might wish to also reset the 
        index during this step...
        &#34;&#34;&#34;
        to_remove = df[&#34;orig_index&#34;].to_numpy()
        index = np.zeros( assay.n() )
        for i in to_remove: index[i] = 1 
        index = np.argwhere( index == 1 )
        index = np.squeeze( index )
        assay.ignore( index, drop = True )

    def _save_computation(self, assay, dilutions, ct_values, linreg):
        &#34;&#34;&#34;
        Creates a new entry in self._computed_values for the newly computed
        efficiency.
        &#34;&#34;&#34;
        self._computed_values[ assay.id() ] = _EfficiencyCurve( 
                                                                dilutions = dilutions, 
                                                                ct_values = ct_values, 
                                                                model = linreg,
                                                                efficiency = assay.efficiency() 
                                                            )
        # also add the Id of the Assay to the _EfficiencyCurve
        self._computed_values[ assay.id() ].id( assay.id() )


    def _compute_efficiency(self, regression_line):
        &#34;&#34;&#34;
        Calculates the efficiency from the regression line slope.
        &#34;&#34;&#34;
        slope = regression_line.slope
        efficiency = -1 / slope
        efficiency = np.exp( efficiency  )
        efficiency -= 1 
        efficiency = round( efficiency, 4 )
        return efficiency

    def _subset_calibrators(self, names, calibrators, df):
        &#34;&#34;&#34;
        Generates a dataframe subset containing only calibrator replicates.
        &#34;&#34;&#34;
        # generate a total query formula for all found calibrators
        q = names[ calibrators ]
        q = &#34;&#39; or group_name == &#39;&#34;.join(q)
        q = &#34;group_name == &#39;&#34; + q + &#34;&#39;&#34;
        df = df.query( q )
        return df

    def _infer_dilution_steps(self, df):
        &#34;&#34;&#34;
        Infers the dilution steps from the group names if they are specified 
        as `calibrator: some_name: dilution` e.g. `calibrator: mysample: 5`. 
        &#34;&#34;&#34;
        try: 
            # get dilution steps from groupnames in format calibrator: name : dil
            steps = df[&#34;group_name&#34;].apply(   lambda x: float( x.split(&#34;:&#34;)[2] )   ) 
            steps = steps.to_numpy()

            # preprocess to get proper format
            self._dilution( steps )    
            
            # get and transform to log-scale
            dilutions = self._dilution()
            dilutions = np.log(dilutions)
            self._reset_dilution()
            return dilutions 
        except: 
            aw.HardWarning(&#34;Calibrator:could_not_infer_dilution&#34;)

    def _generate_dilution_steps(self, df):
        &#34;&#34;&#34;
        Generates a numpy ndarray of log-scaled dilution steps
        for the calibrators.
        &#34;&#34;&#34;
        # generate steps range
        # we shall use the concept of (dilution)^m to generate 
        # the dilution steps. To get m we use a re-anchored df[&#34;group&#34;]
        # column

        self._reset_groups(df)
        steps = df[ &#34;group&#34; ]
        counts = df[&#34;group&#34;].value_counts( sort = False )
        repeats = steps.size if isinstance( self._dilution(), float ) else counts

        # repeat the dilution steps to match the group replicate numbers
        dilutions = np.repeat( self._dilution(), repeats ) 

        # if we only had a single value for the dilution we 
        # also need to now scale the dilutions to generate the
        # actual dilution scale. If we already got a dilution series
        # as input, we must not do this as we otherwise doubly scale...
        if isinstance( self._dilution(), float ):
            dilutions = dilutions ** steps
   
        # save dilutions
        self._dilution( dilutions )

        # and transform to log scale 
        dilutions = np.log( dilutions )

        # and reset the diltion back to the what was 
        # originally set (or None from init)
        self._reset_dilution()
        return dilutions

    def _reset_dilution( self ):
        &#34;&#34;&#34;
        Resets the to the default dilution step to ensure
        the same starting conditions are met for each assay 
        as it is passed through the Calibrator.
        &#34;&#34;&#34;
        self._dilution_step = self._orig_dilution


    def _reset_groups(self, df):
        &#34;&#34;&#34;
        Resets the numeric group identifiers to start continuously from 0.
        This method sets the initial group to 0 and then successively resets any
        gaps to match e.g. a 0,1,3 -&gt; 0,1,2...
        &#34;&#34;&#34;

        # get counts of each group_name in the df
        counts = df[&#34;group_name&#34;].value_counts( sort = False )

        # generate new numeric identifiers for each group
        new_groups = np.arange( len(counts) )

        # transform to match the right repeats
        new_groups = np.repeat( new_groups, counts )
        
        # and set new groups
        df[&#34;group&#34;] = new_groups

    def _has_calibrator_prefix( self, string ):
        &#34;&#34;&#34;
        Checks if the calibrator prefix is the start of a string
        The string is a replicate group name in this case...
        &#34;&#34;&#34;
        calibrator_prefix = defaults.calibrator_prefix
        return string.startswith( calibrator_prefix )

    def _get_efficiency( self, assay : Assay ):
        &#34;&#34;&#34;
        Returns the efficiency from the currently loaded effiencies
        that corresponds to the assay&#39;s Id. Returns None if no match is present.
        &#34;&#34;&#34;
        id = assay.id()
        effs = self.get()

        if id not in effs.keys():
            return None
        else:
            return effs[ id ]

    def _load(self, filename):
        &#34;&#34;&#34;
        Loads a csv file but does not adobt the data as its own yet.
        Returns a dictionary.
        &#34;&#34;&#34;
        # current = json.load( open(filename, &#34;r&#34;) )
        current = pd.read_csv( filename )
        current = current.to_dict( &#34;split&#34; )[ &#34;data&#34; ]
        current = { id : eff for id, eff in current }
        return current

    def _save(self, filename, dict_to_save ):
        &#34;&#34;&#34;
        Saves a dictionary to a csv file.
        &#34;&#34;&#34;
        # json.dump( dict_to_save , open(filename, &#34;w&#34;) )
        df = pd.DataFrame( dict_to_save, index = [&#34;eff&#34;] )
        df = df.transpose().reset_index()
        df.to_csv( filename, index = False )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>qpcr._auxiliary._ID</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qpcr.Calibrator.adopt"><code class="name flex">
<span>def <span class="ident">adopt</span></span>(<span>self, effs:Â dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Adopts an externally generated dictionary of <code>assay : efficiency</code>
structure as its own.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>effs</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary where keys are Assay Ids (<code>str</code>)
and values are <code>float</code> efficiencies.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adopt( self, effs : dict ):
    &#34;&#34;&#34;
    Adopts an externally generated dictionary of `assay : efficiency`
    structure as its own.

    Parameters
    -------
    effs : dict
        A dictionary where keys are Assay Ids (`str`) 
        and values are `float` efficiencies.
    &#34;&#34;&#34;
    if aux.same_type( effs, {} ):
        self._eff_dict = effs
    else: 
        aw.SoftWarning(&#34;Calibrator:cannot_adopt&#34;, effs = effs, eff_type = type(effs).__name__ )</code></pre>
</details>
</dd>
<dt id="qpcr.Calibrator.assign"><code class="name flex">
<span>def <span class="ident">assign</span></span>(<span>self, assay:Â <a title="qpcr.Assay" href="#qpcr.Assay">Assay</a>, remove_calibrators:Â boolÂ =Â True)</span>
</code></dt>
<dd>
<div class="desc"><p>Assigns an efficiency to an <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> based on its Id.
This requires that an efficiency corresponding to the Assay's Id
is present in the currently loaded / computed effiencies.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>assay</code></strong> :&ensp;<code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code></dt>
<dd>A <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object.</dd>
<dt><strong><code>remove_calibrators</code></strong> :&ensp;<code>bool</code></dt>
<dd>If calibrators are present in the assay alongside other groups,
remove the calibrator replicates.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assign( self, assay : Assay, remove_calibrators : bool = True ):
    &#34;&#34;&#34;
    Assigns an efficiency to an `qpcr.Assay` based on its Id.
    This requires that an efficiency corresponding to the Assay&#39;s Id
    is present in the currently loaded / computed effiencies.

    Parameters
    ----------
    assay : qpcr.Assay
        A `qpcr.Assay` object.

    remove_calibrators : bool
        If calibrators are present in the assay alongside other groups, 
        remove the calibrator replicates. 
    &#34;&#34;&#34;
    eff = self._get_efficiency( assay )
    if eff is not None:
        # set assay&#39;s efficiency
        assay.efficiency( eff )

        # check if any groups are declared as calibrators
        # that should be removed from the df
        names = assay.names( as_set = False ).unique()
        calibrators = np.array( [ self._has_calibrator_prefix(i) for i in names ] )
        has_calibrators = any( calibrators )
        if has_calibrators and remove_calibrators: 
            df = assay.get()
            df = self._subset_calibrators(names, calibrators, df)
            df = df.sort_values( raw_col_names[1] ).reset_index()
            df = df.rename( columns = { &#34;index&#34; : &#34;orig_index&#34; } )
            self._remove_calibrators(assay, df)
    else:
        aw.SoftWarning(&#34;Calibrator:could_not_assign&#34;, id = assay.id() )
    return assay</code></pre>
</details>
</dd>
<dt id="qpcr.Calibrator.calibrate"><code class="name flex">
<span>def <span class="ident">calibrate</span></span>(<span>self, assay:Â <a title="qpcr.Assay" href="#qpcr.Assay">Assay</a>, remove_calibrators:Â boolÂ =Â True)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes an efficiency from an <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object.</p>
<p>This method will try to compute a new efficiency. To do this, it will check autonomously if
<code>calibrator : {}</code> replicates are present and use these for computation. If none are
found it will assume the entire assay is to be used as calibrator.</p>
<h2 id="note">Note</h2>
<p>Calibrators are searched for through the group <code>names</code> not the replicate ids!</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>assay</code></strong> :&ensp;<code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code></dt>
<dd>A <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object.</dd>
<dt><strong><code>remove_calibrators</code></strong> :&ensp;<code>bool</code></dt>
<dd>If calibrators are present in the assay alongside other groups,
remove the calibrator replicates after efficiency calculation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calibrate( self, assay : Assay, remove_calibrators : bool = True ):
    &#34;&#34;&#34;
    Computes an efficiency from an `qpcr.Assay` object.

    This method will try to compute a new efficiency. To do this, it will check autonomously if
    `calibrator : {}` replicates are present and use these for computation. If none are 
    found it will assume the entire assay is to be used as calibrator.

    Note
    ----
    Calibrators are searched for through the group `names` not the replicate ids!

    Parameters
    ----------
    assay : qpcr.Assay
        A `qpcr.Assay` object.

    remove_calibrators : bool
        If calibrators are present in the assay alongside other groups, 
        remove the calibrator replicates after efficiency calculation. 
    &#34;&#34;&#34;

    # get the assay&#39;s groupnames and check for the calibrator prefix.
    names = assay.names( as_set = False ).unique()


    # check if any groups are declared as calibrators
    calibrators = np.array( [ self._has_calibrator_prefix(i) for i in names ] )
    has_calibrators = any( calibrators )

    # now get the relevant dataframe for the computation
    # this will either be the entire df (if no calibrator groups are present)
    # or just the subset of calibrators
    df = assay.get()

    if has_calibrators:
        df = self._subset_calibrators(names, calibrators, df)

    # get Ct column name
    ct_name = defaults.raw_col_names[1]
    # drop NaN cols as they are incompatible with linregress anyway...
    df = df[ df[ ct_name] == df[ ct_name ] ]

    # now sort the dataframe by Ct values as they need to strictly
    # increase for dilution series.
    df = df.sort_values( ct_name ).reset_index()
    df = df.rename( columns = { &#34;index&#34; : &#34;orig_index&#34; } )

    # now generate dilution steps ( i.e. &#34;concentrations&#34; )
    # to do that we first need to check if dilutions have not
    # been supplied, and then try to infer them based on the groupnames
    # if we got an input for dilution() we use  that to generate a 
    # dilution steps array...

    # NOTE: The non-log-scaled dilutions are now stored in self._dilution_steps
    #       while the log-scaled versions are returned. Hence, the dilutions 
    #       variable below is the log-scaled version!
    if not self._manual_dilution_set: 
        dilutions = self._infer_dilution_steps(df)
    else:
        dilutions = self._generate_dilution_steps(df)
    
    
    # now interpolate a line through the log dilutions and the ct values
    cts = df[ ct_name ].to_numpy()

    regression_line = stats.linregress( x = dilutions, y = cts )
    # and now compute the efficiency from the regression line
    efficiency = self._compute_efficiency(regression_line)

    # and now assign the efficiency to the assay
    assay.efficiency( efficiency )

    # save the efficiency in self._eff_dict
    # self._eff_dict.update( { assay.id() : assay.efficiency() } )
    self._eff_dict[ assay.id() ] = assay.efficiency()

    # and, finally, save the computed values and the efficiency
    self._save_computation( assay, dilutions, cts, regression_line )

    # now remove the calibrators from the assay
    # but only do so in case there were calibrators found!
    # If the entire assay was used, we do not delete the entries...
    if has_calibrators and remove_calibrators: 
        self._remove_calibrators(assay, df)

    return assay </code></pre>
</details>
</dd>
<dt id="qpcr.Calibrator.clear"><code class="name flex">
<span>def <span class="ident">clear</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Will clear all stored efficiency values and computed data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear( self ):
    &#34;&#34;&#34;
    Will clear all stored efficiency values and computed data.
    &#34;&#34;&#34;
    self._eff_dict = {}
    self._computed_values = {}</code></pre>
</details>
</dd>
<dt id="qpcr.Calibrator.computed_values"><code class="name flex">
<span>def <span class="ident">computed_values</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>The currently stored values from newly
computed efficiencies.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def computed_values( self ):
    &#34;&#34;&#34;
    Returns
    ------
    dict
        The currently stored values from newly
        computed efficiencies.
    &#34;&#34;&#34; 
    return self._computed_values</code></pre>
</details>
</dd>
<dt id="qpcr.Calibrator.dilution"><code class="name flex">
<span>def <span class="ident">dilution</span></span>(<span>self, step:Â floatÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets or sets the dilution steps used. This must be a <code>float</code> fraction
e.g. <code>0.5</code> for a <code>1 : 2</code> dilution series or <code>0.1</code> for a <code>1 : 10</code> series etc.
If there are multiple steps because there is a gap in the dilution series. It is
necessary to supply a step for each group individually e.g. <code>[1,0.5,0.25,0.0625,0.03125]</code>.
if there are 5 dilution steps (originally six but 0.125 was discarded).</p>
<p>Note, both of the above also work with the inverse dilutions e.g. <code>2</code> or <code>[1,2,4,16,32]</code>.</p>
<p>By default the <code><a title="qpcr.Calibrator" href="#qpcr.Calibrator">Calibrator</a></code> tries to infer the dilutions automatically.
This only works, however, if the calibrator groupnames specify <code>calibrator: {some name} : dil</code> where
<code>dil</code> is the inverse dilution step (e.g. <code>calibrator: my_sample: 2</code> for a <code>1 : 2</code> dilution). Note,
it is important that the dilution step is given as the inverse (i.e. <em>not</em> as <code>1:2 or 1/2</code> or something else! )</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>step</code></strong> :&ensp;<code>float</code> or <code>np.ndarray</code></dt>
<dd>The dilution step used.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dilution</code></strong> :&ensp;<code>float</code> or <code>np.ndarray</code></dt>
<dd>The currently used dilution step.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dilution( self, step : float or np.ndarray or tuple = None ):
    &#34;&#34;&#34;
    Gets or sets the dilution steps used. This must be a `float` fraction
    e.g. `0.5` for a `1 : 2` dilution series or `0.1` for a `1 : 10` series etc.
    If there are multiple steps because there is a gap in the dilution series. It is 
    necessary to supply a step for each group individually e.g. `[1,0.5,0.25,0.0625,0.03125]`.
    if there are 5 dilution steps (originally six but 0.125 was discarded).

    Note, both of the above also work with the inverse dilutions e.g. `2` or `[1,2,4,16,32]`.
    
    By default the `qpcr.Calibrator` tries to infer the dilutions automatically.
    This only works, however, if the calibrator groupnames specify `calibrator: {some name} : dil` where
    `dil` is the inverse dilution step (e.g. `calibrator: my_sample: 2` for a `1 : 2` dilution). Note,
    it is important that the dilution step is given as the inverse (i.e. *not* as `1:2 or 1/2` or something else! )

    Parameters
    ----------
    step : float or np.ndarray
        The dilution step used.  

    Returns
    -------
    dilution : float or np.ndarray
        The currently used dilution step.
    &#34;&#34;&#34;
    
    dilution = self._dilution(step)
    self._orig_dilution = self._dilution_step

    # if the dilution is now set to a valid
    # value we set the _manual_dilution_set check to True
    if dilution is not None:
        self._manual_dilution_set = True 
    return dilution</code></pre>
</details>
</dd>
<dt id="qpcr.Calibrator.efficiencies"><code class="name flex">
<span>def <span class="ident">efficiencies</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>The currently stored efficienies.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def efficiencies( self ):
    &#34;&#34;&#34;
    Returns
    ------
    dict
        The currently stored efficienies.
    &#34;&#34;&#34;
    return self._eff_dict</code></pre>
</details>
</dd>
<dt id="qpcr.Calibrator.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, which='efficiencies')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Either the stored efficiencies (if
<code>which = "efficiencies"</code>) or
the computed values of newly computed
efficiencies (if <code>which = "values"</code>).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get( self, which = &#34;efficiencies&#34; ):
    &#34;&#34;&#34;
    Returns
    -------
    dict
        Either the stored efficiencies (if 
        `which = &#34;efficiencies&#34;`) or 
        the computed values of newly computed
        efficiencies (if `which = &#34;values&#34;`).
    &#34;&#34;&#34;
    if which == &#34;efficiencies&#34;:
        return self.efficiencies()
    elif which == &#34;values&#34;:
        return self.computed_values()</code></pre>
</details>
</dd>
<dt id="qpcr.Calibrator.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, filename, merge:Â boolÂ =Â True, supersede:Â boolÂ =Â False)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a <code>csv</code> file of previously computed efficiencies.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>The filepath to load efficiencies from.</dd>
<dt><strong><code>merge</code></strong> :&ensp;<code>bool</code></dt>
<dd>In case efficiencies are already loaded, merge the
new and existing ones. If <code>False</code> the current ones will
be replaced completely.</dd>
<dt><strong><code>supersede</code></strong> :&ensp;<code>bool </code></dt>
<dd>In case efficiencies of the same assay are already loaded
they will be overwritten by the newly incoming ones if <code>supersede = True</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, filename, merge : bool = True, supersede : bool = False ):
    &#34;&#34;&#34;
    Loads a `csv` file of previously computed efficiencies.

    Parameters
    -------
    filename : str
        The filepath to load efficiencies from.
    merge : bool
        In case efficiencies are already loaded, merge the
        new and existing ones. If `False` the current ones will
        be replaced completely.
    supersede : bool 
        In case efficiencies of the same assay are already loaded
        they will be overwritten by the newly incoming ones if `supersede = True`.
    &#34;&#34;&#34;
    try: 
        current = self._load(filename)
        if merge:
            current = { **self._eff_dict, **current } if supersede else { **current, **self._eff_dict }
        self.adopt( current )
        self._loaded_file = filename
        return current
    except: 
        aw.HardWarning(&#34;Calibrator:unknown_filetype&#34;, filename = filename )</code></pre>
</details>
</dd>
<dt id="qpcr.Calibrator.merge"><code class="name flex">
<span>def <span class="ident">merge</span></span>(<span>self, *filenames, outfile=None, adopt=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Merges multiple efficiency files together into a single one.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filenames</code></strong> :&ensp;<code>iterable</code></dt>
<dd>Filepaths to load data from which should be merged together.</dd>
<dt><strong><code>outfile</code></strong> :&ensp;<code>str </code></dt>
<dd>The filepath in which to store the merged efficiencies.
Not saved if set to <code>None</code>.</dd>
<dt><strong><code>adopt</code></strong> :&ensp;<code>bool</code></dt>
<dd>Will adopt the merged dictionary as its own if <code>True</code> (default).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>all_effiencies</code></strong> :&ensp;<code>dict</code></dt>
<dd>The merged dictionary of all efficiencies from all files.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge( self, *filenames, outfile = None, adopt = True ):
    &#34;&#34;&#34;
    Merges multiple efficiency files together into a single one.

    Parameters
    -------
    filenames : iterable
        Filepaths to load data from which should be merged together.
    
    outfile : str 
        The filepath in which to store the merged efficiencies.
        Not saved if set to `None`.
    
    adopt : bool
        Will adopt the merged dictionary as its own if `True` (default).

    Returns
    -------
    all_effiencies : dict
        The merged dictionary of all efficiencies from all files.
    &#34;&#34;&#34;
    all_efficiencies = {}
    for filename in filenames: 
        new = self._load( filename )
        all_efficiencies = { **all_efficiencies, **new }

    if outfile is not None: 
        self._save( outfile, all_efficiencies )

    if adopt:
        self.adopt( all_efficiencies ) 

    return all_efficiencies</code></pre>
</details>
</dd>
<dt id="qpcr.Calibrator.pipe"><code class="name flex">
<span>def <span class="ident">pipe</span></span>(<span>self, assay:Â <a title="qpcr.Assay" href="#qpcr.Assay">Assay</a>, remove_calibrators:Â boolÂ =Â True, ignore_uncalibrated:Â boolÂ =Â False)</span>
</code></dt>
<dd>
<div class="desc"><p>A wrapper for calibrate / assign.</p>
<p>This method will first try to assign pre-computed efficiencies
and if no matching ones are found it will try to calculate a new efficiency
from the assay. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>assay</code></strong> :&ensp;<code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code></dt>
<dd>A <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object.</dd>
<dt><strong><code>remove_calibrators</code></strong> :&ensp;<code>bool</code></dt>
<dd>If calibrators are present in the assay alongside other groups,
remove the calibrator replicates after assignment or efficiency calculation.</dd>
<dt><strong><code>ignore_uncalibrated</code></strong> :&ensp;<code>bool</code></dt>
<dd>If <code>True</code> assays that could neither be newly calibrated nor be assigned an existing
efficiency will be ignored. Otherwise, and error will be raised.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>assay</code></strong> :&ensp;<code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code></dt>
<dd>The now calibrated <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pipe( self, assay : Assay, remove_calibrators : bool = True, ignore_uncalibrated : bool = False ):
    &#34;&#34;&#34;
    A wrapper for calibrate / assign.

    This method will first try to assign pre-computed efficiencies
    and if no matching ones are found it will try to calculate a new efficiency
    from the assay. 

    Parameters
    ----------
    assay : qpcr.Assay
        A `qpcr.Assay` object.
    remove_calibrators : bool
        If calibrators are present in the assay alongside other groups, 
        remove the calibrator replicates after assignment or efficiency calculation. 
    ignore_uncalibrated : bool
        If `True` assays that could neither be newly calibrated nor be assigned an existing
        efficiency will be ignored. Otherwise, and error will be raised. 

    Returns
    -------
    assay : qpcr.Assay
        The now calibrated `qpcr.Assay`.
    &#34;&#34;&#34;
    if self._eff_dict != {}:
        # first try to assign (will leave the assay unchanged if nothing is found)
        eff = self._get_efficiency( assay )
        if eff is not None: 
            assay = self.assign( assay, remove_calibrators = remove_calibrators )
        else: 
            try:
                assay = self.calibrate( assay, remove_calibrators = remove_calibrators )
            except: 
                if not ignore_uncalibrated:
                    aw.HardWarning(&#34;Calibrator:cannot_process_assay&#34;, id = assay.id() )
                else: 
                    aw.SoftWarning(&#34;Calibrator:cannot_process_assay&#34;, id = assay.id() )
    else:
        try: 
            assay = self.calibrate( assay, remove_calibrators = remove_calibrators )
        except:
            if not ignore_uncalibrated:
                aw.HardWarning(&#34;Calibrator:cannot_process_assay&#34;, id = assay.id() )
            else: 
                aw.SoftWarning(&#34;Calibrator:cannot_process_assay&#34;, id = assay.id() )
    return assay</code></pre>
</details>
</dd>
<dt id="qpcr.Calibrator.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, mode:Â strÂ =Â 'interactive', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A shortcut to call a <code>qpcr.Plotters.EfficiencyLines</code> plotter
to visualise the regression lines from de novo efficiency computations.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>The plotting mode. May be either "static" (matplotlib) or "interactive" (plotly).</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Any additional keyword arguments to be passed to the plotter.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>fig</code></strong> :&ensp;<code>plt.figure</code> or <code>plotly.figure</code></dt>
<dd>The figure generated by <code>EfficiencyLines</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot( self, mode : str = &#34;interactive&#34;, **kwargs ):
    &#34;&#34;&#34;
    A shortcut to call a `qpcr.Plotters.EfficiencyLines` plotter
    to visualise the regression lines from de novo efficiency computations.

    Parameters
    -------
    mode : str
        The plotting mode. May be either &#34;static&#34; (matplotlib) or &#34;interactive&#34; (plotly).
    **kwargs
        Any additional keyword arguments to be passed to the plotter.

    Returns
    -------
    fig : plt.figure or plotly.figure
        The figure generated by `EfficiencyLines`.
    &#34;&#34;&#34;
    plotter = Plotters.EfficiencyLines( mode = mode )
    plotter.link( self ) 
    fig = plotter.plot( **kwargs )
    return fig </code></pre>
</details>
</dd>
<dt id="qpcr.Calibrator.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the Calibrator to initial settings. This will
clear all stored efficiency values and computed data!</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset( self ):
    &#34;&#34;&#34;
    Resets the Calibrator to initial settings. This will 
    clear all stored efficiency values and computed data!
    &#34;&#34;&#34;
    self.__init__()</code></pre>
</details>
</dd>
<dt id="qpcr.Calibrator.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, filename:Â strÂ =Â None, mode:Â strÂ =Â 'write')</span>
</code></dt>
<dd>
<div class="desc"><p>Saves the calculated efficiencies to a <code>csv</code> file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>The filepath in which to store the efficiencies. If a file
was already loaded then by default the same file will be
used to save values again.</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>Can be either <code>"write"</code> to fully overwrite an existing file,
with the newly computed data, or <code>"append"</code> to only add newly
computed efficiencies.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save( self, filename : str = None , mode : str = &#34;write&#34; ):
    &#34;&#34;&#34;
    Saves the calculated efficiencies to a `csv` file.

    Parameters
    -------
    filename : str
        The filepath in which to store the efficiencies. If a file
        was already loaded then by default the same file will be 
        used to save values again.
    
    mode : str
        Can be either `&#34;write&#34;` to fully overwrite an existing file,
        with the newly computed data, or `&#34;append&#34;` to only add newly 
        computed efficiencies.
    &#34;&#34;&#34;

    if filename is None and self._loaded_file is not None: 
        filename = self._loaded_file

    if mode == &#34;write&#34;:
        self._save( filename, self._eff_dict )

    elif mode == &#34;append&#34;:
        current = self._load(filename)
        new = { **current, **self._eff_dict }
        self._save( filename, new )

    else: 
        aw.SoftWarning(&#34;Calibrator:unknown_savemode&#34;, mode = mode )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="qpcr.DataReader"><code class="flex name class">
<span>class <span class="ident">DataReader</span></span>
</code></dt>
<dd>
<div class="desc"><p>Handles reading a single file containing input data
for <code><a title="qpcr" href="#qpcr">qpcr</a></code>. </p>
<h2 id="note">Note</h2>
<p>This is a top-level class that is designed
as the central port through which data is read into <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> objects
from both regular and irregular, single- and multi-assay files.
This is the suggested way to read your data for most users, which should
work in most cases.</p>
<p>However, due to the automated setup of the inferred Readers there may be cases where you
will either have a hard time or be unable to read your datafiles using the <code><a title="qpcr.DataReader" href="#qpcr.DataReader">DataReader</a></code>.
In such cases, don't try too long to make it work with the DataReader,
just use one of the <code><a title="qpcr.Readers" href="Readers/index.html">qpcr.Readers</a></code> or even <code><a title="qpcr.Parsers" href="Parsers/index.html">qpcr.Parsers</a></code>directly.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataReader(aux._ID):
    &#34;&#34;&#34;
    Handles reading a single file containing input data
    for `qpcr`. 
    
    Note
    -----
    This is a top-level class that is designed
    as the central port through which data is read into `qpcr.Assay` objects
    from both regular and irregular, single- and multi-assay files.
    This is the suggested way to read your data for most users, which should
    work in most cases.
    
    However, due to the automated setup of the inferred Readers there may be cases where you 
    will either have a hard time or be unable to read your datafiles using the `DataReader`. 
    In such cases, don&#39;t try too long to make it work with the DataReader, 
    just use one of the `qpcr.Readers` or even `qpcr.Parsers`directly.  

    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._replicates = None
        self._names = None
        self._Reader = None             # the functional core will be either a Reader
        self._Data = {}                 # the _Data attribute will store any output from the Reader as a dictionary with filename : data structure.
        self._tmp_data = None           # this will not attempt to distinguish between assays / normalisers, or anything. It&#39;s just an archive of whatever data we got.
                                        # by default data is not stored by the DataReader, it&#39;s read only pipes through, but data can be stored using the .store() method.

    def Reader(self, Reader = None):
        &#34;&#34;&#34;
        Gets or sets the functional core Reader    
        &#34;&#34;&#34;
        if Reader is not None:
            self._Reader = Reader
        return self._Reader

    def clear(self):
        &#34;&#34;&#34;
        Clears all data that was extracted
        &#34;&#34;&#34;
        self._Data = {}
    
    def reset(self):
        &#34;&#34;&#34;
        Resets the core Reader
        &#34;&#34;&#34;
        self._Reader = None
    
    def prune(self):
        &#34;&#34;&#34;
        Resets the DataReader completely
        &#34;&#34;&#34;
        self.__init__()

    def store(self):
        &#34;&#34;&#34;
        Will store the read data. 


        Note 
        -------
        `DataReader` does NOT have a specific data storage facility to distinguish between 
        assays / normalisers, data types, etc. It simply keeps a dictionary of `{filename : data}`
        that can be accessed. This is designed in case multiple files should be read using the same 
        `DataReader` to allow an easier access of the data in case the data outputs are of the same type.

        However, the main intended application of `DataReader` is to use the`read` method&#39;s returned data directly.
        &#34;&#34;&#34;
        self._Data.update(self._tmp_data)
    
    def get(self):
        &#34;&#34;&#34;
        Returns the stored data
        &#34;&#34;&#34;
        return self._Data

    def read(self, filename : str, multi_assay : bool = False, big_table : bool = False, decorator : (bool or str) = None, reset = False, **kwargs):
        &#34;&#34;&#34;
        Reads an input file and extracts available datasets using the
        specified `Reader` or by setting up an approproate `Reader`. 

        Parameters
        ----------
        filename : str
            A filepath to an input datafile.

        multi_assay : bool
            Set to `True` if the file contains multiple assays you wish to read.

        big_table : bool
            Set to `True` if the file is a &#34;Big Table&#34; file. 
            Check out the documentation of the `qpcr.Readers` for more 
            information on &#34;Big Table&#34; files.

        decorator : str or bool
            Set if the file is decorated. This can be set either to `True` for `multi_assay` and multi-sheet (excel) or `big_table` files,
            or it can be set to a valid `qpcr decorator` for single assay files or single-sheet files.
            Check out the documentation of the `qpcr.Parsers` for more information on decorators.

        reset : bool
            If multiple input files should be read but they do not all 
            adhere to the same filetype / datastructure, use `reset = True` 
            to set up a new Reader each time `read` is called.

        **kwargs
            Any additional keyword arguments to be passed to the core Reader.
            Note, while this tries to be utmost versatile there is a limitation
            to costumizibility through the kwargs. If you require streamlined datareading
            use dedicated `qpcr.Readers` and/or `qpcr.Parsers` directly.
        &#34;&#34;&#34;
        self._src = filename
        # vet filesuffix
        suffix = self._filesuffix()
        if suffix not in supported_filetypes:
            aw.HardWarning(&#34;MultiReader:unknown_datafile&#34;, file = self._src)
        
        if reset or self._Reader is None: 
            self.reset()
            self._setup_Reader(
                                multi_assay = multi_assay, 
                                big_table = big_table,
                                decorator = decorator,
                                **kwargs
                            )
        
        # read file and return data
        data = self._Reader._DataReader( 
                                            filename = self._src, 
                                            decorator = decorator, 
                                            **kwargs 
                                    )

        self._tmp_data = {self._src : data}
        return data

    def _filesuffix(self):
        &#34;&#34;&#34;
        Returns the filesuffix
        &#34;&#34;&#34;
        return self._src.split(&#34;.&#34;)[-1]

    def _setup_Reader(self, **kwargs):
        &#34;&#34;&#34;
        Sets up the core Reader 
        &#34;&#34;&#34;
        suffix = self._filesuffix()

        use_multi = aux.from_kwargs(&#34;multi_assay&#34;, False, kwargs, rm = True)
        is_bigtable = aux.from_kwargs(&#34;big_table&#34;, False, kwargs, rm = True)

        if suffix == &#34;csv&#34;:
            if not use_multi and not is_bigtable:
                reader = Readers.SingleReader()
            elif is_bigtable:
                reader = Readers.BigTableReader()
            else:
                reader = Readers.MultiReader()

        elif suffix == &#34;xlsx&#34;:
            
            # check if not only a single sheet should be read
            multi_sheet = &#34;sheet_name&#34; not in kwargs

            if use_multi and multi_sheet:
                reader = Readers.MultiSheetReader()
            elif use_multi and not multi_sheet:
                reader = Readers.MultiReader()
            elif is_bigtable:
                reader = Readers.BigTableReader()
            else:
                reader = Readers.SingleReader()
        self._Reader = reader</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>qpcr._auxiliary._ID</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qpcr.DataReader.Reader"><code class="name flex">
<span>def <span class="ident">Reader</span></span>(<span>self, Reader=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets or sets the functional core Reader</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Reader(self, Reader = None):
    &#34;&#34;&#34;
    Gets or sets the functional core Reader    
    &#34;&#34;&#34;
    if Reader is not None:
        self._Reader = Reader
    return self._Reader</code></pre>
</details>
</dd>
<dt id="qpcr.DataReader.clear"><code class="name flex">
<span>def <span class="ident">clear</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Clears all data that was extracted</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear(self):
    &#34;&#34;&#34;
    Clears all data that was extracted
    &#34;&#34;&#34;
    self._Data = {}</code></pre>
</details>
</dd>
<dt id="qpcr.DataReader.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the stored data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self):
    &#34;&#34;&#34;
    Returns the stored data
    &#34;&#34;&#34;
    return self._Data</code></pre>
</details>
</dd>
<dt id="qpcr.DataReader.prune"><code class="name flex">
<span>def <span class="ident">prune</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the DataReader completely</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prune(self):
    &#34;&#34;&#34;
    Resets the DataReader completely
    &#34;&#34;&#34;
    self.__init__()</code></pre>
</details>
</dd>
<dt id="qpcr.DataReader.read"><code class="name flex">
<span>def <span class="ident">read</span></span>(<span>self, filename:Â str, multi_assay:Â boolÂ =Â False, big_table:Â boolÂ =Â False, decorator:Â boolÂ =Â None, reset=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads an input file and extracts available datasets using the
specified <code><a title="qpcr.Reader" href="#qpcr.Reader">Reader</a></code> or by setting up an approproate <code><a title="qpcr.Reader" href="#qpcr.Reader">Reader</a></code>. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>A filepath to an input datafile.</dd>
<dt><strong><code>multi_assay</code></strong> :&ensp;<code>bool</code></dt>
<dd>Set to <code>True</code> if the file contains multiple assays you wish to read.</dd>
<dt><strong><code>big_table</code></strong> :&ensp;<code>bool</code></dt>
<dd>Set to <code>True</code> if the file is a "Big Table" file.
Check out the documentation of the <code><a title="qpcr.Readers" href="Readers/index.html">qpcr.Readers</a></code> for more
information on "Big Table" files.</dd>
<dt><strong><code>decorator</code></strong> :&ensp;<code>str</code> or <code>bool</code></dt>
<dd>Set if the file is decorated. This can be set either to <code>True</code> for <code>multi_assay</code> and multi-sheet (excel) or <code>big_table</code> files,
or it can be set to a valid <code><a title="qpcr" href="#qpcr">qpcr</a> decorator</code> for single assay files or single-sheet files.
Check out the documentation of the <code><a title="qpcr.Parsers" href="Parsers/index.html">qpcr.Parsers</a></code> for more information on decorators.</dd>
<dt><strong><code>reset</code></strong> :&ensp;<code>bool</code></dt>
<dd>If multiple input files should be read but they do not all
adhere to the same filetype / datastructure, use <code>reset = True</code>
to set up a new Reader each time <code>read</code> is called.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Any additional keyword arguments to be passed to the core Reader.
Note, while this tries to be utmost versatile there is a limitation
to costumizibility through the kwargs. If you require streamlined datareading
use dedicated <code><a title="qpcr.Readers" href="Readers/index.html">qpcr.Readers</a></code> and/or <code><a title="qpcr.Parsers" href="Parsers/index.html">qpcr.Parsers</a></code> directly.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read(self, filename : str, multi_assay : bool = False, big_table : bool = False, decorator : (bool or str) = None, reset = False, **kwargs):
    &#34;&#34;&#34;
    Reads an input file and extracts available datasets using the
    specified `Reader` or by setting up an approproate `Reader`. 

    Parameters
    ----------
    filename : str
        A filepath to an input datafile.

    multi_assay : bool
        Set to `True` if the file contains multiple assays you wish to read.

    big_table : bool
        Set to `True` if the file is a &#34;Big Table&#34; file. 
        Check out the documentation of the `qpcr.Readers` for more 
        information on &#34;Big Table&#34; files.

    decorator : str or bool
        Set if the file is decorated. This can be set either to `True` for `multi_assay` and multi-sheet (excel) or `big_table` files,
        or it can be set to a valid `qpcr decorator` for single assay files or single-sheet files.
        Check out the documentation of the `qpcr.Parsers` for more information on decorators.

    reset : bool
        If multiple input files should be read but they do not all 
        adhere to the same filetype / datastructure, use `reset = True` 
        to set up a new Reader each time `read` is called.

    **kwargs
        Any additional keyword arguments to be passed to the core Reader.
        Note, while this tries to be utmost versatile there is a limitation
        to costumizibility through the kwargs. If you require streamlined datareading
        use dedicated `qpcr.Readers` and/or `qpcr.Parsers` directly.
    &#34;&#34;&#34;
    self._src = filename
    # vet filesuffix
    suffix = self._filesuffix()
    if suffix not in supported_filetypes:
        aw.HardWarning(&#34;MultiReader:unknown_datafile&#34;, file = self._src)
    
    if reset or self._Reader is None: 
        self.reset()
        self._setup_Reader(
                            multi_assay = multi_assay, 
                            big_table = big_table,
                            decorator = decorator,
                            **kwargs
                        )
    
    # read file and return data
    data = self._Reader._DataReader( 
                                        filename = self._src, 
                                        decorator = decorator, 
                                        **kwargs 
                                )

    self._tmp_data = {self._src : data}
    return data</code></pre>
</details>
</dd>
<dt id="qpcr.DataReader.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the core Reader</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    &#34;&#34;&#34;
    Resets the core Reader
    &#34;&#34;&#34;
    self._Reader = None</code></pre>
</details>
</dd>
<dt id="qpcr.DataReader.store"><code class="name flex">
<span>def <span class="ident">store</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Will store the read data. </p>
<h2 id="note">Note</h2>
<p><code><a title="qpcr.DataReader" href="#qpcr.DataReader">DataReader</a></code> does NOT have a specific data storage facility to distinguish between
assays / normalisers, data types, etc. It simply keeps a dictionary of <code>{filename : data}</code>
that can be accessed. This is designed in case multiple files should be read using the same
<code><a title="qpcr.DataReader" href="#qpcr.DataReader">DataReader</a></code> to allow an easier access of the data in case the data outputs are of the same type.</p>
<p>However, the main intended application of <code><a title="qpcr.DataReader" href="#qpcr.DataReader">DataReader</a></code> is to use the<code>read</code> method's returned data directly.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def store(self):
    &#34;&#34;&#34;
    Will store the read data. 


    Note 
    -------
    `DataReader` does NOT have a specific data storage facility to distinguish between 
    assays / normalisers, data types, etc. It simply keeps a dictionary of `{filename : data}`
    that can be accessed. This is designed in case multiple files should be read using the same 
    `DataReader` to allow an easier access of the data in case the data outputs are of the same type.

    However, the main intended application of `DataReader` is to use the`read` method&#39;s returned data directly.
    &#34;&#34;&#34;
    self._Data.update(self._tmp_data)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="qpcr.Normaliser"><code class="flex name class">
<span>class <span class="ident">Normaliser</span></span>
</code></dt>
<dd>
<div class="desc"><p>Handles the second step in Delta-Delta-Ct (normalisation against normaliser assays).</p>
<h2 id="note">Note</h2>
<p>This requires that all have been analysed in the same way before!</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Normaliser(aux._ID):
    &#34;&#34;&#34;
    Handles the second step in Delta-Delta-Ct (normalisation against normaliser assays).
    
    Note
    -----
    This requires that all have been analysed in the same way before!
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()

        self._Normalisers = []
        self._Assays = []
        
        self._Results = Results()
        
        # the actually used normaliser 
        # which will be a pre-processed version
        # from all supplied normalsier assays
        # by default this will be the averaged version
        # of all normaliser assays (but this can be changed)  
        self._normaliser = None

        # setup defaults
        self._prep_func = self._preprocess_normalisers
        self._norm_func = self._divide_by_normaliser

        # store the state if a custom norm func was provided
        self._norm_func_is_set = False


    def clear(self):
        &#34;&#34;&#34;
        Will clear the presently stored results
        &#34;&#34;&#34;
        self._Results = Results()

    
    def prune(self, assays = True, normalisers = True, results = True):
        &#34;&#34;&#34;
        Will clear assays, normalisers, and/or results
        assays : bool
            Will clear any sample assays if True (default).
        
        results : bool
            Will clear any computed results if True (default).
        
        normalisers : bool
            Will clear any normalisers if True (default).
        &#34;&#34;&#34;
        if assays:
            self._Assays = []
        if normalisers: 
            self._Normalisers = []
        if results: 
            self.clear()


    def get(self, copy=False):
        &#34;&#34;&#34;
        Parameters
        ----------
        copy : bool
            Will return a deepcopy of the Results object if `copy = True` (default is `copy = False`).
        
        Returns
        -------
        Results : qpcr.Results
            A `qpcr.Results` object containing the normalised dataframe
        &#34;&#34;&#34;
        if copy: 
            return deepcopy(self._Results)
        return self._Results
    
    def link(self, assays:(list or tuple or Analyser) = None, normalisers:(list or tuple or Analyser) = None):
        &#34;&#34;&#34;
        Links either normalisers or assays-of-interest `qpcr.Assay` objects coming from the same `qpcr.Analyser`.

        Parameters
        ----------
        assays : list or tuple or qpcr.Analyser
            A list of `qpcr.Assay` objects coming from a `qpcr.Analyser` or the `qpcr.Analyser` itself. These assays will be normalised against a normaliser.
        
        normalisers : list or tuple or qpcr.Analyser
            A list of `qpcr.Assay` objects coming from a `qpcr.Analyser` or the `qpcr.Analyser` itself. These assays will be used as normalisers. These will be
            combined into one single pseudo-normaliser which will then be used to normalise the assays. The method of 
            combining the normalisers can be specified using the `qpcr.Normaliser.prep_func` method.
        &#34;&#34;&#34;
        # convert to lists (since the _link methods really want lists)
        if assays is not None and not isinstance(assays, (list, tuple)): 
            assays = [assays]
        self._link_assays(assays)

        if normalisers is not None and not isinstance(normalisers, (list, tuple)): 
            normalisers = [normalisers]
        self._link_normaliser(normalisers)
    
    def prep_func(self, f = None):
        &#34;&#34;&#34;
        Sets any defined function for combined normaliser pre-processing.
        If no `f` is provided, it returns the current `prep_func`.

        Parameters
        ----------
        f : function
            The function may accept one list of `qpcr.Assay` objects, and must return 
            either an `qpcr.Assay` object directly or a `pandas.Dataframe` (that will be migrated to an `qpcr.Assay`).
            The returned dataframe must contain a `&#34;dCt&#34;` column which stores the delta-Ct values ultimately used as 
            &#34;normaliser assay&#34;.  
        &#34;&#34;&#34;
        
        if aux.same_type(f, aux.fileID):
            self._prep_func = f
        elif f is None:
            return f
        else: 
            aw.HardWarning(&#34;Normaliser:cannot_set_prep_func&#34;, func = f)

    def norm_func(self, f = None):
        &#34;&#34;&#34;
        Sets any defined function to perform normalisation of assays against normalisers.
        If no `f` is provided, it returns the current `norm_func`.

        Parameters
        ----------
        f : function
            The function may accept two `qpcr.Assay` objects (named `assay` and `normaliser` which will be forwarded from the `qpcr.Normaliser`). 
            The function may also accept one `pandas.DataFrame` (named `df`) containing two numeric columns of delta-Ct values from a sample-assay (named &#34;s&#34;) and a normaliser-assay (named &#34;n&#34;),
            as well as a group identifier column (named &#34;group&#34;). 
            Whatever inputs it works with, it must return a named numeric `pandas.Series` of the same length as entries in the Assays&#39; dataframes. 
            
            ##### Note
            Support for the dataframe direct usage will be dropped at some point in the future. 

            By default `s/n` is used, where `s` is a column of sample-assay deltaCt values, 
            and `n` is the corresponding `&#34;dCt&#34;` column from the normaliser.

        &#34;&#34;&#34;
        if aux.same_type(f, aux.fileID):
            self._norm_func = f
            self._norm_func_is_set = True
        elif f is None:
            return f
        else: 
            aw.HardWarning(&#34;Normaliser:cannot_set_norm_func&#34;, func = f)

    def normalise(self, mode = &#34;pair-wise&#34;, **kwargs):
        &#34;&#34;&#34;
        Normalises all linked assays against the combined pseudo-normaliser 
        (by default, unless a custom `prep_func` has been specified), 
        and stores the results in a new Results object.

        Parameters
        ----------
        mode : str
            The normalisation mode to use. This can be either `pair-wise` (default), 
            or `combinatoric`, or `permutative`.
            `pair-wise` will normalise replicates only by their partner (i.e. first against first, 
            second by second, etc.). `combinatoric` will normalise all possible combinations of a replicate 
            with all partner replicates of the same group from a normaliser (i.e. first against first, then second, then third, etc.).
            This will generate `n^2` normalised Delta-Delta-Ct values, where `n` is the number of replicates in a group.
            `permutative` will scramble the normaliser replicates randomly and then normalise pair-wise. This mode supports 
            a parameter `k` which specifies the times this process should be repeated, thus generating `n * k` normalised Delta-Delta-Ct values.
            Also, through setting `replace = True` replacement may be allowed during normaliser scrambling.
            Note, this setting will be ignored if a custom `norm_func` is provided.

        **kwargs
            Any additional keyword arguments that may be passed to a custom 
            `norm_func` and `prep_func` (both will receive the kwargs!).
        &#34;&#34;&#34;
        if self._normaliser is None: 
            self._normaliser = self._prep_func(self._Normalisers, **kwargs)
            self._vet_normaliser()

        if self._Assays == [] or self._normaliser is None:
            aw.SoftWarning(&#34;Normaliser:no_data_yet&#34;)
            
        # check which kind of norm_func we should use
        if not self._norm_func_is_set:
            # pair-wise is already default so we don&#39;t check...
            if mode == &#34;combinatoric&#34;:
                self._norm_func = self._tile_normalise
                tiled = deepcopy(self._Assays[0])
                tiled.tile()
                self._Results.adopt_names( tiled )
                del tiled 
            elif mode == &#34;permutative&#34;:
                self._norm_func = self._permutate_normalise
                n = aux.from_kwargs(&#34;k&#34;, 1, kwargs)
                tiled = deepcopy(self._Assays[0])
                tiled.stack(n)
                self._Results.adopt_names( tiled )
                del tiled 
            elif mode == &#34;pair-wise&#34;:
                self._Results.adopt_names( self._Assays[0] )
        else:
            self._Results.adopt_names( self._Assays[0] )

        self._Results.drop_cols()

        # perform normalisation for each assay 
        for assay in self._Assays:

            # get data
            # assay_df = assay.get()

            # apply normalisation (delta-delta-Ct)
            normalised = self._norm_func_wrapper(
                                                    assay = assay, 
                                                    normaliser = self._normaliser, 
                                                    **kwargs
                                            )

            # # store results in _Results
            # self._store_to_Results(assay, normalised)

            # and store results also in the Assay itself
            assay.add_ddCt( self._normaliser.id(), normalised )

            # and store to results
            self._Results.add_ddCt(assay)

    def _vet_normaliser(self):
        &#34;&#34;&#34;
        Checks if the normaliser is already a qpcr.Assay object, and if not
        convert it to one. 
        &#34;&#34;&#34;
        if not isinstance(self._normaliser, Assay):
            tmp = Assay()
            tmp.adopt(self._normaliser)
            tmp.id(&#34;combined_normaliser&#34;)
            self._normaliser = tmp
            

    def _store_to_Results(self, assay, normalised):
        &#34;&#34;&#34;
        Stores computed Delta-delta-ct (normalisation)
        into the _Results object.
        &#34;&#34;&#34;
        column_name = f&#34;{assay.id()}_rel_{self._normaliser.id()}&#34;
        normalised = normalised.rename(column_name)
        self._Results.add(normalised)


    def _norm_func_wrapper(self, assay, normaliser, **kwargs):
        &#34;&#34;&#34;
        The wrapper that will apply the _norm_func to the sample and normaliser dataframes and return a pandas series of normalised values
        &#34;&#34;&#34;
        # for double normalised we want the same columns as dct and norm...

        # FUTURE DROP HERE
        # In the future we will not be creating the tmp_df 
        # directly anymore, but intead will only pass assay and normaliser. 

        sample_dCt = assay.dCt()
        groups = assay.groups( as_set = False )
        norm_dCt = normaliser.dCt()

        tmp_df = pd.DataFrame( dict( group = groups, s = sample_dCt, n = norm_dCt )  )

        results = self._norm_func(df = tmp_df, assay = assay, normaliser = normaliser, **kwargs)

        # this is the old call from before factoring out to Assays 
        # dCt_col, norm_col = self._prep_columns(sample_assay, dCt_col, norm_col)

        # tmp_df = normaliser.join(sample_assay, lsuffix=&#34;_s&#34;)
        # # tmp_df = sample_assay.join(normaliser, rsuffix = &#34;_n&#34;)
        # results = self._norm_func(tmp_df[[dCt_col, norm_col]], **kwargs)
        return results

    def _tile_normalise(self, assay, normaliser, **kwargs):
        &#34;&#34;&#34;
        Normalises assays and normalisers group wise, iteratively normalising
        each individual replicate against all replicates from the normaliser.
        This generates `n**2` normalised Delta-Delta-Ct values where `n` is the
        group size. 
        &#34;&#34;&#34;

        # get the untiled data
        adf = assay.get()
        groups = assay.groups()
        ndf = normaliser.get()
        
        # get the column to draw data from
        col = aux.from_kwargs(&#34;col&#34;, &#34;dCt&#34;, kwargs)

        # tile the assay
        assay.tile() 

        # generate results array
        ddCts = np.zeros( len( assay.get() ) )
       
        # now compute ddCt
        idx = 0
        for group in groups: 

            a_dCt = adf.query( f&#34;group == {group}&#34; )[ col ].to_numpy()
            n_dCt = ndf.query( f&#34;group == {group}&#34; )[ col ].to_numpy()
            
            for a in a_dCt:
                for n in n_dCt:

                    r = a / n
                    
                    try: 
                        ddCts[idx] = r 
                    except: 
                        break 

                    idx += 1

        ddCts = pd.Series( ddCts )       
        ddCts.name = &#34;ddCt&#34;
        return ddCts
        
    
    def _permutate_normalise(self, assay, normaliser, **kwargs):
        &#34;&#34;&#34;
        Scrambles randomly the normaliser&#39;s replicate values group-wise 
        &#34;&#34;&#34;
        # get the untiled data
        adf = assay.get()
        groups = assay.groups()
        ndf = normaliser.get()
        
        # get the column to draw data from
        col = aux.from_kwargs(&#34;col&#34;, &#34;dCt&#34;, kwargs)

        # stack the assay
        # get the number of permutations to perform
        n = aux.from_kwargs(&#34;k&#34;, 1, kwargs)  
        assay.stack(n) 

        # get replace argument for random choice
        replace = aux.from_kwargs(&#34;replace&#34;, False, kwargs)

        # generate results array
        ddCts = np.zeros( len( assay.get() ) )
       
        # now compute ddCt
        idx = 0
        for group in groups: 
            for i in range(n):
                a_dCt = adf.query( f&#34;group == {group}&#34; )[ col ].to_numpy()
                n_dCt = ndf.query( f&#34;group == {group}&#34; )[ col ].to_numpy()
                
                # randomly scramble the normaliser replicates
                np.random.seed( defaults.default_seed )
                if replace:
                    # in case of replacement, we generate a normal 
                    # distribution from the replicate values and weigh
                    # the random.choice with the given probabilities of the values 
                    # being chosen. Since the probabilities do not themselves correspond to 1
                    # we normalise against the limited available subset to generate a probabilities
                    # array that sums up to 1.
                    mu, sd = stats.norm.fit(n_dCt)
                    probs = stats.norm.pdf(n_dCt, loc = mu, scale = sd)
                    probs = probs / np.sum(probs)
                    n_dCt = np.random.choice( n_dCt, size = n_dCt.size, replace = replace, p = probs )
                else:
                    n_dCt = np.random.choice( n_dCt, size = n_dCt.size, replace = replace )
                length = a_dCt.size
                r = a_dCt / n_dCt
                        
                try: 
                    ddCts[idx : idx + length] = r 
                except: 
                    break 

                idx += length

        ddCts = pd.Series( ddCts )       
        ddCts.name = &#34;ddCt&#34;
        return ddCts

    def _divide_by_normaliser(self, df, **kwargs):
        &#34;&#34;&#34;
        Performs normalisation of sample s against normaliser n
        s and n are specified as two pandas dataframe columns
        Note, that the dataframe must ONLY contain these two columns, first the dCt sample, then the normaliser!
        (default _norm_func)
        &#34;&#34;&#34;
        s, n = df[&#34;s&#34;], df[&#34;n&#34;]
        # this is the old call from before factoring out to Assays
        # dCt_col, norm_col = df.columns
        # s, n = df[dCt_col], df[norm_col]
        return s / n

    def _link_assays(self, assays):
        &#34;&#34;&#34;
        Links any provided assays and checks their datatype in the process...
        &#34;&#34;&#34;
        if assays is not None:
            for sample in assays: 
                if aux.same_type(sample, Assay()):
                    self._Assays.append(sample)

                elif aux.same_type(sample, Analyser()):
                    self._Assays.append(sample.get())
                
                else: 
                    aw.SoftWarning(&#34;Normaliser:unknown_data&#34;, s = sample)
                
    def _link_normaliser(self, normalisers):
        &#34;&#34;&#34;
        Checks if normaliser is provided and has proper datatype to be added...
        &#34;&#34;&#34;
        if normalisers is not None:
            for normaliser in normalisers:

                if aux.same_type(normaliser, Assay()):
                    self._Normalisers.append(normaliser)

                elif aux.same_type(normaliser, Analyser()):
                    self._Normalisers.append(normaliser.get())

                else: 
                    aw.SoftWarning(&#34;Normaliser:norm_unknown_data&#34;, s = normaliser)

    def _preprocess_normalisers(self, *args, **kwargs):
        &#34;&#34;&#34;
        Averages the provided normalisers row-wise for all normalisers into a 
        single combined normaliser, that will be stored as a new Assay object.
        &#34;&#34;&#34;

        # initialise new Results to store the dCt values form all normalisers
        combined = Results()
        
        # setup names using the first normaliser
        combined.adopt_names(self._Normalisers[0])
        combined.drop_cols(&#34;dCt&#34;)
        combined.adopt_id(self._Normalisers[0])

        # now add all dCt columns from all normalisers
        for norm in self._Normalisers:
            combined.add_dCt(norm)
        
        # remove the non-dCt columns as they would interfere with
        # pre-processing. But we keep the &#34;group&#34; column because some
        # custom prep_func may want to use the group references.
        # i.e. we just replace any &#34;Ct&#34; columns that may have smuggled in if 
        # a non-default process is used for assembly
        combined.drop_cols( raw_col_names[1] )

        # now generate the combined normaliser
        combined_normaliser = self._average(combined)
        combined_normaliser = combined_normaliser.rename(&#34;dCt&#34;)
        combined.add(combined_normaliser)
        
        # now assemble the normaliser into a qpcr.Assay
        combined = combined.get()
        normaliser = Assay()
        normaliser.adopt(combined)
        normaliser.adopt_id(self._Normalisers[0])


        self._normaliser = normaliser  
        if len(self._Normalisers) &gt; 1:
            self._update_combined_id()
  
        # forward combined_id to self and _Results 
        self.adopt_id(self._normaliser)
        self._Results.adopt_id(self._normaliser)

        return self._normaliser

    def _update_combined_id(self):
        &#34;&#34;&#34;
        Generates a new id based on all normaliser ids,
        joining them as a+b+c,...
        &#34;&#34;&#34;
        ids = [N.id() for N in self._Normalisers]
        ids = &#34;+&#34;.join(ids)
        self._normaliser.id_reset()
        self._normaliser.id(ids)
        

    def _average(self, combined):
        &#34;&#34;&#34;
        Averages row-wise all Normaliser entries and 
        generates a series of their per-row means
        (default preprocess_normalisers function)
        &#34;&#34;&#34;
        tmp_df = combined.get()

        # drop group as it is a numeric column 
        # and would otherwise skew the average
        if &#34;group&#34; in tmp_df.columns:
            tmp_df = tmp_df.drop(columns = [&#34;group&#34;]) 
        tmp_df = tmp_df.mean(axis = 1, numeric_only = True)

        return tmp_df</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>qpcr._auxiliary._ID</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qpcr.Normaliser.clear"><code class="name flex">
<span>def <span class="ident">clear</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Will clear the presently stored results</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear(self):
    &#34;&#34;&#34;
    Will clear the presently stored results
    &#34;&#34;&#34;
    self._Results = Results()</code></pre>
</details>
</dd>
<dt id="qpcr.Normaliser.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, copy=False)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>copy</code></strong> :&ensp;<code>bool</code></dt>
<dd>Will return a deepcopy of the Results object if <code>copy = True</code> (default is <code>copy = False</code>).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Results</code></strong> :&ensp;<code><a title="qpcr.Results" href="#qpcr.Results">Results</a></code></dt>
<dd>A <code><a title="qpcr.Results" href="#qpcr.Results">Results</a></code> object containing the normalised dataframe</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self, copy=False):
    &#34;&#34;&#34;
    Parameters
    ----------
    copy : bool
        Will return a deepcopy of the Results object if `copy = True` (default is `copy = False`).
    
    Returns
    -------
    Results : qpcr.Results
        A `qpcr.Results` object containing the normalised dataframe
    &#34;&#34;&#34;
    if copy: 
        return deepcopy(self._Results)
    return self._Results</code></pre>
</details>
</dd>
<dt id="qpcr.Normaliser.link"><code class="name flex">
<span>def <span class="ident">link</span></span>(<span>self, assays:Â listÂ =Â None, normalisers:Â listÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Links either normalisers or assays-of-interest <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> objects coming from the same <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>assays</code></strong> :&ensp;<code>list</code> or <code>tuple</code> or <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code></dt>
<dd>A list of <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> objects coming from a <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code> or the <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code> itself. These assays will be normalised against a normaliser.</dd>
<dt><strong><code>normalisers</code></strong> :&ensp;<code>list</code> or <code>tuple</code> or <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code></dt>
<dd>A list of <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> objects coming from a <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code> or the <code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code> itself. These assays will be used as normalisers. These will be
combined into one single pseudo-normaliser which will then be used to normalise the assays. The method of
combining the normalisers can be specified using the <code><a title="qpcr.Normaliser.prep_func" href="#qpcr.Normaliser.prep_func">Normaliser.prep_func()</a></code> method.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def link(self, assays:(list or tuple or Analyser) = None, normalisers:(list or tuple or Analyser) = None):
    &#34;&#34;&#34;
    Links either normalisers or assays-of-interest `qpcr.Assay` objects coming from the same `qpcr.Analyser`.

    Parameters
    ----------
    assays : list or tuple or qpcr.Analyser
        A list of `qpcr.Assay` objects coming from a `qpcr.Analyser` or the `qpcr.Analyser` itself. These assays will be normalised against a normaliser.
    
    normalisers : list or tuple or qpcr.Analyser
        A list of `qpcr.Assay` objects coming from a `qpcr.Analyser` or the `qpcr.Analyser` itself. These assays will be used as normalisers. These will be
        combined into one single pseudo-normaliser which will then be used to normalise the assays. The method of 
        combining the normalisers can be specified using the `qpcr.Normaliser.prep_func` method.
    &#34;&#34;&#34;
    # convert to lists (since the _link methods really want lists)
    if assays is not None and not isinstance(assays, (list, tuple)): 
        assays = [assays]
    self._link_assays(assays)

    if normalisers is not None and not isinstance(normalisers, (list, tuple)): 
        normalisers = [normalisers]
    self._link_normaliser(normalisers)</code></pre>
</details>
</dd>
<dt id="qpcr.Normaliser.norm_func"><code class="name flex">
<span>def <span class="ident">norm_func</span></span>(<span>self, f=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets any defined function to perform normalisation of assays against normalisers.
If no <code>f</code> is provided, it returns the current <code>norm_func</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>function</code></dt>
<dd>
<p>The function may accept two <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> objects (named <code>assay</code> and <code>normaliser</code> which will be forwarded from the <code><a title="qpcr.Normaliser" href="#qpcr.Normaliser">Normaliser</a></code>).
The function may also accept one <code>pandas.DataFrame</code> (named <code>df</code>) containing two numeric columns of delta-Ct values from a sample-assay (named "s") and a normaliser-assay (named "n"),
as well as a group identifier column (named "group").
Whatever inputs it works with, it must return a named numeric <code>pandas.Series</code> of the same length as entries in the Assays' dataframes. </p>
<h5 id="note">Note</h5>
<p>Support for the dataframe direct usage will be dropped at some point in the future. </p>
<p>By default <code>s/n</code> is used, where <code>s</code> is a column of sample-assay deltaCt values,
and <code>n</code> is the corresponding <code>"dCt"</code> column from the normaliser.</p>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def norm_func(self, f = None):
    &#34;&#34;&#34;
    Sets any defined function to perform normalisation of assays against normalisers.
    If no `f` is provided, it returns the current `norm_func`.

    Parameters
    ----------
    f : function
        The function may accept two `qpcr.Assay` objects (named `assay` and `normaliser` which will be forwarded from the `qpcr.Normaliser`). 
        The function may also accept one `pandas.DataFrame` (named `df`) containing two numeric columns of delta-Ct values from a sample-assay (named &#34;s&#34;) and a normaliser-assay (named &#34;n&#34;),
        as well as a group identifier column (named &#34;group&#34;). 
        Whatever inputs it works with, it must return a named numeric `pandas.Series` of the same length as entries in the Assays&#39; dataframes. 
        
        ##### Note
        Support for the dataframe direct usage will be dropped at some point in the future. 

        By default `s/n` is used, where `s` is a column of sample-assay deltaCt values, 
        and `n` is the corresponding `&#34;dCt&#34;` column from the normaliser.

    &#34;&#34;&#34;
    if aux.same_type(f, aux.fileID):
        self._norm_func = f
        self._norm_func_is_set = True
    elif f is None:
        return f
    else: 
        aw.HardWarning(&#34;Normaliser:cannot_set_norm_func&#34;, func = f)</code></pre>
</details>
</dd>
<dt id="qpcr.Normaliser.normalise"><code class="name flex">
<span>def <span class="ident">normalise</span></span>(<span>self, mode='pair-wise', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Normalises all linked assays against the combined pseudo-normaliser
(by default, unless a custom <code>prep_func</code> has been specified),
and stores the results in a new Results object.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>The normalisation mode to use. This can be either <code>pair-wise</code> (default),
or <code>combinatoric</code>, or <code>permutative</code>.
<code>pair-wise</code> will normalise replicates only by their partner (i.e. first against first,
second by second, etc.). <code>combinatoric</code> will normalise all possible combinations of a replicate
with all partner replicates of the same group from a normaliser (i.e. first against first, then second, then third, etc.).
This will generate <code>n^2</code> normalised Delta-Delta-Ct values, where <code>n</code> is the number of replicates in a group.
<code>permutative</code> will scramble the normaliser replicates randomly and then normalise pair-wise. This mode supports
a parameter <code>k</code> which specifies the times this process should be repeated, thus generating <code>n * k</code> normalised Delta-Delta-Ct values.
Also, through setting <code>replace = True</code> replacement may be allowed during normaliser scrambling.
Note, this setting will be ignored if a custom <code>norm_func</code> is provided.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Any additional keyword arguments that may be passed to a custom
<code>norm_func</code> and <code>prep_func</code> (both will receive the kwargs!).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalise(self, mode = &#34;pair-wise&#34;, **kwargs):
    &#34;&#34;&#34;
    Normalises all linked assays against the combined pseudo-normaliser 
    (by default, unless a custom `prep_func` has been specified), 
    and stores the results in a new Results object.

    Parameters
    ----------
    mode : str
        The normalisation mode to use. This can be either `pair-wise` (default), 
        or `combinatoric`, or `permutative`.
        `pair-wise` will normalise replicates only by their partner (i.e. first against first, 
        second by second, etc.). `combinatoric` will normalise all possible combinations of a replicate 
        with all partner replicates of the same group from a normaliser (i.e. first against first, then second, then third, etc.).
        This will generate `n^2` normalised Delta-Delta-Ct values, where `n` is the number of replicates in a group.
        `permutative` will scramble the normaliser replicates randomly and then normalise pair-wise. This mode supports 
        a parameter `k` which specifies the times this process should be repeated, thus generating `n * k` normalised Delta-Delta-Ct values.
        Also, through setting `replace = True` replacement may be allowed during normaliser scrambling.
        Note, this setting will be ignored if a custom `norm_func` is provided.

    **kwargs
        Any additional keyword arguments that may be passed to a custom 
        `norm_func` and `prep_func` (both will receive the kwargs!).
    &#34;&#34;&#34;
    if self._normaliser is None: 
        self._normaliser = self._prep_func(self._Normalisers, **kwargs)
        self._vet_normaliser()

    if self._Assays == [] or self._normaliser is None:
        aw.SoftWarning(&#34;Normaliser:no_data_yet&#34;)
        
    # check which kind of norm_func we should use
    if not self._norm_func_is_set:
        # pair-wise is already default so we don&#39;t check...
        if mode == &#34;combinatoric&#34;:
            self._norm_func = self._tile_normalise
            tiled = deepcopy(self._Assays[0])
            tiled.tile()
            self._Results.adopt_names( tiled )
            del tiled 
        elif mode == &#34;permutative&#34;:
            self._norm_func = self._permutate_normalise
            n = aux.from_kwargs(&#34;k&#34;, 1, kwargs)
            tiled = deepcopy(self._Assays[0])
            tiled.stack(n)
            self._Results.adopt_names( tiled )
            del tiled 
        elif mode == &#34;pair-wise&#34;:
            self._Results.adopt_names( self._Assays[0] )
    else:
        self._Results.adopt_names( self._Assays[0] )

    self._Results.drop_cols()

    # perform normalisation for each assay 
    for assay in self._Assays:

        # get data
        # assay_df = assay.get()

        # apply normalisation (delta-delta-Ct)
        normalised = self._norm_func_wrapper(
                                                assay = assay, 
                                                normaliser = self._normaliser, 
                                                **kwargs
                                        )

        # # store results in _Results
        # self._store_to_Results(assay, normalised)

        # and store results also in the Assay itself
        assay.add_ddCt( self._normaliser.id(), normalised )

        # and store to results
        self._Results.add_ddCt(assay)</code></pre>
</details>
</dd>
<dt id="qpcr.Normaliser.prep_func"><code class="name flex">
<span>def <span class="ident">prep_func</span></span>(<span>self, f=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets any defined function for combined normaliser pre-processing.
If no <code>f</code> is provided, it returns the current <code>prep_func</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>function</code></dt>
<dd>The function may accept one list of <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> objects, and must return
either an <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object directly or a <code>pandas.Dataframe</code> (that will be migrated to an <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code>).
The returned dataframe must contain a <code>"dCt"</code> column which stores the delta-Ct values ultimately used as
"normaliser assay".</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prep_func(self, f = None):
    &#34;&#34;&#34;
    Sets any defined function for combined normaliser pre-processing.
    If no `f` is provided, it returns the current `prep_func`.

    Parameters
    ----------
    f : function
        The function may accept one list of `qpcr.Assay` objects, and must return 
        either an `qpcr.Assay` object directly or a `pandas.Dataframe` (that will be migrated to an `qpcr.Assay`).
        The returned dataframe must contain a `&#34;dCt&#34;` column which stores the delta-Ct values ultimately used as 
        &#34;normaliser assay&#34;.  
    &#34;&#34;&#34;
    
    if aux.same_type(f, aux.fileID):
        self._prep_func = f
    elif f is None:
        return f
    else: 
        aw.HardWarning(&#34;Normaliser:cannot_set_prep_func&#34;, func = f)</code></pre>
</details>
</dd>
<dt id="qpcr.Normaliser.prune"><code class="name flex">
<span>def <span class="ident">prune</span></span>(<span>self, assays=True, normalisers=True, results=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Will clear assays, normalisers, and/or results
assays : bool
Will clear any sample assays if True (default).</p>
<p>results : bool
Will clear any computed results if True (default).</p>
<p>normalisers : bool
Will clear any normalisers if True (default).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prune(self, assays = True, normalisers = True, results = True):
    &#34;&#34;&#34;
    Will clear assays, normalisers, and/or results
    assays : bool
        Will clear any sample assays if True (default).
    
    results : bool
        Will clear any computed results if True (default).
    
    normalisers : bool
        Will clear any normalisers if True (default).
    &#34;&#34;&#34;
    if assays:
        self._Assays = []
    if normalisers: 
        self._Normalisers = []
    if results: 
        self.clear()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="qpcr.Reader"><code class="flex name class">
<span>class <span class="ident">Reader</span></span>
<span>(</span><span>filename:Â str, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads qpcr raw data files in csv or excel format to get a single dataset. </p>
<h2 id="note">Note</h2>
<p>This implementation of the default Reader is now deprecated and will be removed
at some point in the future. The new impelentation is the <code>qpcr.Readers.SingleReader</code>.
Please, use that one instead.</p>
<h2 id="input-data-files">Input Data Files</h2>
<p>Valid input files are either regular <code>csv</code> files, or
irregular <code>csv</code> or <code>excel</code> files,
that specify assays by one replicate identifier column and one Ct value column.</p>
<p>Irregular input files may specify multiple assays as separate tables,
one assay has to be selected using the <code>assay</code> argument.
Separate assay tables may be either below one another (separated by blank lines!)
or besides one another (requires <code>transpose = True</code>).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>A filepath to a raw data file.
If the file is a <code>csv</code> file, it has to have two named columns; one for replicate names, one for Ct values.
Both csv (<code>,</code> spearated) and csv2 (<code>;</code> separated) are accepted.
If the file is an <code>excel</code> file it the relevant sections of the spreadsheet are identified automatically.
But they require identifying headers. By default <code>Name</code> and <code>Ct</code> are assumed but these can be changed using
the <code>name_label</code> and <code>Ct_label</code> arguments that can be passed as kwargs (they will be forwarded to the <code>.read()</code> method).</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Any additional keyword arguments that shall be passed to the <code>read()</code> method which is immediately called during init.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Reader(_CORE_Reader):
    &#34;&#34;&#34;
    Reads qpcr raw data files in csv or excel format to get a single dataset. 

    Note
    -----
    This implementation of the default Reader is now deprecated and will be removed
    at some point in the future. The new impelentation is the `qpcr.Readers.SingleReader`.
    Please, use that one instead.

    Input Data Files
    ----------------
    Valid input files are either regular `csv` files, or  irregular `csv` or `excel` files, 
    that specify assays by one replicate identifier column and one Ct value column.

    Irregular input files may specify multiple assays as separate tables, 
    one assay has to be selected using the `assay` argument. 
    Separate assay tables may be either below one another (separated by blank lines!)
    or besides one another (requires `transpose = True`).

    Parameters
    ----------
    filename : str
        A filepath to a raw data file.
        If the file is a `csv` file, it has to have two named columns; one for replicate names, one for Ct values. 
        Both csv (`,` spearated) and csv2 (`;` separated) are accepted.
        If the file is an `excel` file it the relevant sections of the spreadsheet are identified automatically. 
        But they require identifying headers. By default `Name` and `Ct` are assumed but these can be changed using 
        the `name_label` and `Ct_label` arguments that can be passed as kwargs (they will be forwarded to the `.read()` method). 

    **kwargs
        Any additional keyword arguments that shall be passed to the `read()` method which is immediately called during init.
    &#34;&#34;&#34;
    def __init__(self, filename:str, **kwargs) -&gt; pd.DataFrame: 
        super().__init__()
        self._src = filename
        if self._filesuffix() == &#34;csv&#34;:
            self._delimiter = &#34;;&#34; if self._is_csv2() else &#34;,&#34;
        self.read(**kwargs)

    def _is_csv2(self):
        &#34;&#34;&#34;
        Tests if csv file is ; delimited (True) or common , (False)
        &#34;&#34;&#34;
        with open(self._src, &#34;r&#34;) as openfile: 
            content = openfile.read()
        if &#34;;&#34; in content: 
            return True
        return False

    def _has_header(self):
        &#34;&#34;&#34;
        Checks if column headers are provided in the data file
        It does so by checking if the second element in the first row is numeric
        if it is numeric (returns None &lt;&lt; False) no headers are presumed. Otherwise
        it returns 0 (as in first row has headers)...
        &#34;&#34;&#34;
        with open(self._src, &#34;r&#34;) as openfile: 
            content = openfile.read().split(&#34;\n&#34;)[0]
            content = content.split(self._delimiter)
        try: 
            second_col = content[1]
            second_col = float(second_col)
        except ValueError:
            return 0 # Headers in row 0
        return None  # no headers</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="qpcr._CORE_Reader" href="#qpcr._CORE_Reader">_CORE_Reader</a></li>
<li>qpcr._auxiliary._ID</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="qpcr._CORE_Reader" href="#qpcr._CORE_Reader">_CORE_Reader</a></b></code>:
<ul class="hlist">
<li><code><a title="qpcr._CORE_Reader.get" href="#qpcr._CORE_Reader.get">get</a></code></li>
<li><code><a title="qpcr._CORE_Reader.n" href="#qpcr._CORE_Reader.n">n</a></code></li>
<li><code><a title="qpcr._CORE_Reader.read" href="#qpcr._CORE_Reader.read">read</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="qpcr.Results"><code class="flex name class">
<span>class <span class="ident">Results</span></span>
</code></dt>
<dd>
<div class="desc"><p>Handles a pandas dataframe for data and computed results from a <code><a title="qpcr" href="#qpcr">qpcr</a></code> class. </p>
<h2 id="note">Note</h2>
<p>This is a central data collection that can inherit directly from <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> objects and from
extrenally computed sources. Please, note that it will not perform extensive vetting on its data input,
so make sure to only provide proper data input when manually assembling your <code><a title="qpcr.Results" href="#qpcr.Results">Results</a></code>!</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Results(aux._ID):
    &#34;&#34;&#34;
    Handles a pandas dataframe for data and computed results from a `qpcr` class. 
    
    Note
    -----
    This is a central data collection that can inherit directly from `qpcr.Assay` objects and from 
    extrenally computed sources. Please, note that it will not perform extensive vetting on its data input, 
    so make sure to only provide proper data input when manually assembling your `qpcr.Results`!
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._df = None
        self._Assay = None
        self._stats_results = {
                                &#34;group&#34; : [], 
                                defaults.default_dataset_header : [], 
                                &#34;mean&#34; : [], &#34;stdev&#34; : [], &#34;median&#34; : []
                            }
        self._stats_df = None

    def adopt_names(self, Assay:Assay):
        &#34;&#34;&#34;
        Links an instance of Assay to be used as reference for group_names
        It copies the `group` and `group_name` columns to the results storing dataframe.
        This step can only be performed once!

        Parameters
        ----------
        Assay : qpcr.Assay
            A `qpcr.Assay` object whose group_name column will be copied.
        &#34;&#34;&#34;
        if self.is_empty():
            self._df = Assay.get()
            self._drop_setup_cols()
        else:
            named_identically = all( self._df[&#34;group_name&#34;] == Assay.get()[&#34;group_name&#34;] )
            if not named_identically:
                aw.SoftWarning(&#34;Results:cannot_link&#34;)


    def add_Ct(self, assay : Assay):
        &#34;&#34;&#34;
        Adds a `&#34;Ct&#34;` column with Delta-Ct values from an `qpcr.Assay`.
        It will store these as a new column using the Assay&#39;s `id` as header.

        Parameters
        -------
        assay : qpcr.Assay
            An `qpcr.Assay` object from which to import.
        &#34;&#34;&#34;
        id = assay.id()
        Ct = assay.get()[ raw_col_names[1] ]
        Ct.name = id
        self.add(Ct)

    def add_dCt(self, assay : Assay):
        &#34;&#34;&#34;
        Adds a `&#34;dCt&#34;` column with Delta-Ct values from an `qpcr.Assay`.
        It will store these as a new column using the Assay&#39;s `id` as header.

        Parameters
        -------
        assay : qpcr.Assay
            An `qpcr.Assay` object from which to import.
        &#34;&#34;&#34;
        dCt = assay.dCt()
        self.add(dCt)

    def add_ddCt(self, assay : Assay):
        &#34;&#34;&#34;
        Adds all `&#34;rel_{}&#34;` columns with Delta-Delta-Ct values from an `qpcr.Assay`.
        It will store these as new columns using the Assay&#39;s `id` + the `_rel_{}` composite id.

        Parameters
        -------
        assay : qpcr.Assay
            An `qpcr.Assay` object from which to import.
        &#34;&#34;&#34;
        df = assay.ddCt()
        # add data
        self.add(df)

    # FUTURE FEATURE HERE
    # this feature will be for fold-change columns of second normalisation
    # currently this is not yet supported but is supposed to be an implemented feature of the 
    # Normaliser. The Normaliser is currently perfectly able to perform second normalisation but
    # does not yet have an integrated method of properly storing the results and there is also not
    # # yet a method for pairing assays together for second normalisation.
    # def add_fc(self, assay : Assay):
    #     &#34;&#34;&#34;
    #     Adds all `&#34;fc_{}&#34;` columns with Delta-Delta-Ct values from an `qpcr.Assay`.
    #     It will store these as new columns using the Assay&#39;s `id` + the `_fc_{}` composite id.

    #     Parameters
    #     -------
    #     assay : qpcr.Assay
    #         An `qpcr.Assay` object from which to import.
    #     &#34;&#34;&#34;
    #     id = assay.id()
    #     df = assay.get()
    #     # get the ddCt containing columns
    #     rel_cols = [ i for i in df.columns if &#34;fc_&#34; in i ]
    #     # generate new composite ids 
    #     new_names = [ f&#34;{id}_{i}&#34; for i in rel_cols ]
    #     # get ddCt columns 
    #     df = df[rel_cols]
    #     # rename the columns to include the assay id
    #     df = df.rename( columns = { new : old for new, old in zip(new_names, rel_cols) } )

    #     # add data
    #     self.add(df)

    def names(self, as_set = False):
        &#34;&#34;&#34;
        Returns 
        -------
        names : list or None
            The adopted `group_names` 
            (only works if a `qpcr.Assay` has already been linked 
            using `adopt_names()`!)
        &#34;&#34;&#34;
        if self._df is not None:
            names = self._df[&#34;group_name&#34;]
            if as_set: names = aux.sorted_set(names)
            return names
        return None

    def get(self):
        &#34;&#34;&#34;
        Returns 
        -------
        data : pd.DataFrame
            The results dataframe
        &#34;&#34;&#34;
        return self._df

    def is_empty(self):
        &#34;&#34;&#34;
        Checks if any results have been stored so far.

        Returns
        -------
        bool
            `True` if NO data is yet stored, else `False`.
        &#34;&#34;&#34;
        return self._df is None

    def drop_groups(self, groups : (list or str)):
        &#34;&#34;&#34;
        Removes specific groups of replicates from the DataFrame.

        Parameters
        ----------
        groups : list
            Either the numeric group identifiers or the group names
            of the groups to be removed, or a `regex` pattern defining which groups
            should be dropped (this is useful for systematically removing RT- groups etc.)
        &#34;&#34;&#34;
        # check for regex pattern
        # and get corresponding group names 
        if isinstance(groups, str):
            groups = [i for i in self._df[&#34;group_name&#34;] if re.match(groups, i) is not None]
        
        # get the right reference column and query to use to be 
        # used (either group or group_name)
        ref_query = &#34;group != {group}&#34; if isinstance( groups[0], int ) else &#34;group_name != &#39;{group}&#39;&#34;
        
        # remove groups from dataset
        for group in groups: 
            self._df = self._df.query(ref_query.format(group = group))

            # also drop from stats df
            if self._stats_df is not None:
                self._stats_df = self._stats_df.query(ref_query.format(group = group))
    

    def add(self, column:pd.Series, replace : bool = False):
        &#34;&#34;&#34;
        Adds some new column of data.

        Note
        ----
        The `column` argument has to be named for this to work. However, there are 
        already implemented methods dedicated to adding specifically Delta-Ct, Delta-Delta-Ct or just
        Ct values to the Results.

        Parameters
        ----------
        column : pd.Series
            A named pandas Series or DataFrame that can be joined into the already
            stored dataframe.
        replace : bool
            In case results from a computation with the same identifiers are already stored
            no new data can be stored under that id. Either the new data must be renamed or
            `replace = True` must be set to overwrite the presently stored data. 
        &#34;&#34;&#34;
        if isinstance(column, pd.Series):
            if column.name in self._df.columns:
                if not replace:  
                    aw.SoftWarning(&#34;Results:name_overlap&#34;, name = column.name)
                else: 
                    self._df[column.name] = column
            else: 
                self._df = self._df.join(column)
        else: 
            for i in column.columns:
                if i in self._df.columns: 
                    if not replace: 
                        aw.SoftWarning(&#34;Results:name_overlap&#34;, name = i )
                col = column[i]
                self._df[i] = col

    def merge(self, *Results):
        &#34;&#34;&#34;
        Merges any number of other qpcr.Results objects into this one.
        The source id of the results is added as column-name suffix. 

        Parameters
        ----------
        *Results
            An arbitrary number of qpcr.Results objects.

        &#34;&#34;&#34;
        new_df = self._df
        for R in Results: 
            R_df = R.get()

            # get only the delta-delta-Ct columns
            cols = [i for i in R_df.columns if i not in ref_cols]
            R_df = R_df[cols]


            # we merge the dataframes first without adding 
            # some new id suffix, only do so if this fails
            try: 
                new_df = pd.merge(new_df, R_df, 
                                    right_index = True, left_index = True, 
                                )
            except: 
                new_df = pd.merge(new_df, R_df, 
                                right_index = True, left_index = True, 
                                suffixes = [f&#34;_{self.id()}&#34;, f&#34;_{R.id()}&#34;]
                            )
        self._df = new_df

    def drop_cols(self, *cols):
        &#34;&#34;&#34;
        Drops all specified columns from the dataframes
        this is used for normaliser pre-processing.

        Parameters
        ----------
        *cols
            Any column names (as `str`) to be dropped.
            If no names are specified any/all `deltaCt` data-containing columns are dropped!
            If this is the case then the only columns retained are: `&#34;group&#34;, &#34;group_name&#34;, &#34;id&#34;, &#34;assay&#34;`.
        &#34;&#34;&#34;
        if cols == ():
            _to_drop = [c for c in self._df.columns if c not in [ &#34;group&#34;, &#34;group_name&#34;, raw_col_names[0], defaults.default_dataset_header ]]
        else:
            _to_drop = [c for c in cols if c in list(self._df.columns)]
        self._df = self._df.drop(columns = _to_drop)
        
    def rename_cols(self, cols:dict):
        &#34;&#34;&#34;
        Renames columns according to a dictionary as key -&gt; value.

        Parameters
        ----------
        cols : dict
            A dictionary specifying old column names (keys) and new colums names (values).
        &#34;&#34;&#34;
        self._df = self._df.rename(columns = cols)


    def stats(self, recompute = False) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        Computes summary statistis about the replicate groups: 
        `Mean`, `Median`, and `StDev` of all replicate groups, for all datasets (assays).
        
        Parameters
        ----------
        recompute : bool
            Statistics will only be once unless recompute is set to `True`.

        Returns
        -------
        stats_df : pd.DataFrame
            A new dataframe containing the computed statistics for each replicate group.

        &#34;&#34;&#34;
        default_dataset_header = defaults.default_dataset_header
        # if stats_df is already present, return but sorted according to assays, not groups (nicer for user to inspect)
        if self._stats_df is not None and not recompute:
            return self._stats_df.sort_values(default_dataset_header)
        elif recompute: 
            self._stats_results = {&#34;group&#34; : [], default_dataset_header : [], &#34;mean&#34; : [], &#34;stdev&#34; : [], &#34;median&#34; : []}
            self._stats_df = None

        # get groups and corresponding assay columns 
        groups = aux.sorted_set(list(self._df[&#34;group&#34;]))
        assays = [c for c in self._df.columns if c not in ref_cols]
     
        # compute stats for all replicates per group
        for group in groups:
            group_subset = self._df.query(f&#34;group == {group}&#34;)
            
            median = self._stat_var(group_subset, np.nanmedian)
            mean = self._stat_var(group_subset, np.nanmean)
            stdv = self._stat_var(group_subset, np.nanstd)
            self._add_stats(assays, group, median, mean, stdv)
            
        # add group names
        self._add_stats_names(assays)

        self._stats_df = pd.DataFrame(self._stats_results)
        return self._stats_df.sort_values(default_dataset_header)

    def save(self, path, df = True, stats = True):
        &#34;&#34;&#34;
        Saves a csv file for each specified type of results.

        Parameters
        ----------
        path : str
            Path has to be a filepath if only one type of results shall be saved (i.e. either `df` or `stats`), 
            otherwise a path to the directory where both `df` and `stats` shall be saved.
        
        df : bool
            Save the results dataframe containing all replicate values (the full results).
            Default is `df = True`.
        
        stats : bool
            Save the results dataframe containing summary statistics for all replicate groups.
            Default is `stats = True`.
        
        &#34;&#34;&#34;
        if df and stats and not os.path.isdir(path):
            aw.HardWarning(&#34;Results:save_need_dir&#34;)

        if df:
            # in case of raw results export we don&#39;t need the &#34;assay&#34; column as all 
            # assays are stored as separate columns anyaway, so it doesn&#39;t store any useful data
            _df = self._df
            if &#34;assay&#34; in _df.columns: _df = self._df.drop( columns = [&#34;assay&#34;] )
            self._save_single(path, _df, &#34;_df&#34;)
        if stats:
            # compute stats if none have been computed yet...
            if self._stats_df is None:
                self.stats()
            self._save_single(path, self._stats_df, &#34;_stats&#34;)

    def drop_rel(self):
        &#34;&#34;&#34;
        Crops the `X_rel_Y` column-names of Delta-Delta-Ct results to just `X`.
        I.e. reduces back to the assay-of-interest name only.
        &#34;&#34;&#34;
        colnames = self._df.columns
        to_change = {i : i.split(&#34;_rel_&#34;)[0] for i in colnames if &#34;_rel_&#34; in i }
        self.rename_cols(to_change)

        # also recompute the stats df with new names...
        if self._stats_df is not None: 
            self.stats(recompute = True)

    def split(self, reset_names = False, drop_rel = True):
        &#34;&#34;&#34;
        Splits the stored results dataframe into separate qpcr.Results objects containing only a signle deltaCt column each.

        Parameters
        ----------
        reset_names : bool
            Resets the deltaCt column-name from `&#34;X_rel_Y&#34;` to just `&#34;dCt&#34;`.

        drop_rel : bool
            Crops `&#34;X_rel_Y&#34;` deltaCt column-names to just `&#34;X&#34;`. 

        Returns 
        -------
        objects : list
            A list of qpcr.Results objects containing only a single dCt column each (retaining group columns etc.)
        &#34;&#34;&#34;
        shared_columns = [i for i in self._df.columns if i in ref_cols]
        dct_columns = [i for i in self._df.columns if i not in ref_cols]
        
        dfs = [self._df[shared_columns + [i]] for i in dct_columns]
        objects = [Results() for i in dfs]

        for o, df, dct_col in zip(objects, dfs, dct_columns): 
            o._df = df
            if reset_names:
                o.rename_cols({dct_col : &#34;dCt&#34;})
            if drop_rel: 
                o.drop_rel()

            o.id(dct_col)
        
        return objects


    def preview( self, kind : str = &#34;AssayBars&#34;, mode : str = &#34;static&#34;, **kwargs ):
        &#34;&#34;&#34;
        A shortcut to call on a `qpcr.Plotters.PreviewResults` wrapper to visualise 
        the results.

        Parameters
        ----------
        kind : str
            The kind of Plotter to call. This can be any of the four wrapped 
            Plotters, e.g. `kind = &#34;GroupBars&#34;`.
        mode : str
            The plotting mode. May be either &#34;static&#34; (matplotlib) or &#34;interactive&#34; (plotly).

        Returns
        -------
        fig : plt.figure or plotly.figure
            The figure generated by `PreviewResults`.
        &#34;&#34;&#34;
        preview_results = Plotters.PreviewResults( mode = mode, kind = kind )
        preview_results.params( **kwargs )
        preview_results.link( self )
        fig = preview_results.plot()
        return fig 

    def _save_single(self, path, src, suffix=&#34;&#34;):
        &#34;&#34;&#34;
        Saves either self._df or self._stats_df to a csv file based on a path
        (path can be either filename or directory)
        &#34;&#34;&#34;
        filename = path if not os.path.isdir(path) else os.path.join(path, f&#34;rel_{self.id()}{suffix}.csv&#34;)
        src.to_csv(filename, index = False)
        
    def _drop_setup_cols(self):
        &#34;&#34;&#34;
        Removes unnnecessary columns from the df during self._df setup with link()
        &#34;&#34;&#34;
        # drop the Ct columns
        relcols = [i for i in self._df.columns if &#34;rel_&#34; in i]
        self.drop_cols(
                        raw_col_names[1], &#34;dCt&#34;, *relcols
                    )



    def _add_stats_names(self, samples):
        &#34;&#34;&#34;
        Adds a group_name column to self._stats_result with appropriate
        repetition of group_names for each group of replicates...
        &#34;&#34;&#34;
        self._stats_results[&#34;group_name&#34;] = []
        group_names = aux.sorted_set(list(self._df[&#34;group_name&#34;]))
        for group_name in group_names:
            self._stats_results[&#34;group_name&#34;].extend([group_name] * len(samples))

    def _add_stats(self, samples, group, median, mean, stdv):
        &#34;&#34;&#34;
        Adds new summary entries to self._stats_results
        &#34;&#34;&#34;
        self._stats_results[&#34;group&#34;].extend([group] * len(samples))
        self._stats_results[&#34;assay&#34;].extend(samples)
        self._stats_results[&#34;median&#34;].extend(median)
        self._stats_results[&#34;mean&#34;].extend(mean)
        self._stats_results[&#34;stdev&#34;].extend(stdv)


    def _stat_var(self, group_subset, func, **kwargs):
        &#34;&#34;&#34;
        Performs a function (like mean or stdv) over all rows
        and returns the result as list with a float for each column in the df
        any function can be passed as long as it works with an iterable
        &#34;&#34;&#34;
        # ignore group and group_name columns
        ignore = [raw_col_names[0], &#34;group&#34;, &#34;group_name&#34;, &#34;assay&#34;]
        all_cols = [g for g in group_subset.columns if g not in ignore]
        tmp = group_subset[all_cols]
        # compute stats based on func
        stats = []
        for col in tmp.columns:
            try: 
                stat = func(tmp[col], **kwargs)
            except: 
                stat = np.nan
            stats.append(stat)
        return stats</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>qpcr._auxiliary._ID</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qpcr.Results.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self, column:Â pandas.core.series.Series, replace:Â boolÂ =Â False)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds some new column of data.</p>
<h2 id="note">Note</h2>
<p>The <code>column</code> argument has to be named for this to work. However, there are
already implemented methods dedicated to adding specifically Delta-Ct, Delta-Delta-Ct or just
Ct values to the Results.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>column</code></strong> :&ensp;<code>pd.Series</code></dt>
<dd>A named pandas Series or DataFrame that can be joined into the already
stored dataframe.</dd>
<dt><strong><code>replace</code></strong> :&ensp;<code>bool</code></dt>
<dd>In case results from a computation with the same identifiers are already stored
no new data can be stored under that id. Either the new data must be renamed or
<code>replace = True</code> must be set to overwrite the presently stored data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add(self, column:pd.Series, replace : bool = False):
    &#34;&#34;&#34;
    Adds some new column of data.

    Note
    ----
    The `column` argument has to be named for this to work. However, there are 
    already implemented methods dedicated to adding specifically Delta-Ct, Delta-Delta-Ct or just
    Ct values to the Results.

    Parameters
    ----------
    column : pd.Series
        A named pandas Series or DataFrame that can be joined into the already
        stored dataframe.
    replace : bool
        In case results from a computation with the same identifiers are already stored
        no new data can be stored under that id. Either the new data must be renamed or
        `replace = True` must be set to overwrite the presently stored data. 
    &#34;&#34;&#34;
    if isinstance(column, pd.Series):
        if column.name in self._df.columns:
            if not replace:  
                aw.SoftWarning(&#34;Results:name_overlap&#34;, name = column.name)
            else: 
                self._df[column.name] = column
        else: 
            self._df = self._df.join(column)
    else: 
        for i in column.columns:
            if i in self._df.columns: 
                if not replace: 
                    aw.SoftWarning(&#34;Results:name_overlap&#34;, name = i )
            col = column[i]
            self._df[i] = col</code></pre>
</details>
</dd>
<dt id="qpcr.Results.add_Ct"><code class="name flex">
<span>def <span class="ident">add_Ct</span></span>(<span>self, assay:Â <a title="qpcr.Assay" href="#qpcr.Assay">Assay</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds a <code>"Ct"</code> column with Delta-Ct values from an <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code>.
It will store these as a new column using the Assay's <code>id</code> as header.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>assay</code></strong> :&ensp;<code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code></dt>
<dd>An <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object from which to import.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_Ct(self, assay : Assay):
    &#34;&#34;&#34;
    Adds a `&#34;Ct&#34;` column with Delta-Ct values from an `qpcr.Assay`.
    It will store these as a new column using the Assay&#39;s `id` as header.

    Parameters
    -------
    assay : qpcr.Assay
        An `qpcr.Assay` object from which to import.
    &#34;&#34;&#34;
    id = assay.id()
    Ct = assay.get()[ raw_col_names[1] ]
    Ct.name = id
    self.add(Ct)</code></pre>
</details>
</dd>
<dt id="qpcr.Results.add_dCt"><code class="name flex">
<span>def <span class="ident">add_dCt</span></span>(<span>self, assay:Â <a title="qpcr.Assay" href="#qpcr.Assay">Assay</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds a <code>"dCt"</code> column with Delta-Ct values from an <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code>.
It will store these as a new column using the Assay's <code>id</code> as header.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>assay</code></strong> :&ensp;<code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code></dt>
<dd>An <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object from which to import.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_dCt(self, assay : Assay):
    &#34;&#34;&#34;
    Adds a `&#34;dCt&#34;` column with Delta-Ct values from an `qpcr.Assay`.
    It will store these as a new column using the Assay&#39;s `id` as header.

    Parameters
    -------
    assay : qpcr.Assay
        An `qpcr.Assay` object from which to import.
    &#34;&#34;&#34;
    dCt = assay.dCt()
    self.add(dCt)</code></pre>
</details>
</dd>
<dt id="qpcr.Results.add_ddCt"><code class="name flex">
<span>def <span class="ident">add_ddCt</span></span>(<span>self, assay:Â <a title="qpcr.Assay" href="#qpcr.Assay">Assay</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds all <code>"rel_{}"</code> columns with Delta-Delta-Ct values from an <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code>.
It will store these as new columns using the Assay's <code>id</code> + the <code>_rel_{}</code> composite id.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>assay</code></strong> :&ensp;<code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code></dt>
<dd>An <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object from which to import.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_ddCt(self, assay : Assay):
    &#34;&#34;&#34;
    Adds all `&#34;rel_{}&#34;` columns with Delta-Delta-Ct values from an `qpcr.Assay`.
    It will store these as new columns using the Assay&#39;s `id` + the `_rel_{}` composite id.

    Parameters
    -------
    assay : qpcr.Assay
        An `qpcr.Assay` object from which to import.
    &#34;&#34;&#34;
    df = assay.ddCt()
    # add data
    self.add(df)</code></pre>
</details>
</dd>
<dt id="qpcr.Results.adopt_names"><code class="name flex">
<span>def <span class="ident">adopt_names</span></span>(<span>self, Assay:Â <a title="qpcr.Assay" href="#qpcr.Assay">Assay</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Links an instance of Assay to be used as reference for group_names
It copies the <code>group</code> and <code>group_name</code> columns to the results storing dataframe.
This step can only be performed once!</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>Assay</code></strong> :&ensp;<code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code></dt>
<dd>A <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object whose group_name column will be copied.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adopt_names(self, Assay:Assay):
    &#34;&#34;&#34;
    Links an instance of Assay to be used as reference for group_names
    It copies the `group` and `group_name` columns to the results storing dataframe.
    This step can only be performed once!

    Parameters
    ----------
    Assay : qpcr.Assay
        A `qpcr.Assay` object whose group_name column will be copied.
    &#34;&#34;&#34;
    if self.is_empty():
        self._df = Assay.get()
        self._drop_setup_cols()
    else:
        named_identically = all( self._df[&#34;group_name&#34;] == Assay.get()[&#34;group_name&#34;] )
        if not named_identically:
            aw.SoftWarning(&#34;Results:cannot_link&#34;)</code></pre>
</details>
</dd>
<dt id="qpcr.Results.drop_cols"><code class="name flex">
<span>def <span class="ident">drop_cols</span></span>(<span>self, *cols)</span>
</code></dt>
<dd>
<div class="desc"><p>Drops all specified columns from the dataframes
this is used for normaliser pre-processing.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>*cols</code></strong></dt>
<dd>Any column names (as <code>str</code>) to be dropped.
If no names are specified any/all <code>deltaCt</code> data-containing columns are dropped!
If this is the case then the only columns retained are: <code>"group", "group_name", "id", "assay"</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_cols(self, *cols):
    &#34;&#34;&#34;
    Drops all specified columns from the dataframes
    this is used for normaliser pre-processing.

    Parameters
    ----------
    *cols
        Any column names (as `str`) to be dropped.
        If no names are specified any/all `deltaCt` data-containing columns are dropped!
        If this is the case then the only columns retained are: `&#34;group&#34;, &#34;group_name&#34;, &#34;id&#34;, &#34;assay&#34;`.
    &#34;&#34;&#34;
    if cols == ():
        _to_drop = [c for c in self._df.columns if c not in [ &#34;group&#34;, &#34;group_name&#34;, raw_col_names[0], defaults.default_dataset_header ]]
    else:
        _to_drop = [c for c in cols if c in list(self._df.columns)]
    self._df = self._df.drop(columns = _to_drop)</code></pre>
</details>
</dd>
<dt id="qpcr.Results.drop_groups"><code class="name flex">
<span>def <span class="ident">drop_groups</span></span>(<span>self, groups:Â list)</span>
</code></dt>
<dd>
<div class="desc"><p>Removes specific groups of replicates from the DataFrame.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>groups</code></strong> :&ensp;<code>list</code></dt>
<dd>Either the numeric group identifiers or the group names
of the groups to be removed, or a <code>regex</code> pattern defining which groups
should be dropped (this is useful for systematically removing RT- groups etc.)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_groups(self, groups : (list or str)):
    &#34;&#34;&#34;
    Removes specific groups of replicates from the DataFrame.

    Parameters
    ----------
    groups : list
        Either the numeric group identifiers or the group names
        of the groups to be removed, or a `regex` pattern defining which groups
        should be dropped (this is useful for systematically removing RT- groups etc.)
    &#34;&#34;&#34;
    # check for regex pattern
    # and get corresponding group names 
    if isinstance(groups, str):
        groups = [i for i in self._df[&#34;group_name&#34;] if re.match(groups, i) is not None]
    
    # get the right reference column and query to use to be 
    # used (either group or group_name)
    ref_query = &#34;group != {group}&#34; if isinstance( groups[0], int ) else &#34;group_name != &#39;{group}&#39;&#34;
    
    # remove groups from dataset
    for group in groups: 
        self._df = self._df.query(ref_query.format(group = group))

        # also drop from stats df
        if self._stats_df is not None:
            self._stats_df = self._stats_df.query(ref_query.format(group = group))</code></pre>
</details>
</dd>
<dt id="qpcr.Results.drop_rel"><code class="name flex">
<span>def <span class="ident">drop_rel</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Crops the <code>X_rel_Y</code> column-names of Delta-Delta-Ct results to just <code>X</code>.
I.e. reduces back to the assay-of-interest name only.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_rel(self):
    &#34;&#34;&#34;
    Crops the `X_rel_Y` column-names of Delta-Delta-Ct results to just `X`.
    I.e. reduces back to the assay-of-interest name only.
    &#34;&#34;&#34;
    colnames = self._df.columns
    to_change = {i : i.split(&#34;_rel_&#34;)[0] for i in colnames if &#34;_rel_&#34; in i }
    self.rename_cols(to_change)

    # also recompute the stats df with new names...
    if self._stats_df is not None: 
        self.stats(recompute = True)</code></pre>
</details>
</dd>
<dt id="qpcr.Results.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<p>data : pd.DataFrame
The results dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self):
    &#34;&#34;&#34;
    Returns 
    -------
    data : pd.DataFrame
        The results dataframe
    &#34;&#34;&#34;
    return self._df</code></pre>
</details>
</dd>
<dt id="qpcr.Results.is_empty"><code class="name flex">
<span>def <span class="ident">is_empty</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks if any results have been stored so far.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd><code>True</code> if NO data is yet stored, else <code>False</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_empty(self):
    &#34;&#34;&#34;
    Checks if any results have been stored so far.

    Returns
    -------
    bool
        `True` if NO data is yet stored, else `False`.
    &#34;&#34;&#34;
    return self._df is None</code></pre>
</details>
</dd>
<dt id="qpcr.Results.merge"><code class="name flex">
<span>def <span class="ident">merge</span></span>(<span>self, *Results)</span>
</code></dt>
<dd>
<div class="desc"><p>Merges any number of other qpcr.Results objects into this one.
The source id of the results is added as column-name suffix. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>*Results</code></strong></dt>
<dd>An arbitrary number of qpcr.Results objects.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge(self, *Results):
    &#34;&#34;&#34;
    Merges any number of other qpcr.Results objects into this one.
    The source id of the results is added as column-name suffix. 

    Parameters
    ----------
    *Results
        An arbitrary number of qpcr.Results objects.

    &#34;&#34;&#34;
    new_df = self._df
    for R in Results: 
        R_df = R.get()

        # get only the delta-delta-Ct columns
        cols = [i for i in R_df.columns if i not in ref_cols]
        R_df = R_df[cols]


        # we merge the dataframes first without adding 
        # some new id suffix, only do so if this fails
        try: 
            new_df = pd.merge(new_df, R_df, 
                                right_index = True, left_index = True, 
                            )
        except: 
            new_df = pd.merge(new_df, R_df, 
                            right_index = True, left_index = True, 
                            suffixes = [f&#34;_{self.id()}&#34;, f&#34;_{R.id()}&#34;]
                        )
    self._df = new_df</code></pre>
</details>
</dd>
<dt id="qpcr.Results.names"><code class="name flex">
<span>def <span class="ident">names</span></span>(<span>self, as_set=False)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<p>names : list or None
The adopted <code>group_names</code>
(only works if a <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> has already been linked
using <code>adopt_names()</code>!)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def names(self, as_set = False):
    &#34;&#34;&#34;
    Returns 
    -------
    names : list or None
        The adopted `group_names` 
        (only works if a `qpcr.Assay` has already been linked 
        using `adopt_names()`!)
    &#34;&#34;&#34;
    if self._df is not None:
        names = self._df[&#34;group_name&#34;]
        if as_set: names = aux.sorted_set(names)
        return names
    return None</code></pre>
</details>
</dd>
<dt id="qpcr.Results.preview"><code class="name flex">
<span>def <span class="ident">preview</span></span>(<span>self, kind:Â strÂ =Â 'AssayBars', mode:Â strÂ =Â 'static', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A shortcut to call on a <code>qpcr.Plotters.PreviewResults</code> wrapper to visualise
the results.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>kind</code></strong> :&ensp;<code>str</code></dt>
<dd>The kind of Plotter to call. This can be any of the four wrapped
Plotters, e.g. <code>kind = "GroupBars"</code>.</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>The plotting mode. May be either "static" (matplotlib) or "interactive" (plotly).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>fig</code></strong> :&ensp;<code>plt.figure</code> or <code>plotly.figure</code></dt>
<dd>The figure generated by <code>PreviewResults</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preview( self, kind : str = &#34;AssayBars&#34;, mode : str = &#34;static&#34;, **kwargs ):
    &#34;&#34;&#34;
    A shortcut to call on a `qpcr.Plotters.PreviewResults` wrapper to visualise 
    the results.

    Parameters
    ----------
    kind : str
        The kind of Plotter to call. This can be any of the four wrapped 
        Plotters, e.g. `kind = &#34;GroupBars&#34;`.
    mode : str
        The plotting mode. May be either &#34;static&#34; (matplotlib) or &#34;interactive&#34; (plotly).

    Returns
    -------
    fig : plt.figure or plotly.figure
        The figure generated by `PreviewResults`.
    &#34;&#34;&#34;
    preview_results = Plotters.PreviewResults( mode = mode, kind = kind )
    preview_results.params( **kwargs )
    preview_results.link( self )
    fig = preview_results.plot()
    return fig </code></pre>
</details>
</dd>
<dt id="qpcr.Results.rename_cols"><code class="name flex">
<span>def <span class="ident">rename_cols</span></span>(<span>self, cols:Â dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Renames columns according to a dictionary as key -&gt; value.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>cols</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary specifying old column names (keys) and new colums names (values).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rename_cols(self, cols:dict):
    &#34;&#34;&#34;
    Renames columns according to a dictionary as key -&gt; value.

    Parameters
    ----------
    cols : dict
        A dictionary specifying old column names (keys) and new colums names (values).
    &#34;&#34;&#34;
    self._df = self._df.rename(columns = cols)</code></pre>
</details>
</dd>
<dt id="qpcr.Results.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, path, df=True, stats=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves a csv file for each specified type of results.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path has to be a filepath if only one type of results shall be saved (i.e. either <code>df</code> or <code>stats</code>),
otherwise a path to the directory where both <code>df</code> and <code>stats</code> shall be saved.</dd>
<dt><strong><code>df</code></strong> :&ensp;<code>bool</code></dt>
<dd>Save the results dataframe containing all replicate values (the full results).
Default is <code>df = True</code>.</dd>
<dt><strong><code>stats</code></strong> :&ensp;<code>bool</code></dt>
<dd>Save the results dataframe containing summary statistics for all replicate groups.
Default is <code>stats = True</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, path, df = True, stats = True):
    &#34;&#34;&#34;
    Saves a csv file for each specified type of results.

    Parameters
    ----------
    path : str
        Path has to be a filepath if only one type of results shall be saved (i.e. either `df` or `stats`), 
        otherwise a path to the directory where both `df` and `stats` shall be saved.
    
    df : bool
        Save the results dataframe containing all replicate values (the full results).
        Default is `df = True`.
    
    stats : bool
        Save the results dataframe containing summary statistics for all replicate groups.
        Default is `stats = True`.
    
    &#34;&#34;&#34;
    if df and stats and not os.path.isdir(path):
        aw.HardWarning(&#34;Results:save_need_dir&#34;)

    if df:
        # in case of raw results export we don&#39;t need the &#34;assay&#34; column as all 
        # assays are stored as separate columns anyaway, so it doesn&#39;t store any useful data
        _df = self._df
        if &#34;assay&#34; in _df.columns: _df = self._df.drop( columns = [&#34;assay&#34;] )
        self._save_single(path, _df, &#34;_df&#34;)
    if stats:
        # compute stats if none have been computed yet...
        if self._stats_df is None:
            self.stats()
        self._save_single(path, self._stats_df, &#34;_stats&#34;)</code></pre>
</details>
</dd>
<dt id="qpcr.Results.split"><code class="name flex">
<span>def <span class="ident">split</span></span>(<span>self, reset_names=False, drop_rel=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Splits the stored results dataframe into separate qpcr.Results objects containing only a signle deltaCt column each.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>reset_names</code></strong> :&ensp;<code>bool</code></dt>
<dd>Resets the deltaCt column-name from <code>"X_rel_Y"</code> to just <code>"dCt"</code>.</dd>
<dt><strong><code>drop_rel</code></strong> :&ensp;<code>bool</code></dt>
<dd>Crops <code>"X_rel_Y"</code> deltaCt column-names to just <code>"X"</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>objects : list
A list of qpcr.Results objects containing only a single dCt column each (retaining group columns etc.)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split(self, reset_names = False, drop_rel = True):
    &#34;&#34;&#34;
    Splits the stored results dataframe into separate qpcr.Results objects containing only a signle deltaCt column each.

    Parameters
    ----------
    reset_names : bool
        Resets the deltaCt column-name from `&#34;X_rel_Y&#34;` to just `&#34;dCt&#34;`.

    drop_rel : bool
        Crops `&#34;X_rel_Y&#34;` deltaCt column-names to just `&#34;X&#34;`. 

    Returns 
    -------
    objects : list
        A list of qpcr.Results objects containing only a single dCt column each (retaining group columns etc.)
    &#34;&#34;&#34;
    shared_columns = [i for i in self._df.columns if i in ref_cols]
    dct_columns = [i for i in self._df.columns if i not in ref_cols]
    
    dfs = [self._df[shared_columns + [i]] for i in dct_columns]
    objects = [Results() for i in dfs]

    for o, df, dct_col in zip(objects, dfs, dct_columns): 
        o._df = df
        if reset_names:
            o.rename_cols({dct_col : &#34;dCt&#34;})
        if drop_rel: 
            o.drop_rel()

        o.id(dct_col)
    
    return objects</code></pre>
</details>
</dd>
<dt id="qpcr.Results.stats"><code class="name flex">
<span>def <span class="ident">stats</span></span>(<span>self, recompute=False) â€‘>Â pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Computes summary statistis about the replicate groups:
<code>Mean</code>, <code>Median</code>, and <code>StDev</code> of all replicate groups, for all datasets (assays).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>recompute</code></strong> :&ensp;<code>bool</code></dt>
<dd>Statistics will only be once unless recompute is set to <code>True</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>stats_df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>A new dataframe containing the computed statistics for each replicate group.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stats(self, recompute = False) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Computes summary statistis about the replicate groups: 
    `Mean`, `Median`, and `StDev` of all replicate groups, for all datasets (assays).
    
    Parameters
    ----------
    recompute : bool
        Statistics will only be once unless recompute is set to `True`.

    Returns
    -------
    stats_df : pd.DataFrame
        A new dataframe containing the computed statistics for each replicate group.

    &#34;&#34;&#34;
    default_dataset_header = defaults.default_dataset_header
    # if stats_df is already present, return but sorted according to assays, not groups (nicer for user to inspect)
    if self._stats_df is not None and not recompute:
        return self._stats_df.sort_values(default_dataset_header)
    elif recompute: 
        self._stats_results = {&#34;group&#34; : [], default_dataset_header : [], &#34;mean&#34; : [], &#34;stdev&#34; : [], &#34;median&#34; : []}
        self._stats_df = None

    # get groups and corresponding assay columns 
    groups = aux.sorted_set(list(self._df[&#34;group&#34;]))
    assays = [c for c in self._df.columns if c not in ref_cols]
 
    # compute stats for all replicates per group
    for group in groups:
        group_subset = self._df.query(f&#34;group == {group}&#34;)
        
        median = self._stat_var(group_subset, np.nanmedian)
        mean = self._stat_var(group_subset, np.nanmean)
        stdv = self._stat_var(group_subset, np.nanstd)
        self._add_stats(assays, group, median, mean, stdv)
        
    # add group names
    self._add_stats_names(assays)

    self._stats_df = pd.DataFrame(self._stats_results)
    return self._stats_df.sort_values(default_dataset_header)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="qpcr.SampleReader"><code class="flex name class">
<span>class <span class="ident">SampleReader</span></span>
</code></dt>
<dd>
<div class="desc"><p>Sets up a Reader+Assay pipeline that reads in a single datafile and handles the
extracted dataset in a pandas dataframe.
Its <code>read()</code> method directly returns a <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object that can be piped to Analyser. </p>
<h2 id="note">Note</h2>
<p>This is now deprecated and will be removed in a future version! Please, use the <code><a title="qpcr.DataReader" href="#qpcr.DataReader">DataReader</a></code> instead.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SampleReader(Assay):
    &#34;&#34;&#34;
    Sets up a Reader+Assay pipeline that reads in a single datafile and handles the 
    extracted dataset in a pandas dataframe. 
    Its `read()` method directly returns a `qpcr.Assay` object that can be piped to Analyser. 
    
    Note
    ----
    This is now deprecated and will be removed in a future version! Please, use the `qpcr.DataReader` instead.
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        
        aw.SoftWarning(&#34;Versions:Deprecation&#34;, old = &#34;SampleReader&#34;, new = &#34;DataReader&#34;)

        self._replicates = None
        self._names = None
        self._Reader = None
        self._Assay = None

    # def __str__(self):
    #     header = f&#34;qpcr.SampleReader ({self._id})&#34;
    #     reps = f&#34;Replicate settings:\t{self._replicates}&#34;
    #     names = f&#34;Name settings:\t\t{self._names}&#34;

    #     header_line = max([len(i) for i in [header, reps, names]])
    #     header_line = &#34;-&#34; * header_line

    #     string = f&#34;{header_line}\n{header}\n{header_line}\n\n{reps}\n{names}\n\n{header_line}&#34;
    #     return string

    def replicates(self, replicates:(int or tuple)):
        &#34;&#34;&#34;
        Set the replicates specifics to use for grouping.

        Parameters
        ----------
        replicates : int or tuple
            Can be an `integer` (equal group sizes, e.g. `3` for triplicates), 
            or a `tuple` (uneven group sizes, e.g. `(3,2,3)` if the second group is only a duplicate). 
        &#34;&#34;&#34;
        self._replicates = replicates

    def names(self, names:(list or dict)):
        &#34;&#34;&#34;
        Set names for replicates groups.

        Parameters
        ----------
        names : list or dict
            Either a `list` (new names without repetitions) or `dict` (key = old name, value = new name) specifying new group names. 
            Group names only need to be specified once, and are applied to all replicate entries.
        &#34;&#34;&#34;
        self._names = names
        
    def read(self, filename, **kwargs):
        &#34;&#34;&#34;
        Reads one raw datafile (csv or excel format).

        Parameters
        ----------
        filename : str
            A filepath to a raw data file.
            If the file is a `csv` file, it has to have two named columns; one for replicate names, one for Ct values. 
            Both csv (`,` spearated) and csv2 (`;` separated) are accepted.
            If the file is an `excel` file it the relevant sections of the spreadsheet are identified automatically. 
            But they require identifying headers. By default `Name` and `Ct` are assumed but these can be changed using 
            the `name_label` and `Ct_label` arguments that can be passed to `read()` as kwargs.
        **kwargs
            Any additional keyword arguments that should be passed to the `qpcr.Reader`.

        Returns
        -------
        Assay : qpcr.Assay
            A `qpcr.Assay` object containing the grouped and renamed data.
        &#34;&#34;&#34;
        self._Reader = Reader(filename, **kwargs)
        self._Reader.id(aux.fileID(filename))

        self._Assay = Assay(self._Reader)
        self._Assay.adopt_id(self._Reader)

        if self._replicates is not None:
            self._Assay.replicates(self._replicates)
        else: 
            pass 
            # VITAL CHANGE HERE
            # since Assays can now infer replicates we 
            # don&#39;t raise an Error if no replicates are specified manually...
            # aw.HardWarning(&#34;SampleReader:no_reps_yet&#34;)

        if self._names is not None:
            self._Assay.group(infer_names = False)
            self._Assay.rename(self._names)
        else:
            self._Assay.group()
            
        return self._Assay</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></li>
<li>qpcr._auxiliary._ID</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qpcr.SampleReader.names"><code class="name flex">
<span>def <span class="ident">names</span></span>(<span>self, names:Â list)</span>
</code></dt>
<dd>
<div class="desc"><p>Set names for replicates groups.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>names</code></strong> :&ensp;<code>list</code> or <code>dict</code></dt>
<dd>Either a <code>list</code> (new names without repetitions) or <code>dict</code> (key = old name, value = new name) specifying new group names.
Group names only need to be specified once, and are applied to all replicate entries.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def names(self, names:(list or dict)):
    &#34;&#34;&#34;
    Set names for replicates groups.

    Parameters
    ----------
    names : list or dict
        Either a `list` (new names without repetitions) or `dict` (key = old name, value = new name) specifying new group names. 
        Group names only need to be specified once, and are applied to all replicate entries.
    &#34;&#34;&#34;
    self._names = names</code></pre>
</details>
</dd>
<dt id="qpcr.SampleReader.read"><code class="name flex">
<span>def <span class="ident">read</span></span>(<span>self, filename, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads one raw datafile (csv or excel format).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>A filepath to a raw data file.
If the file is a <code>csv</code> file, it has to have two named columns; one for replicate names, one for Ct values.
Both csv (<code>,</code> spearated) and csv2 (<code>;</code> separated) are accepted.
If the file is an <code>excel</code> file it the relevant sections of the spreadsheet are identified automatically.
But they require identifying headers. By default <code>Name</code> and <code>Ct</code> are assumed but these can be changed using
the <code>name_label</code> and <code>Ct_label</code> arguments that can be passed to <code>read()</code> as kwargs.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Any additional keyword arguments that should be passed to the <code><a title="qpcr.Reader" href="#qpcr.Reader">Reader</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Assay</code></strong> :&ensp;<code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code></dt>
<dd>A <code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code> object containing the grouped and renamed data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read(self, filename, **kwargs):
    &#34;&#34;&#34;
    Reads one raw datafile (csv or excel format).

    Parameters
    ----------
    filename : str
        A filepath to a raw data file.
        If the file is a `csv` file, it has to have two named columns; one for replicate names, one for Ct values. 
        Both csv (`,` spearated) and csv2 (`;` separated) are accepted.
        If the file is an `excel` file it the relevant sections of the spreadsheet are identified automatically. 
        But they require identifying headers. By default `Name` and `Ct` are assumed but these can be changed using 
        the `name_label` and `Ct_label` arguments that can be passed to `read()` as kwargs.
    **kwargs
        Any additional keyword arguments that should be passed to the `qpcr.Reader`.

    Returns
    -------
    Assay : qpcr.Assay
        A `qpcr.Assay` object containing the grouped and renamed data.
    &#34;&#34;&#34;
    self._Reader = Reader(filename, **kwargs)
    self._Reader.id(aux.fileID(filename))

    self._Assay = Assay(self._Reader)
    self._Assay.adopt_id(self._Reader)

    if self._replicates is not None:
        self._Assay.replicates(self._replicates)
    else: 
        pass 
        # VITAL CHANGE HERE
        # since Assays can now infer replicates we 
        # don&#39;t raise an Error if no replicates are specified manually...
        # aw.HardWarning(&#34;SampleReader:no_reps_yet&#34;)

    if self._names is not None:
        self._Assay.group(infer_names = False)
        self._Assay.rename(self._names)
    else:
        self._Assay.group()
        
    return self._Assay</code></pre>
</details>
</dd>
<dt id="qpcr.SampleReader.replicates"><code class="name flex">
<span>def <span class="ident">replicates</span></span>(<span>self, replicates:Â int)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the replicates specifics to use for grouping.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>replicates</code></strong> :&ensp;<code>int</code> or <code>tuple</code></dt>
<dd>Can be an <code>integer</code> (equal group sizes, e.g. <code>3</code> for triplicates),
or a <code>tuple</code> (uneven group sizes, e.g. <code>(3,2,3)</code> if the second group is only a duplicate).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def replicates(self, replicates:(int or tuple)):
    &#34;&#34;&#34;
    Set the replicates specifics to use for grouping.

    Parameters
    ----------
    replicates : int or tuple
        Can be an `integer` (equal group sizes, e.g. `3` for triplicates), 
        or a `tuple` (uneven group sizes, e.g. `(3,2,3)` if the second group is only a duplicate). 
    &#34;&#34;&#34;
    self._replicates = replicates</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></b></code>:
<ul class="hlist">
<li><code><a title="qpcr.Assay.Ct" href="#qpcr.Assay.Ct">Ct</a></code></li>
<li><code><a title="qpcr.Assay.add_dCt" href="#qpcr.Assay.add_dCt">add_dCt</a></code></li>
<li><code><a title="qpcr.Assay.add_ddCt" href="#qpcr.Assay.add_ddCt">add_ddCt</a></code></li>
<li><code><a title="qpcr.Assay.adopt" href="#qpcr.Assay.adopt">adopt</a></code></li>
<li><code><a title="qpcr.Assay.boxplot" href="#qpcr.Assay.boxplot">boxplot</a></code></li>
<li><code><a title="qpcr.Assay.dCt" href="#qpcr.Assay.dCt">dCt</a></code></li>
<li><code><a title="qpcr.Assay.ddCt" href="#qpcr.Assay.ddCt">ddCt</a></code></li>
<li><code><a title="qpcr.Assay.efficiency" href="#qpcr.Assay.efficiency">efficiency</a></code></li>
<li><code><a title="qpcr.Assay.get" href="#qpcr.Assay.get">get</a></code></li>
<li><code><a title="qpcr.Assay.group" href="#qpcr.Assay.group">group</a></code></li>
<li><code><a title="qpcr.Assay.groups" href="#qpcr.Assay.groups">groups</a></code></li>
<li><code><a title="qpcr.Assay.ignore" href="#qpcr.Assay.ignore">ignore</a></code></li>
<li><code><a title="qpcr.Assay.link" href="#qpcr.Assay.link">link</a></code></li>
<li><code><a title="qpcr.Assay.n" href="#qpcr.Assay.n">n</a></code></li>
<li><code><a title="qpcr.Assay.rename" href="#qpcr.Assay.rename">rename</a></code></li>
<li><code><a title="qpcr.Assay.rename_cols" href="#qpcr.Assay.rename_cols">rename_cols</a></code></li>
<li><code><a title="qpcr.Assay.save" href="#qpcr.Assay.save">save</a></code></li>
<li><code><a title="qpcr.Assay.stack" href="#qpcr.Assay.stack">stack</a></code></li>
<li><code><a title="qpcr.Assay.tile" href="#qpcr.Assay.tile">tile</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="qpcr._CORE_Reader"><code class="flex name class">
<span>class <span class="ident">_CORE_Reader</span></span>
</code></dt>
<dd>
<div class="desc"><p>The class handling the core functions of the Reader class.
The standard qpcr.Reader inherits from this. </p>
<h2 id="note">Note</h2>
<p>This implementation is deprecated. The _CORE_Reader has been moved
(and further updated since) to the <code><a title="qpcr.Readers" href="Readers/index.html">qpcr.Readers</a></code> submodule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class _CORE_Reader(aux._ID):
    &#34;&#34;&#34;
    The class handling the core functions of the Reader class. 
    The standard qpcr.Reader inherits from this. 

    Note
    ----
    This implementation is deprecated. The _CORE_Reader has been moved
    (and further updated since) to the `qpcr.Readers` submodule. 

    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._src = None
        self._delimiter = None
        self._df = None

    def get(self):
        &#34;&#34;&#34;
        Returns
        -------
        data : pd.DataFrame
            The dataframe from the datafile.
        &#34;&#34;&#34;
        return self._df

    def n(self):
        &#34;&#34;&#34;
        Returns
        -------
        n : int
            The number of replicates (entries) in the dataframe.
        &#34;&#34;&#34;
        return len(self._df[raw_col_names[0]])


    def read(self, **kwargs):
        &#34;&#34;&#34;
        Reads the given data file.

        If the data file is an Excel file replicates and their Ct values will be 
        extracted from the first excel sheet of the file. Note, this assumes by default
        that the replicates are headed by the label `&#34;Name&#34;` and the corresponding Ct values
        are headed by the label `&#34;Ct&#34;`. Both labels have to be on the same row. 

        If these labels do not match your excel file, you may
        specify `name_label` and `Ct_label` as additional arguments.
        &#34;&#34;&#34;
        suffix = self._filesuffix()
        if suffix == &#34;csv&#34;:
            try: 
                self._csv_read()
            except:
                # setup parser
                parser = Parsers.CsvParser()
                # check it file should be read transposed
                transpose = aux.from_kwargs(&#34;transpose&#34;, False, kwargs, rm = True)
                if transpose:
                    parser.transpose()
                
                # setup patterns and store assay-of-interest
                assay_pattern = aux.from_kwargs(&#34;assay_pattern&#34;, &#34;Rotor-Gene&#34;, kwargs)
                assay_of_interest = aux.from_kwargs(&#34;assay&#34;, None, kwargs, rm=True)
                parser.assay_pattern(assay_pattern)
                
                # get data column labels
                id_label = aux.from_kwargs(&#34;id_label&#34;, &#34;Name&#34;, kwargs, rm = True)
                ct_label = aux.from_kwargs(&#34;ct_label&#34;, &#34;Ct&#34;, kwargs, rm = True)
                parser.labels(id_label,ct_label)
                
                # pipe the datafile through the parser
                parser.pipe(self._src, **kwargs)

                if len(parser.assays()) &gt; 1:
                    if assay_of_interest is None: 
                        aw.HardWarning(&#34;Reader:cannot_read_multifile&#34;, file = self._src, assays = parser.assays())
                    self._df = parser.get(assay_of_interest)
                    self.id(assay_of_interest)
                else:
                    assay_of_interest = parser.assays()[0]
                    self._df = parser.get(assay_of_interest)
                    self.id(assay_of_interest)

        elif suffix == &#34;xlsx&#34;:
            # setup parser
            parser = Parsers.ExcelParser()
            # check it file should be read transposed
            transpose = aux.from_kwargs(&#34;transpose&#34;, False, kwargs, rm = True)
            if transpose:
                parser.transpose()
            
            # check for sheet_name
            sheet_name = aux.from_kwargs(&#34;sheet_name&#34;, 0, kwargs, rm = True)

            # setup patterns and store assay-of-interest
            assay_pattern = aux.from_kwargs(&#34;assay_pattern&#34;, &#34;Rotor-Gene&#34;, kwargs)
            assay_of_interest = aux.from_kwargs(&#34;assay&#34;, None, kwargs, rm=True)
            parser.assay_pattern(assay_pattern)
            
            # get data column labels
            id_label = aux.from_kwargs(&#34;id_label&#34;, &#34;Name&#34;, kwargs, rm = True)
            ct_label = aux.from_kwargs(&#34;ct_label&#34;, &#34;Ct&#34;, kwargs, rm = True)
            parser.labels(id_label,ct_label)

            # pipe the datafile through the parser
            parser.read(self._src, sheet_name = sheet_name)
            parser.parse(**kwargs)

            if len(parser.assays()) &gt; 1:
                if assay_of_interest is None: 
                    aw.HardWarning(&#34;Reader:cannot_read_multifile&#34;, file = self._src, assays = parser.assays(), traceback = False)
                self._df = parser.get(assay_of_interest)
                self.id(assay_of_interest)
            else:
                assay_of_interest = parser.assays()[0]
                self._df = parser.get(assay_of_interest)
                self.id(assay_of_interest)
        
    def _csv_read(self, **kwargs):
        &#34;&#34;&#34;
        Reads the given data file if it&#39;s a csv file
        &#34;&#34;&#34;
        df = None
        try: 
            df = pd.read_csv(
                                self._src, 
                                sep = self._delimiter, 
                                header = aux.from_kwargs(&#34;header&#34;, 0, kwargs), 
                                names = raw_col_names
                            )
        except: 
            aw.HardWarning(&#34;Reader:cannot_read_csv&#34;, file = self._src)

        # check if a valid Ct column was found
        Ct = raw_col_names[1]
        full_valid_Ct_col = len(  df[ df[Ct] == df[Ct] ]  ) == len(df)
        if not full_valid_Ct_col:
            aw.HardWarning(&#34;Reader:cannot_read_csv&#34;, file = self._src)

        self._df = df

    def _filesuffix(self):
        &#34;&#34;&#34;
        Returns the file-suffix of the provided file
        &#34;&#34;&#34;
        try: 
            suffix = self._src.split(&#34;.&#34;)[-1]
        except: 
            pass
        return suffix</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>qpcr._auxiliary._ID</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="qpcr.Reader" href="#qpcr.Reader">Reader</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qpcr._CORE_Reader.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The dataframe from the datafile.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self):
    &#34;&#34;&#34;
    Returns
    -------
    data : pd.DataFrame
        The dataframe from the datafile.
    &#34;&#34;&#34;
    return self._df</code></pre>
</details>
</dd>
<dt id="qpcr._CORE_Reader.n"><code class="name flex">
<span>def <span class="ident">n</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of replicates (entries) in the dataframe.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def n(self):
    &#34;&#34;&#34;
    Returns
    -------
    n : int
        The number of replicates (entries) in the dataframe.
    &#34;&#34;&#34;
    return len(self._df[raw_col_names[0]])</code></pre>
</details>
</dd>
<dt id="qpcr._CORE_Reader.read"><code class="name flex">
<span>def <span class="ident">read</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads the given data file.</p>
<p>If the data file is an Excel file replicates and their Ct values will be
extracted from the first excel sheet of the file. Note, this assumes by default
that the replicates are headed by the label <code>"Name"</code> and the corresponding Ct values
are headed by the label <code>"Ct"</code>. Both labels have to be on the same row. </p>
<p>If these labels do not match your excel file, you may
specify <code>name_label</code> and <code>Ct_label</code> as additional arguments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read(self, **kwargs):
    &#34;&#34;&#34;
    Reads the given data file.

    If the data file is an Excel file replicates and their Ct values will be 
    extracted from the first excel sheet of the file. Note, this assumes by default
    that the replicates are headed by the label `&#34;Name&#34;` and the corresponding Ct values
    are headed by the label `&#34;Ct&#34;`. Both labels have to be on the same row. 

    If these labels do not match your excel file, you may
    specify `name_label` and `Ct_label` as additional arguments.
    &#34;&#34;&#34;
    suffix = self._filesuffix()
    if suffix == &#34;csv&#34;:
        try: 
            self._csv_read()
        except:
            # setup parser
            parser = Parsers.CsvParser()
            # check it file should be read transposed
            transpose = aux.from_kwargs(&#34;transpose&#34;, False, kwargs, rm = True)
            if transpose:
                parser.transpose()
            
            # setup patterns and store assay-of-interest
            assay_pattern = aux.from_kwargs(&#34;assay_pattern&#34;, &#34;Rotor-Gene&#34;, kwargs)
            assay_of_interest = aux.from_kwargs(&#34;assay&#34;, None, kwargs, rm=True)
            parser.assay_pattern(assay_pattern)
            
            # get data column labels
            id_label = aux.from_kwargs(&#34;id_label&#34;, &#34;Name&#34;, kwargs, rm = True)
            ct_label = aux.from_kwargs(&#34;ct_label&#34;, &#34;Ct&#34;, kwargs, rm = True)
            parser.labels(id_label,ct_label)
            
            # pipe the datafile through the parser
            parser.pipe(self._src, **kwargs)

            if len(parser.assays()) &gt; 1:
                if assay_of_interest is None: 
                    aw.HardWarning(&#34;Reader:cannot_read_multifile&#34;, file = self._src, assays = parser.assays())
                self._df = parser.get(assay_of_interest)
                self.id(assay_of_interest)
            else:
                assay_of_interest = parser.assays()[0]
                self._df = parser.get(assay_of_interest)
                self.id(assay_of_interest)

    elif suffix == &#34;xlsx&#34;:
        # setup parser
        parser = Parsers.ExcelParser()
        # check it file should be read transposed
        transpose = aux.from_kwargs(&#34;transpose&#34;, False, kwargs, rm = True)
        if transpose:
            parser.transpose()
        
        # check for sheet_name
        sheet_name = aux.from_kwargs(&#34;sheet_name&#34;, 0, kwargs, rm = True)

        # setup patterns and store assay-of-interest
        assay_pattern = aux.from_kwargs(&#34;assay_pattern&#34;, &#34;Rotor-Gene&#34;, kwargs)
        assay_of_interest = aux.from_kwargs(&#34;assay&#34;, None, kwargs, rm=True)
        parser.assay_pattern(assay_pattern)
        
        # get data column labels
        id_label = aux.from_kwargs(&#34;id_label&#34;, &#34;Name&#34;, kwargs, rm = True)
        ct_label = aux.from_kwargs(&#34;ct_label&#34;, &#34;Ct&#34;, kwargs, rm = True)
        parser.labels(id_label,ct_label)

        # pipe the datafile through the parser
        parser.read(self._src, sheet_name = sheet_name)
        parser.parse(**kwargs)

        if len(parser.assays()) &gt; 1:
            if assay_of_interest is None: 
                aw.HardWarning(&#34;Reader:cannot_read_multifile&#34;, file = self._src, assays = parser.assays(), traceback = False)
            self._df = parser.get(assay_of_interest)
            self.id(assay_of_interest)
        else:
            assay_of_interest = parser.assays()[0]
            self._df = parser.get(assay_of_interest)
            self.id(assay_of_interest)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">

<a href="https://github.com/NoahHenrikKleinschmidt/qpcr">
<img src="./qpcr_light.svg" width = "100%" >
</a>
<h1>Index</h1>

<div class="toc">
<ul>
<li><a href="#terminology">Terminology</a><ul>
<li><a href="#qpcr-vs-qpcr">qpcr vs qPCR</a></li>
<li><a href="#file-or-datafile">File (or "datafile")</a></li>
<li><a href="#regular-vs-irregular-datafiles">"Regular" vs "irregular" datafiles</a></li>
<li><a href="#an-qpcrassay-and-a-dataset-assay">An qpcr.Assay and a "dataset" / "assay"</a></li>
<li><a href="#replicates">Replicates</a></li>
<li><a href="#groups-of-replicates">Groups (of Replicates)</a></li>
<li><a href="#specifying-groups-of-replicates">Specifying (Groups of) Replicates</a></li>
<li><a href="#delta-ct-vs-delta-delta-ct-vs-normalisation">Delta-Ct vs Delta-Delta-Ct vs normalisation</a></li>
<li><a href="#the-anchor-and-the-reference-group">The anchor and the "reference group"</a></li>
<li><a href="#assays-vs-normalisers">"assays" vs "normalisers"</a></li>
<li><a href="#samples">"Samples"</a></li>
</ul>
</li>
<li><a href="#some-more-basics">Some more basics</a><ul>
<li><a href="#the-values-stored-by-qpcr">The values stored by qpcr</a></li>
<li><a href="#modes-of-normalisation">Modes of normalisation</a></li>
<li><a href="#pipelines">pipelines</a></li>
<li><a href="#results-my-final-results">"Results" = my final results?</a></li>
<li><a href="#getting-your-data">getting your data</a></li>
<li><a href="#link-vs-add-vs-pipe">link vs add vs pipe</a></li>
</ul>
</li>
<li><a href="#getting-started">Getting started</a></li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="qpcr.Filters" href="Filters/index.html">qpcr.Filters</a></code></li>
<li><code><a title="qpcr.Parsers" href="Parsers/index.html">qpcr.Parsers</a></code></li>
<li><code><a title="qpcr.Pipes" href="Pipes/index.html">qpcr.Pipes</a></code></li>
<li><code><a title="qpcr.Plotters" href="Plotters/index.html">qpcr.Plotters</a></code></li>
<li><code><a title="qpcr.Readers" href="Readers/index.html">qpcr.Readers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="qpcr.Analyser" href="#qpcr.Analyser">Analyser</a></code></h4>
<ul class="two-column">
<li><code><a title="qpcr.Analyser.DeltaCt" href="#qpcr.Analyser.DeltaCt">DeltaCt</a></code></li>
<li><code><a title="qpcr.Analyser.anchor" href="#qpcr.Analyser.anchor">anchor</a></code></li>
<li><code><a title="qpcr.Analyser.efficiency" href="#qpcr.Analyser.efficiency">efficiency</a></code></li>
<li><code><a title="qpcr.Analyser.func" href="#qpcr.Analyser.func">func</a></code></li>
<li><code><a title="qpcr.Analyser.get" href="#qpcr.Analyser.get">get</a></code></li>
<li><code><a title="qpcr.Analyser.link" href="#qpcr.Analyser.link">link</a></code></li>
<li><code><a title="qpcr.Analyser.pipe" href="#qpcr.Analyser.pipe">pipe</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="qpcr.Assay" href="#qpcr.Assay">Assay</a></code></h4>
<ul class="two-column">
<li><code><a title="qpcr.Assay.Ct" href="#qpcr.Assay.Ct">Ct</a></code></li>
<li><code><a title="qpcr.Assay.add_dCt" href="#qpcr.Assay.add_dCt">add_dCt</a></code></li>
<li><code><a title="qpcr.Assay.add_ddCt" href="#qpcr.Assay.add_ddCt">add_ddCt</a></code></li>
<li><code><a title="qpcr.Assay.adopt" href="#qpcr.Assay.adopt">adopt</a></code></li>
<li><code><a title="qpcr.Assay.boxplot" href="#qpcr.Assay.boxplot">boxplot</a></code></li>
<li><code><a title="qpcr.Assay.dCt" href="#qpcr.Assay.dCt">dCt</a></code></li>
<li><code><a title="qpcr.Assay.ddCt" href="#qpcr.Assay.ddCt">ddCt</a></code></li>
<li><code><a title="qpcr.Assay.efficiency" href="#qpcr.Assay.efficiency">efficiency</a></code></li>
<li><code><a title="qpcr.Assay.get" href="#qpcr.Assay.get">get</a></code></li>
<li><code><a title="qpcr.Assay.group" href="#qpcr.Assay.group">group</a></code></li>
<li><code><a title="qpcr.Assay.groups" href="#qpcr.Assay.groups">groups</a></code></li>
<li><code><a title="qpcr.Assay.ignore" href="#qpcr.Assay.ignore">ignore</a></code></li>
<li><code><a title="qpcr.Assay.link" href="#qpcr.Assay.link">link</a></code></li>
<li><code><a title="qpcr.Assay.n" href="#qpcr.Assay.n">n</a></code></li>
<li><code><a title="qpcr.Assay.names" href="#qpcr.Assay.names">names</a></code></li>
<li><code><a title="qpcr.Assay.rename" href="#qpcr.Assay.rename">rename</a></code></li>
<li><code><a title="qpcr.Assay.rename_cols" href="#qpcr.Assay.rename_cols">rename_cols</a></code></li>
<li><code><a title="qpcr.Assay.replicates" href="#qpcr.Assay.replicates">replicates</a></code></li>
<li><code><a title="qpcr.Assay.save" href="#qpcr.Assay.save">save</a></code></li>
<li><code><a title="qpcr.Assay.stack" href="#qpcr.Assay.stack">stack</a></code></li>
<li><code><a title="qpcr.Assay.tile" href="#qpcr.Assay.tile">tile</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="qpcr.Calibrator" href="#qpcr.Calibrator">Calibrator</a></code></h4>
<ul class="two-column">
<li><code><a title="qpcr.Calibrator.adopt" href="#qpcr.Calibrator.adopt">adopt</a></code></li>
<li><code><a title="qpcr.Calibrator.assign" href="#qpcr.Calibrator.assign">assign</a></code></li>
<li><code><a title="qpcr.Calibrator.calibrate" href="#qpcr.Calibrator.calibrate">calibrate</a></code></li>
<li><code><a title="qpcr.Calibrator.clear" href="#qpcr.Calibrator.clear">clear</a></code></li>
<li><code><a title="qpcr.Calibrator.computed_values" href="#qpcr.Calibrator.computed_values">computed_values</a></code></li>
<li><code><a title="qpcr.Calibrator.dilution" href="#qpcr.Calibrator.dilution">dilution</a></code></li>
<li><code><a title="qpcr.Calibrator.efficiencies" href="#qpcr.Calibrator.efficiencies">efficiencies</a></code></li>
<li><code><a title="qpcr.Calibrator.get" href="#qpcr.Calibrator.get">get</a></code></li>
<li><code><a title="qpcr.Calibrator.load" href="#qpcr.Calibrator.load">load</a></code></li>
<li><code><a title="qpcr.Calibrator.merge" href="#qpcr.Calibrator.merge">merge</a></code></li>
<li><code><a title="qpcr.Calibrator.pipe" href="#qpcr.Calibrator.pipe">pipe</a></code></li>
<li><code><a title="qpcr.Calibrator.plot" href="#qpcr.Calibrator.plot">plot</a></code></li>
<li><code><a title="qpcr.Calibrator.reset" href="#qpcr.Calibrator.reset">reset</a></code></li>
<li><code><a title="qpcr.Calibrator.save" href="#qpcr.Calibrator.save">save</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="qpcr.DataReader" href="#qpcr.DataReader">DataReader</a></code></h4>
<ul class="two-column">
<li><code><a title="qpcr.DataReader.Reader" href="#qpcr.DataReader.Reader">Reader</a></code></li>
<li><code><a title="qpcr.DataReader.clear" href="#qpcr.DataReader.clear">clear</a></code></li>
<li><code><a title="qpcr.DataReader.get" href="#qpcr.DataReader.get">get</a></code></li>
<li><code><a title="qpcr.DataReader.prune" href="#qpcr.DataReader.prune">prune</a></code></li>
<li><code><a title="qpcr.DataReader.read" href="#qpcr.DataReader.read">read</a></code></li>
<li><code><a title="qpcr.DataReader.reset" href="#qpcr.DataReader.reset">reset</a></code></li>
<li><code><a title="qpcr.DataReader.store" href="#qpcr.DataReader.store">store</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="qpcr.Normaliser" href="#qpcr.Normaliser">Normaliser</a></code></h4>
<ul class="two-column">
<li><code><a title="qpcr.Normaliser.clear" href="#qpcr.Normaliser.clear">clear</a></code></li>
<li><code><a title="qpcr.Normaliser.get" href="#qpcr.Normaliser.get">get</a></code></li>
<li><code><a title="qpcr.Normaliser.link" href="#qpcr.Normaliser.link">link</a></code></li>
<li><code><a title="qpcr.Normaliser.norm_func" href="#qpcr.Normaliser.norm_func">norm_func</a></code></li>
<li><code><a title="qpcr.Normaliser.normalise" href="#qpcr.Normaliser.normalise">normalise</a></code></li>
<li><code><a title="qpcr.Normaliser.prep_func" href="#qpcr.Normaliser.prep_func">prep_func</a></code></li>
<li><code><a title="qpcr.Normaliser.prune" href="#qpcr.Normaliser.prune">prune</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="qpcr.Reader" href="#qpcr.Reader">Reader</a></code></h4>
</li>
<li>
<h4><code><a title="qpcr.Results" href="#qpcr.Results">Results</a></code></h4>
<ul class="two-column">
<li><code><a title="qpcr.Results.add" href="#qpcr.Results.add">add</a></code></li>
<li><code><a title="qpcr.Results.add_Ct" href="#qpcr.Results.add_Ct">add_Ct</a></code></li>
<li><code><a title="qpcr.Results.add_dCt" href="#qpcr.Results.add_dCt">add_dCt</a></code></li>
<li><code><a title="qpcr.Results.add_ddCt" href="#qpcr.Results.add_ddCt">add_ddCt</a></code></li>
<li><code><a title="qpcr.Results.adopt_names" href="#qpcr.Results.adopt_names">adopt_names</a></code></li>
<li><code><a title="qpcr.Results.drop_cols" href="#qpcr.Results.drop_cols">drop_cols</a></code></li>
<li><code><a title="qpcr.Results.drop_groups" href="#qpcr.Results.drop_groups">drop_groups</a></code></li>
<li><code><a title="qpcr.Results.drop_rel" href="#qpcr.Results.drop_rel">drop_rel</a></code></li>
<li><code><a title="qpcr.Results.get" href="#qpcr.Results.get">get</a></code></li>
<li><code><a title="qpcr.Results.is_empty" href="#qpcr.Results.is_empty">is_empty</a></code></li>
<li><code><a title="qpcr.Results.merge" href="#qpcr.Results.merge">merge</a></code></li>
<li><code><a title="qpcr.Results.names" href="#qpcr.Results.names">names</a></code></li>
<li><code><a title="qpcr.Results.preview" href="#qpcr.Results.preview">preview</a></code></li>
<li><code><a title="qpcr.Results.rename_cols" href="#qpcr.Results.rename_cols">rename_cols</a></code></li>
<li><code><a title="qpcr.Results.save" href="#qpcr.Results.save">save</a></code></li>
<li><code><a title="qpcr.Results.split" href="#qpcr.Results.split">split</a></code></li>
<li><code><a title="qpcr.Results.stats" href="#qpcr.Results.stats">stats</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="qpcr.SampleReader" href="#qpcr.SampleReader">SampleReader</a></code></h4>
<ul class="">
<li><code><a title="qpcr.SampleReader.names" href="#qpcr.SampleReader.names">names</a></code></li>
<li><code><a title="qpcr.SampleReader.read" href="#qpcr.SampleReader.read">read</a></code></li>
<li><code><a title="qpcr.SampleReader.replicates" href="#qpcr.SampleReader.replicates">replicates</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="qpcr._CORE_Reader" href="#qpcr._CORE_Reader">_CORE_Reader</a></code></h4>
<ul class="">
<li><code><a title="qpcr._CORE_Reader.get" href="#qpcr._CORE_Reader.get">get</a></code></li>
<li><code><a title="qpcr._CORE_Reader.n" href="#qpcr._CORE_Reader.n">n</a></code></li>
<li><code><a title="qpcr._CORE_Reader.read" href="#qpcr._CORE_Reader.read">read</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>