<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>qpcr.Parsers.Parsers API documentation</title>
<meta name="description" content="This module contains classes designed to work with irregularly structured datafiles.
It provides `Parsers` that are able to extract the replicate â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>qpcr.Parsers.Parsers</code></h1>
</header>
<section id="section-intro">
<p>This module contains classes designed to work with irregularly structured datafiles.
It provides <code>Parsers</code> that are able to extract the replicate identifiers and Ct values as pandas DataFrames
from irregular <code>csv</code> and <code>excel</code> files. </p>
<h2 id="working-with-irregular-files">Working with "irregular files"</h2>
<hr>
<p>Any datafile that does not only consist of a replicate identifer and Ct column is called "irregular".
In fact, most excel sheets or csv exports from qPCR machines are actually irregular as they often contain some
information about the run, and melting curve data, and so forth. Such data is not relevant or of interest to <code><a title="qpcr" href="../index.html">qpcr</a></code>, however,
so we have to extract the data we are intersted in. This is the job of the <code><a title="qpcr.Parsers" href="index.html">qpcr.Parsers</a></code>. They read in an irregular datafile and use
a guided-parsing approach to find the relevant sections within the datafile. There are essentially two ways how they can do this, which are explained below. </p>
<blockquote>
<p>In fact, <code><a title="qpcr.Parsers" href="index.html">qpcr.Parsers</a></code> are already implemented in the <code><a title="qpcr.Readers" href="../Readers/index.html">qpcr.Readers</a></code> so you will often
be able to read irregular datafiles directly with one of the <code><a title="qpcr.Readers" href="../Readers/index.html">qpcr.Readers</a></code> and will not have to manually work with the <code><a title="qpcr.Parsers" href="index.html">qpcr.Parsers</a></code>. </p>
</blockquote>
<h3 id="finding-relevant-datasets-through-assay_patterns">"Finding" relevant datasets through <code>assay_patterns</code></h3>
<p>The Parsers are quipped with a method called <code><a title="qpcr.Parsers.Parsers._CORE_Parser.find_assays" href="#qpcr.Parsers.Parsers._CORE_Parser.find_assays">_CORE_Parser.find_assays()</a></code> which locates assays (or more formally "datasets") within the datafile
using <code>regex</code>. Of course, in order to do that they need to know the patterns they are supposed to look for. Some patterns are already pre-specified
in the <code>qpcr.Parsers.assay_patterns</code> dictionary and can simply be specified using their key. If your own pattern is not yet pre-defined,
<a href="https://github.com/NoahHenrikKleinschmidt/qpcr/issues">post an issue on github</a> and supply some samples of how your assays usually appear in your datafiles
alongside with the name of the machine that produces your datafiles.
Of course, you can also manually specify your own regex pattern. The only constraint is that is <em>must</em> have <em>one</em> capturing group for the assay name.</p>
<blockquote>
<p>Please note, all assay headers must be located either in the same column or the same row to be identified by a <code>Parser</code>!</p>
</blockquote>
<p>Once the assays in your datafile are identified, the data columns belonging to them are searched for. The constraint here is that they must start either
in the row exactly below the assay headers or have at most one single row in between them. Anything else is no good! The data columns <em>must</em> be labelled
(i.e. have a header). By default <code>Name</code> and <code>Ct</code> are assumed as data column labels / headers, but this can be changed. </p>
<p>If your datafiles contain multiple datasets / assays, the <code><a title="qpcr.Parsers" href="index.html">qpcr.Parsers</a></code> will be able to extract all of them and store their extracted data. </p>
<h2 id="working-with-multi-assay-files">Working with <code>multi-assay</code> files</h2>
<hr>
<p>While working with datafiles that contain multiple assays you will likely want to use <em>all</em> the assays from the datafile, here's how the <code><a title="qpcr.Parsers" href="index.html">qpcr.Parsers</a></code> help you do this. </p>
<h3 id="getting-all-assays-from-a-multi-assay-file">Getting all assays from a multi-assay file</h3>
<p>Here it depends on what you want to do. Do you wish to extract the individual assays to make individual files from them or do you want to use them directly for an analysis?
Both options are possible. </p>
<h4 id="making-indivdual-assay-files-from-a-multi-assay-file">Making indivdual assay files from a multi-assay file</h4>
<p>This is the core-business of the <code><a title="qpcr.Parsers" href="index.html">qpcr.Parsers</a></code>. So you can simply set up a Parser, set a saving location using the
Parser's <code>save_to</code> method and then <code>pipe</code> your file through. All done at this point. Of course, you can also work with the dataframes directly
using the Parser's <code>get</code> method. Like this you easily separate the assays which you can then pass to your main analysis as assays and normalisers. </p>
<h4 id="using-a-multi-assay-file-directly-for-my-analysis">Using a multi-assay file directly for my analysis</h4>
<p>So, you want to just feed in your one datafile and expect to get a table with your Delta-Delta-Ct values for all assays against all normalisers?!
Sure, no problem! Parsers will be able to do that, but you can more easily read a multi-assay file using the <code>qpcr.Readers.MultiReader</code> which allows you (after setup) to simply <code>pipe</code> through your datafile
and returns immediately a list of your assays-of-interest and normaliser-assays. How does it know which is which though?
That's the super-important topic of the next paragraph.</p>
<h4 id="decorators">Decorators</h4>
<p>A <code>decorator</code> technically is a function that wraps another function when coding. Well, that's not quite the case for the <code><a title="qpcr" href="../index.html">qpcr</a></code> decorators but the idea is similar.
Instead of wrapping functions the <code><a title="qpcr" href="../index.html">qpcr</a></code> decorators wrap assays in a multi-assay file. What does "wrap" mean? It means they provide meta-data about the assay
in question. What does that mean? There are multiple implemented <code><a title="qpcr" href="../index.html">qpcr</a></code> decorators. For irregular multi-assay files the two important decorators are:
<code>@qpcr:assay</code> and <code>@qpcr:normaliser</code> â€“ is it now clear what they do? They are placed in the
cell <em>exactly above</em> the cell where the assay header is located (seriously, anything else won't do!) and tell the <code><a title="qpcr.Parsers" href="index.html">qpcr.Parsers</a></code> (because all the <code>MultiReader</code> is doing is setting up some <code>Parsers</code>)
if a specific assay is an "assay-of-interest" or a "normaliser-assay".</p>
<p>So, let's recap this quickly: <code><a title="qpcr.Parsers" href="index.html">qpcr.Parsers</a></code> can identify assays either through <em>de novo</em> finding using <code>regex</code> patterns <em>or</em> through decorators. To tell a Parser to use a specific decorator
for finding assays you can specify the <code>decorator</code> argument in <code>pipe</code> or <code>parse</code>. To specify a decorator like this you only write <code>qpcr:assay</code> <code>qpcr:normaliser</code>
(the <code>key</code> is bascially the decorator but without the <code>@</code>).</p>
<p>If you work with <code><a title="qpcr.Parsers" href="index.html">qpcr.Parsers</a></code> directly you can choose if you only wish to extract "assays-of-interest" (decorated as <code>@qpcr:assay</code>) <em>or</em> "normalisers", or whatever other decorators are available.
However, this flexibility is not available when calling Parsers indirectly through one of the <code><a title="qpcr.Readers" href="../Readers/index.html">qpcr.Readers</a></code>.</p>
<blockquote>
<p>Please note, just like assay headers, all decorators must be located either in the same column or the same row to be identified by a <code>Parser</code>!</p>
</blockquote>
<table>
<thead>
<tr>
<th>Decorator</th>
<th>Code-reference</th>
<th>Filetype</th>
<th>Available for / Used by <code><a title="qpcr.Readers" href="../Readers/index.html">qpcr.Readers</a></code></th>
</tr>
</thead>
<tbody>
<tr>
<td>@qpcr:all</td>
<td>qpcr:all</td>
<td>Irregular single- or multi-assay files.</td>
<td><code>SingleReader</code>, <code>MultiReader</code>, <code>MultiSheetReader</code></td>
</tr>
<tr>
<td>@qpcr:assay</td>
<td>qpcr:assay</td>
<td>Irregular single- or multi-assay files.</td>
<td><code>SingleReader</code>, <code>MultiReader</code>, <code>MultiSheetReader</code></td>
</tr>
<tr>
<td>@qpcr:normaliser</td>
<td>qpcr:normaliser</td>
<td>Irregular single- or multi-assay files.</td>
<td><code>SingleReader</code>, <code>MultiReader</code>, <code>MultiSheetReader</code></td>
</tr>
<tr>
<td>@qpcr:group</td>
<td>qpcr:group</td>
<td>Horizontal Big Table files</td>
<td><code>BigTableReader</code></td>
</tr>
<tr>
<td>@qpcr</td>
<td>qpcr:column</td>
<td>Horizontal or vertical Big Table files</td>
<td><code>BigTableReader</code></td>
</tr>
</tbody>
</table>
<blockquote>
<h4 id="two-things-of-note">Two things of Note:</h4>
<ul>
<li>When specifying a decorator any non-decorated assay will be <em>ignored</em>!</li>
<li>If you are using <code>excel</code> you may have to add a single tick <code>'</code> in front of your decorators.</li>
</ul>
</blockquote>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This module contains classes designed to work with irregularly structured datafiles.
It provides `Parsers` that are able to extract the replicate identifiers and Ct values as pandas DataFrames
from irregular `csv` and `excel` files. 


## Working with &#34;irregular files&#34;
----

Any datafile that does not only consist of a replicate identifer and Ct column is called &#34;irregular&#34;. 
In fact, most excel sheets or csv exports from qPCR machines are actually irregular as they often contain some
information about the run, and melting curve data, and so forth. Such data is not relevant or of interest to `qpcr`, however, 
so we have to extract the data we are intersted in. This is the job of the `qpcr.Parsers`. They read in an irregular datafile and use 
a guided-parsing approach to find the relevant sections within the datafile. There are essentially two ways how they can do this, which are explained below. 

&gt; In fact, `qpcr.Parsers` are already implemented in the `qpcr.Readers` so you will often 
be able to read irregular datafiles directly with one of the `qpcr.Readers` and will not have to manually work with the `qpcr.Parsers`. 

### &#34;Finding&#34; relevant datasets through `assay_patterns`
The Parsers are quipped with a method called `_CORE_Parser.find_assays` which locates assays (or more formally &#34;datasets&#34;) within the datafile
using `regex`. Of course, in order to do that they need to know the patterns they are supposed to look for. Some patterns are already pre-specified
in the `qpcr.Parsers.assay_patterns` dictionary and can simply be specified using their key. If your own pattern is not yet pre-defined, 
[post an issue on github](https://github.com/NoahHenrikKleinschmidt/qpcr/issues) and supply some samples of how your assays usually appear in your datafiles 
alongside with the name of the machine that produces your datafiles.
Of course, you can also manually specify your own regex pattern. The only constraint is that is _must_ have *one* capturing group for the assay name.

&gt; Please note, all assay headers must be located either in the same column or the same row to be identified by a `Parser`!

Once the assays in your datafile are identified, the data columns belonging to them are searched for. The constraint here is that they must start either
in the row exactly below the assay headers or have at most one single row in between them. Anything else is no good! The data columns _must_ be labelled 
(i.e. have a header). By default `Name` and `Ct` are assumed as data column labels / headers, but this can be changed. 

If your datafiles contain multiple datasets / assays, the `qpcr.Parsers` will be able to extract all of them and store their extracted data. 


## Working with `multi-assay` files
----

While working with datafiles that contain multiple assays you will likely want to use *all* the assays from the datafile, here&#39;s how the `qpcr.Parsers` help you do this. 

### Getting all assays from a multi-assay file
Here it depends on what you want to do. Do you wish to extract the individual assays to make individual files from them or do you want to use them directly for an analysis?
Both options are possible. 

#### Making indivdual assay files from a multi-assay file
This is the core-business of the `qpcr.Parsers`. So you can simply set up a Parser, set a saving location using the 
Parser&#39;s `save_to` method and then `pipe` your file through. All done at this point. Of course, you can also work with the dataframes directly
using the Parser&#39;s `get` method. Like this you easily separate the assays which you can then pass to your main analysis as assays and normalisers. 

#### Using a multi-assay file directly for my analysis
So, you want to just feed in your one datafile and expect to get a table with your Delta-Delta-Ct values for all assays against all normalisers?!
Sure, no problem! Parsers will be able to do that, but you can more easily read a multi-assay file using the `qpcr.Readers.MultiReader` which allows you (after setup) to simply `pipe` through your datafile
and returns immediately a list of your assays-of-interest and normaliser-assays. How does it know which is which though? 
That&#39;s the super-important topic of the next paragraph.

#### Decorators
A `decorator` technically is a function that wraps another function when coding. Well, that&#39;s not quite the case for the `qpcr` decorators but the idea is similar. 
Instead of wrapping functions the `qpcr` decorators wrap assays in a multi-assay file. What does &#34;wrap&#34; mean? It means they provide meta-data about the assay
in question. What does that mean? There are multiple implemented `qpcr` decorators. For irregular multi-assay files the two important decorators are:
`@qpcr:assay` and `@qpcr:normaliser` â€“ is it now clear what they do? They are placed in the 
cell _exactly above_ the cell where the assay header is located (seriously, anything else won&#39;t do!) and tell the `qpcr.Parsers` (because all the `MultiReader` is doing is setting up some `Parsers`) 
if a specific assay is an &#34;assay-of-interest&#34; or a &#34;normaliser-assay&#34;.


So, let&#39;s recap this quickly: `qpcr.Parsers` can identify assays either through _de novo_ finding using `regex` patterns *or* through decorators. To tell a Parser to use a specific decorator 
for finding assays you can specify the `decorator` argument in `pipe` or `parse`. To specify a decorator like this you only write `qpcr:assay` `qpcr:normaliser` 
(the `key` is bascially the decorator but without the `@`).

If you work with `qpcr.Parsers` directly you can choose if you only wish to extract &#34;assays-of-interest&#34; (decorated as `@qpcr:assay`) *or* &#34;normalisers&#34;, or whatever other decorators are available.
However, this flexibility is not available when calling Parsers indirectly through one of the `qpcr.Readers`.

&gt; Please note, just like assay headers, all decorators must be located either in the same column or the same row to be identified by a `Parser`!


| Decorator | Code-reference | Filetype                                 | Available for / Used by `qpcr.Readers`                                   |
| --------- | -------------- | ---------------------------------------- | ------------------------------------------------------------ |
| @qpcr:all | qpcr:all       | Irregular single- or multi-assay files. | `SingleReader`, `MultiReader`, `MultiSheetReader` |
| @qpcr:assay | qpcr:assay       | Irregular single- or multi-assay files. | `SingleReader`, `MultiReader`, `MultiSheetReader`  |
| @qpcr:normaliser | qpcr:normaliser       | Irregular single- or multi-assay files. | `SingleReader`, `MultiReader`, `MultiSheetReader`  |
| @qpcr:group | qpcr:group | Horizontal Big Table files | `BigTableReader` |
| @qpcr | qpcr:column | Horizontal or vertical Big Table files | `BigTableReader` |



&gt; #### Two things of Note: 
&gt; - When specifying a decorator any non-decorated assay will be *ignored*!
&gt; - If you are using `excel` you may have to add a single tick `&#39;` in front of your decorators.
&#34;&#34;&#34;

import qpcr
import qpcr._auxiliary as aux
import qpcr._auxiliary.defaults as defaults
import qpcr._auxiliary.warnings as aw
import pandas as pd
import numpy as np
import re
from io import StringIO
from copy import deepcopy
import os

__pdoc__ = {
    &#34;_CORE_Parser&#34; : True
}

# this is the dictionary where we store pre-defined 
# patterns of headers above assays within the datafiles
# important here is that they must specify a capturing group for the assay name.

assay_patterns = {
                    &#34;all&#34;           : r&#34;([A-Za-z0-9.:, ()_\-/]+)&#34;,
                    &#34;Rotor-Gene&#34;    : r&#34;Quantitative analysis of .+(?&lt;=\()([A-Za-z0-9.:, _\-/]+)&#34;,
                }

decorators = {
                    &#34;qpcr:all&#34;          : &#34;(@qpcr:|&#39;@qpcr:)&#34;,
                    &#34;qpcr:assay&#34;        : &#34;(@qpcr:assay\s{0,}|&#39;@qpcr:assay\s{0,})&#34;,
                    &#34;qpcr:normaliser&#34;   : &#34;(@qpcr:normaliser\s{0,}|&#39;@qpcr:normaliser\s{0,})&#34;,     
                    &#34;qpcr:group&#34;        : &#34;(@qpcr:group\s{0,}|&#39;@qpcr:group\s{0,})&#34;,
                    &#34;qpcr:column&#34;       : &#34;(@qpcr|&#39;@qpcr)&#34;,

            }

plain_decorators = {
                    &#34;qpcr:all&#34;          : &#34;@qpcr:&#34;,
                    &#34;qpcr:assay&#34;        : &#34;@qpcr:assay&#34;,
                    &#34;qpcr:normaliser&#34;   : &#34;@qpcr:normaliser&#34;,
                    &#34;qpcr:group&#34;        : &#34;@qpcr:group&#34;,
                    &#34;qpcr:column&#34;       : &#34;@qpcr&#34;,
            }

# get the standard column headers to use for the 
# replicate id and Ct column of the finished dataframes
standard_id_header = defaults.raw_col_names[0]
standard_ct_header = defaults.raw_col_names[1]

default_group_name = defaults.default_group_name
default_dataset_header = defaults.default_dataset_header

# set a dummy default value for any np.nan values
# in the column storing the assay headers
# we do this in case the &#34;all&#34; assay_pattern is used without decorators
# in this case any cell would be selected as &#34;nan&#34; also matches the pattern
dummy_blank = &#34;$&#34;

class _CORE_Parser:
    &#34;&#34;&#34;
    This is the functional core for the irregular multi-assay file-reader classes.
    It handles the regex searching and numpy indexing of relevant column subsets of the datafiles.
    &#34;&#34;&#34;
    def __init__(self):
        self._src = None
        self._pattern = None
        self._data = None

        # the found assays, these will be arrays/lists that store the indices, 

        self._assay_indices = None                  # indices of the assay identifiers
        self._assay_names = None                    # names of the assays

        self._assay_names_start_indices = None      # indices of the rep. id headers
        self._assay_names_end_indices = None        # indices of the last entry of the rep. id columns

        self._assay_ct_start_indices = None         # indices of the ct headers
        self._assay_ct_end_indices = None           # indices of the last entry of the ct columns

        # a dictionary to store all assay dataframes
        self._dfs = {}

        # setup the labels for replicate ids and ct value column headers
        self.labels()

        # we must specify a maximum allowed length for the assay names before hand 
        # (since we&#39;re using numpy arrays for storing the names, which require enough open slots to store the characters)
        self._max_assay_name_length = 20
    
        # a folder into which the new assay-split datafiles should be stored
        self._save_loc = None

        # set transpose option in case datasets are stored not on separate row ranges but separate column ranges
        self._transpose = False


        # set up a BigTable data range
        self._bigtable_range = None

        # setup a warning for decorators without patterns warning that it will set to default
        # we do this here in case the MultiSheetReader calls read+parse here multiple times
        # but we don&#39;t need to know that it defaults every time.
        self._WARNING_decorators_but_no_patterns = aw.SoftWarning(&#34;Parser:decorators_but_no_pattern&#34;, once = True)

    def prune(self):
        &#34;&#34;&#34;
        Completely resets the Parser, clearing all data and preset-specifics such as the assay_pattern.
        &#34;&#34;&#34;
        self.__init__()

    def clear(self):
        &#34;&#34;&#34;
        Clears all datasets that were extracted.
        &#34;&#34;&#34;
        self._dfs = {}

        self._assay_indices = None                  # indices of the assay identifiers
        self._assay_names = None                    # names of the assays

        self._assay_names_start_indices = None      # indices of the rep. id headers
        self._assay_names_end_indices = None        # indices of the last entry of the rep. id columns

        self._assay_ct_start_indices = None         # indices of the ct headers
        self._assay_ct_end_indices = None           # indices of the last entry of the ct columns

    def transpose(self):
        &#34;&#34;&#34;
        Inverts the `col` index used by `qpcr.Parsers._CORE_Parser.find_assays` and `qpcr.Parsers._CORE_Parser.find_by_decorator`.
        By default the `col` refers to a column. After using `transpose` it will be interpreted as a `row`.
        Note
        ----
        This is method is dynamic, so repeated calling of `transpose` will keep reverting the interpretation from row to col, back to row, etc.
        &#34;&#34;&#34;
        self._transpose = not self._transpose

    def save_to(self, location : str = None):
        &#34;&#34;&#34;
        Sets the location into which the individual assay datafiles should be saved.
        Parameters
        ----------
        location : str
            The path to a directory where the newly generated assay datafiles shall be saved.
            If this directory does not yet exist, it will be automatically made.
        &#34;&#34;&#34;
        if location is not None: 
            self._save_loc = location
            if not os.path.exists(self._save_loc):
                os.mkdir(self._save_loc)
        return self._save_loc
    
    def get(self, assay : str = None):
        &#34;&#34;&#34;
        Parameters
        ----------
        assay : str
            The name of an assay found in the datafile. Available assays can be assessed using the `self.assays` method.
    
        Returns
        -------
        data : pd.DataFrame or dict
            Either a specific pandas dataframe of one of the assays (if an `assay` name was specified)
            or the entire dictionary of all found dataframes from all assays.
        &#34;&#34;&#34;
        if assay is not None:
            data = self._dfs[assay]
        else: 
            data = self._dfs
        return data

    def save(self):
        &#34;&#34;&#34;
        Saves the individual assays as separate csv files.
        This requires that a saving directory has been set using `self.save_to`.
        The files will simply be named according to the assay name (i.e. `ActinB.csv` for instance).
        &#34;&#34;&#34;
        if self._save_loc is None:
            aw.SoftWarning(&#34;Parser:no_save_loc&#34;)
        else:
            for assay, df in self._dfs.items():
                assay_path = os.path.join(self.save_to(), f&#34;{assay}.csv&#34;)
                df.to_csv(assay_path, index = False)

    def labels(self, id_label : str = &#34;Name&#34;, ct_label : str = &#34;Ct&#34;):
        &#34;&#34;&#34;
        Sets the headers for the relevant data columns for each assay within the datafile.

        Parameters
        ----------
        id_label : str
            The header above the column containing replicate identifiers. 
        
        ct_label : str
            The header above the column containing the replicates&#39; Ct values.
        &#34;&#34;&#34;
        self._id_label = id_label
        self._ct_label = ct_label

    def assays(self):
        &#34;&#34;&#34;
        Returns
        -------
        names : list
            The names of the found assays of the datafile
        &#34;&#34;&#34;        
        return list(self._dfs.keys())

    def assay_pattern(self, pattern : str = None, *flags):
        &#34;&#34;&#34;
        Sets up a regex pattern defining the assay declarations within the datafile.

        Parameters
        ----------
        pattern : str
            A string containing either the key to a predefined pattern from the `assay_patterns` dictionary, 
            or directly regex pattern. 
            If a regex pattern is directly provided, that pattern must contain a capturing group
            for the assay name that can be extracted.
        *flags 
            Any additional flags to pass to `re.compile()` for the regex pattern

        Returns
        -------
        pattern : re.Pattern
            The currently used regex pattern to identify assays within the datafile.
        &#34;&#34;&#34;
        if pattern is not None: 
            # try to get the pattern from the predefined patterns via key
            _pattern = aux.from_kwargs(pattern, None, assay_patterns)
            _pattern = pattern if _pattern is None else _pattern
            self._pattern = re.compile(_pattern, *flags)
        return self._pattern

    def max_assay_name_length(self, length = 20):
        &#34;&#34;&#34;
        Sets the maximum allowed name length (number of characters) assay names.
        
        Parameters
        ----------
        length : int
            The maximum number of characters to store for the assay name. 
            Default is `length = 20` characters.
        &#34;&#34;&#34;
        self._max_assay_name_length = length
    
    def parse(self, **kwargs):
        &#34;&#34;&#34;
        A wrapper for find_assays+find_columns+make_dataframes
        This is the functional core of the Parser&#39;s `pipe` method.

        Parameters
        -------
        **kwargs
            Any additional keyword argument that will be passed to any of the wrapped methods.
        &#34;&#34;&#34;
        decorator = aux.from_kwargs(&#34;decorator&#34;, None, kwargs, rm = True)
        if decorator is not None:
            self.find_by_decorator(decorator = decorator, **kwargs)
        else: 
            self.find_assays(**kwargs)
        
        # ignore if no assays were found (default is false, unless we use multi-assay multi-sheet files)
        ignore_empty = aux.from_kwargs(&#34;ignore_empty&#34;, False, kwargs)
        if ignore_empty:
            try: 
                self.find_columns()
                self.make_dataframes(**kwargs)
            except: 
                pass
        else: 
            self.find_columns()
            self.make_dataframes(**kwargs)

    def find_by_decorator(self, decorator : str, col = 0, **kwargs):
        &#34;&#34;&#34;
        Parses through a column of the datafile and finds all assays that are decorated with a specific decorator.
        Note that this requires that the decorator is in the cell above the assay header. Also, make sure to specify
        an `assay_pattern` to extract the assay name. If no `assay_pattern` is provided, it will simply take the entire cell content.
        
        Parameters
        -----------
        decorator : str
            One of the available `qpcr-decorator`&#39;s for irregular multi-assay files. 
            Available decorators can be assessed via the `qpcr.Parsers.decorators` dictionary keys.
        col : int
            The column in which to look for assay identifiers. 
            By default the first column `col = 0`.
        
        Returns
        -------
        assay_indices : np.ndarray
            The indices (row, col) of all assays found.
        names : np.ndarray
            The extracted names of all assays found.
        &#34;&#34;&#34;
        # ignore if no assays were found (default is false, unless we use multi-assay multi-sheet files)
        ignore_empty = aux.from_kwargs(&#34;ignore_empty&#34;, False, kwargs)
        # get the pattern required (or raise error if invalid decorators are provided)
        if decorator not in decorators.keys():
            aw.HardWarning(&#34;Parser:invalid_decorator&#34;, d = decorator, all_d = list(decorators.keys()))

        decorator_pattern = re.compile( decorators[decorator] )
        decorator_indices, decorator_names = self.find_assays(col = col, pattern = decorator_pattern, **kwargs )

        # check if decorators were identified
        found_indices = decorator_indices.size &gt; 0
        if not found_indices:
            if not ignore_empty: # if none were found either raise error or ignore
                aw.HardWarning(&#34;Parser:no_decorators_found&#34;)
            else: 
                return

        # if no assay_pattern was specified then default to generic &#34;all&#34; to get full cell contents
        if self.assay_pattern() is None:
            self._WARNING_decorators_but_no_patterns.trigger()
            self.assay_pattern(&#34;all&#34;)

        assay_indices = decorator_indices
        # get assay indices as the cells IMMEDIATELY BELOW the decorators
        # we adjust either col or the row indices depending on the transposition
        if self._transpose:
            col = col + 1
        else:
            assay_indices = assay_indices + 1
      
        # get all assay header cells into an array to extract their names
        array = self._prep_header_array(col, assay_indices)

        # adjust avaliable length of stored assay names
        max_length = max(
                            list(  map(len, array)  )
                        )
        self.max_assay_name_length(max_length)

        names = np.array([&#34;-&#34;*self._max_assay_name_length for _ in range(len(array))]) # we need to pre-specify the max allowed length for the assay names by filling an array with some dummy placeholders (&#39;-&#39;)
        idx = 0
        for entry in array:
            # try:
            match = self.assay_pattern().search(entry)
            if match is not None: 
                name = match.group(1)
                names[idx] = name
            # except: 
            #     continue
            idx += 1

        self._assay_indices = assay_indices
        self._assay_names = names
        
        return assay_indices, names

    def find_assays(self, col = 0, **kwargs):
        &#34;&#34;&#34;
        Parses through a column of the datafile and identifies all indices of cells that match the provided `assay_pattern``.
        It stores these values internally and also returns the results as numpy arrays.

        Parameters
        -----------
        col : int
            The column in which to look for assay identifiers. 
            By default the first column `col = 0`.

        Returns
        -------
        indices : np.ndarray
            The indices (row, col) of all assays found.
        names : np.ndarray
            The extracted names of all assays found.
        &#34;&#34;&#34;
        
        custom_pattern = aux.from_kwargs(&#34;pattern&#34;, None, kwargs)
        if self._pattern is None and custom_pattern is None: 
            aw.HardWarning(&#34;Parser:no_pattern_yet&#34;)

        pattern_to_use = self._pattern if custom_pattern is None else custom_pattern

        array = self._prep_header_array(col = col)

        indices = np.zeros(len(array))
        names = np.array([&#34;-&#34;*self._max_assay_name_length for _ in range(len(array))]) # we need to pre-specify the max allowed length for the assay names by filling an array with some dummy placeholders (&#39;-&#39;)
        idx = 0
        for entry in array:
            # try: 
            match = pattern_to_use.search(entry)
            if match is not None: 
                name = match.group(1)
                names[idx] = name
                indices[idx] = 1
            # except: 
            #     continue
            idx += 1
        indices = np.argwhere(indices == 1)

        if indices.size == 0:
            # ignore if no assays were found (default is false, 
            # unless we use multi-assay multi-sheet files)
            ignore_empty = aux.from_kwargs(&#34;ignore_empty&#34;, False, kwargs)
            if not ignore_empty:
                aw.HardWarning(&#34;Parser:no_assays_found&#34;, traceback = False)

        names = names[indices]
        names = names.reshape(len(names))

        self._assay_indices = indices
        self._assay_names = names
        
        return indices, names
    
    def find_columns(self):
        &#34;&#34;&#34;
        Identifies the relevant data column belonging to each assay within the datafile.
        &#34;&#34;&#34;
        # search indices of the starts of id and ct columns
        # these are now the row, col coordinates of each name_column header
        name_col_starts = self._find_column_starts(
                                                    label = self._id_label, 
                                                    ref_indices = self._assay_indices
                                                )
        # these are now the row, col coordinates of each ct_column header
        ct_col_starts = self._find_column_starts(
                                                    label = self._ct_label, 
                                                    ref_indices = self._assay_indices
                                                )

        # now we need to generate know also the end indices of the datacolumns
        name_col_ends = self._find_column_ends(name_col_starts)
        
        # now that we know the end indices for the replicate id column we will adopt the end row indices
        # onto the ct column as well (we don&#39;t parse through the Ct column because it might have missing 
        # Ct values intersperced which would prematurely terminate the parsing...)

        # (1) we transpose to have all row indices easily accessible in the first line
        # (2) we adopt row indices from the transposed name col
        # (3) and transpose back to get our final ct end indices
        ct_col_ends = deepcopy( np.transpose(ct_col_starts) )
        name_col_ends_t = np.transpose(name_col_ends)
        ct_col_ends[0] = name_col_ends_t[0]
        ct_col_ends = np.transpose(ct_col_ends)

        # now store the data
        self._assay_names_start_indices = name_col_starts
        self._assay_names_end_indices = name_col_ends
        self._assay_ct_start_indices = ct_col_starts
        self._assay_ct_end_indices = ct_col_ends

    def make_dataframes(self, allow_nan_ct : bool = True, default_to : float = None, **kwargs):
        &#34;&#34;&#34;
        Generates a set of `pandas DataFrame`s each containing two columns 
        (one for the replicate identifiers, one for the Ct values)
        for subsequent use with the main `qpcr` module.

        Parameters
        ------
        allow_nan_Ct : bool
            Allows Ct values to be NaN within the final dataframe (if `True`, default).
            If no NaN Ct values should be maintained a default value for NaN Ct values must be specified
            using `default_to`.
        default_to : float
            The default value to replace NaN Ct values with. 
            This is ignored if `allow_nan_ct = True`.
        &#34;&#34;&#34;  

        adx = 0
        # print(self._assay_names, self._assay_indices, self._assay_names_start_indices, self._assay_names_end_indices)
        for assay in self._assay_names:
            
            # get the assay&#39;s indices of both replicate id and ct columns
            names_start = self._assay_names_start_indices[adx]
            names_end = self._assay_names_end_indices[adx]
            ct_start = self._assay_ct_start_indices[adx]
            ct_end = self._assay_ct_end_indices[adx]

            # generate the final index slices from the total array
            # of both replicate id (names) and ct columns
            names_range, names_col = self._make_index_range(names_start, names_end, crop_first = True)
            ct_range, ct_col = self._make_index_range(ct_start, ct_end, crop_first = True)

            # get the assay data
            assay_names = self._data[names_range, names_col]
            assay_cts = self._data[ct_range, ct_col]

            # and convert to numeric data
            try: 
                assay_cts = assay_cts.astype(float)
            except ValueError as e:
                assay_cts = np.genfromtxt(  np.array(assay_cts, dtype=str)  )
                bad_value = e.__str__().split(&#34;: &#34;)[1]
                aw.SoftWarning(&#34;Parser:found_non_readable_cts&#34;, assay = assay, bad_value = bad_value)

            # assemble the assay dataframe 
            assay_df = pd.DataFrame(
                                    {
                                        standard_id_header : assay_names, 
                                        standard_ct_header : assay_cts,
                                    }
                                )

            if not allow_nan_ct:
                if not isinstance(default_to, (int, float)): 
                    aw.HardWarning(&#34;Parser:no_ct_nan_default&#34;, d = default_to)
                
                # apply defaulting lambda function
                assay_df[ standard_ct_header ] = assay_df[ standard_ct_header ].apply(
                                                                                        lambda x: x if x == x else default_to
                                                                                    )
            # and store dataframe
            self._dfs.update(
                                { assay : assay_df }
                            )

            adx += 1

    def _make_BigTable_range(self, **kwargs):
        &#34;&#34;&#34;
        Generates a pandas DataFrame of a subsection of an irregular datafile
        containing a &#34;big data table&#34; with multiple assays specified in it. 

        It makes use of the `id_label` specified using `_CORE_Parser.labels` as the
        anchor. The resulting dataframe fill contain all rows from the cell where `id_label``
        as located until the data is empty. 

        If additionally `replicates` are specified in the `kwargs` 
        the starting positions of assay replicates are inferred based on `decorators`. 
        Note, this only works for `horizontal` Big Tables!
        &#34;&#34;&#34;
        is_horizontal = aux.from_kwargs(&#34;is_horizontal&#34;, False, kwargs)
        
        # get the main data
        data = self._data.astype(&#34;str&#34;)
        ref_col_header = self._id_label   
        
        # find big table starting row
        idx = np.argwhere(data == ref_col_header)

        # vet that we actually found the big table
        if idx.size == 0:
            aw.HardWarning(&#34;Parser:no_bigtable_header&#34;, header = ref_col_header)
        idx = idx.reshape(idx.size)
        start, col = idx

        idx = 1
        while True:
            try: 
                entry = data[start+idx, col]
            except: 
                break
            if entry == &#34;nan&#34;:
                break
            idx += 1

        end = start + idx

        # put a -1 offset on the rows if horziontal, as the decorators 
        # are in the row above the actual column headers.
        if is_horizontal:
            start -= 1

        # generate bigtable data range and store
        relevant_data = data[start : end, : ]
        self._bigtable_range = relevant_data  


    def _infer_BigTable_groups(self, **kwargs):
        &#34;&#34;&#34;
        Gets the group ranges from the bigtable datarange.
        Note, this is only used in case of horizontal big tables.
        &#34;&#34;&#34;
        # get the relevant data
        array = self._bigtable_range
        maxrows, allcols = array.shape

        ignore_empty = aux.from_kwargs(&#34;ignore_empty&#34;,False,kwargs)

        # get and vet replicates
        replicates = aux.from_kwargs(&#34;replicates&#34;, None, kwargs, rm = True)
        if replicates is None: 
            aw.HardWarning(&#34;Parser:bigtable_no_replicates&#34;, traceback = False)
        replicates, names = self._vet_replicates(ignore_empty, replicates, array, **kwargs)

        
        rdx = 0 # counter for the replicate groups
        
        data_array = None # this array will store the entire transposed data

        decorator = plain_decorators[&#34;qpcr:group&#34;]

        # now get the assays in question
        # we already vetted if the file is properly 
        # decorated during _vet_replicates
            
        # find decorated starting columns
        # get only column indices            
        indices = np.argwhere(array == decorator)
        indices = indices[ : , 1 ]

        if indices.size == 1:
            indices = [indices]

        # get total slice of bigtable rows
        rows = slice( 1, maxrows )
        
        # iterate over each group
        for col in indices: 
            
            rep = replicates[rdx]

            # get data columns
            cols = slice( col, col + rep )
            
            # get data
            data = array[  rows, cols  ]
            
            # rename data cols if names are provided
            if names is not None: 
                name = names[rdx]
                data[ 0 , : ] = name
            
            # concatenate data into a single array
            if data_array is None: 
                data_array = data
            else:
                data_array = np.concatenate(
                                                ( data_array, data ),
                                                axis = 1,
                                            )
            rdx += 1

        # remove groups from the data array
        groups = data_array[ 0, : ]
        data_array = data_array[ 1:, : ]

        # Actually, right here, instead of having to infer our own replicate 
        # names thare are then just group0 group0 group0 group1 ...
        # We can simply use the sample repeat / tile approach we used to make the
        # group1 etc. replicate names, based on the ACTUAL groups (like the ones we 
        # have just split off from the data -&gt; their first row are already the replicate
        # identifiers we just use those directly... )

        # reshape data into a single column
        data_array = np.concatenate(data_array, axis = 0)

        # now repeat the groups to match the stacked new data column
        groups_tiled = np.tile(groups, data_array.size // groups.size )

        # now get the dataset id column
        id_col = self._id_label
        id_start = np.where(array == id_col)
        id_row, id_col = id_start
        id_rows = slice( int(id_row + 1), maxrows )
        id_col = array[  id_rows, id_col  ]
        
        # repeat dataset ids to match stacked new data column
        ids_tiled = np.repeat(  id_col , groups.size  )

        # assemble all data
        total_data = [ids_tiled, groups_tiled, data_array]
        headers = [ default_dataset_header, standard_id_header, standard_ct_header ]  

        # check for qpcr column and if present, get and adjust shape
        self._BigTable_horizontal_qpcr_col(array, maxrows, groups, total_data, headers)


        # combine the three columns (dataset id, groups, and Ct (actual data_array))
        data_array = np.stack( total_data, axis = 1 )

        # add default names into the first row
        data_array = np.concatenate(
                                        ( 
                                            [headers],
                                            data_array
                                       ),   axis = 0
                                )
        
        # actually return the finished array
        return data_array

    def _BigTable_horizontal_qpcr_col(self, array, maxrows, groups, total_data, headers):
        &#34;&#34;&#34;
        Checks if a &#34;@qpcr&#34; column is present in the data and if so, adjusts its shape and 
        adds it to the data to be assembled for the assays.
        &#34;&#34;&#34;
        column_decorator = plain_decorators[&#34;qpcr:column&#34;]
        qpcr_col = np.where(array == column_decorator)
        qpcr_row, qpcr_col = qpcr_col
        if len(qpcr_col) != 0:
            
            qpcr_rows = slice( int(qpcr_row + 1), maxrows )
            qpcr_col = array[  qpcr_rows, qpcr_col  ]

            qpcr_tiled = np.repeat(  qpcr_col , groups.size  )
            total_data.append(qpcr_tiled)
            headers.append(&#34;@qpcr&#34;)

    def _vet_replicates(self, ignore_empty, replicates, array, **kwargs):
        &#34;&#34;&#34;
        Checks if provided replicates cover all data groups (annoated columns).
        And it also gets the names supposed to be used for the columns.
        &#34;&#34;&#34;
        # get assays for each decorator
        groups = 0
        decorator = plain_decorators[&#34;qpcr:group&#34;]
    
        # find decorated starting columns
        indices = np.argwhere(array == decorator)
        if indices.size == 0 and not ignore_empty:
            aw.HardWarning(&#34;Parser:no_decorators_found&#34;, traceback = False)

        # get only column indices            
        indices = indices[ : , 1 ]
        groups += indices.size

        # check if replicates are an integer, if so transform to 
        # tuple that cover all found assays
        if aux.same_type(replicates, 1): 
            replicates = np.tile([replicates], groups)
        # else, check it it&#39;s a formula that needs to be read out to a tuple.
        elif aux.same_type(replicates, &#34;&#34;):
            replicates = qpcr.Assay()._reps_from_formula(replicates)
        
        # if replicates are already a tuple, make sure they cover all rows
        elif aux.same_type(replicates, ()):
            all_covered = groups == len(replicates)
            if not all_covered:
                aw.HardWarning(&#34;Assay:reps_dont_cover&#34;, n_samples = groups, reps = replicates, traceback = False)
            
        # get names for assays
        group_names = aux.from_kwargs(&#34;names&#34;, None, kwargs)

        # vet that names cover
        if group_names is not None and len(group_names) != len(replicates):
            aw.HardWarning(&#34;Assay:groupnames_dont_colver&#34;, traceback = False, current_groups = f&#34;None, but needs to be {len(replicates)} names.&#34;, new_received = group_names)
        
        # return tranformed replicates
        return replicates, group_names

        

    def _prep_header_array(self, col = None, row = None):
        &#34;&#34;&#34;
        Generates the array in which header entries should be searched for
        &#34;&#34;&#34;
        
        if row is None and col is not None: 
            array = self._data[:, col] if not self._transpose else self._data[col, :]
        elif row is not None and col is None:
            array = self._data[row, :] if not self._transpose else self._data[:, row]
        elif row is not None and col is not None:
            array = self._data[row, col] if not self._transpose else self._data[col, row]
        else:
            aw.HardWarning(&#34;Parser:invalid_range&#34;)

        # re-format to str and reset &#34;nan&#34; to dummy_blank
        array = array.astype(str)
        array[ np.argwhere(array == &#34;nan&#34;) ] = dummy_blank
        array = array.reshape(array.size)
        return array


    def _make_index_range(self, start_indices, end_indices, crop_first = True):
        &#34;&#34;&#34;
        Generates an index range for a data column based on start and stop indices.
        This assumes that the column entry (i.e. entry[1]) is always the same and only the rows are different.
        
        Parameters
        ----------
        start_indices : np.ndarray
            Row, col indices of the header of the data column
        end_indices : np.ndarray
            Row, col indices of the last entry of the data column
        crop_first : bool
            If set to True it will offset the start row indices by +1 to exclude the header.
        &#34;&#34;&#34;

        start = start_indices[0] + 1 if crop_first else start_indices[0]
        end = end_indices[0]

        row_range = slice(  start, end  )
        col = start_indices[1]
        return row_range, col

    def _find_column_starts(self, label, ref_indices):
        &#34;&#34;&#34;
        This function uses the assays&#39; found reference row indices to 
        now search for the coordinates of the labeled cell so we know where a data column starts
        &#34;&#34;&#34;

        # get index to match to ref_indices
        idx_to_match = 0 if not self._transpose else 1

        data = self._data
        all_found = np.argwhere(data == label)
        row_indices = np.transpose(all_found)[ idx_to_match ]

    
        # adjust coordinates +1 as the headers would be in the row below the assay declaration
        # we only have to do this if we use the default setting of assays in the same column
        if not self._transpose:
            ref_indices = ref_indices + 1

        # ref_indices = ref_indices + 1 
        matching_rows = np.where(np.isin(row_indices, ref_indices))
                
        # if no matches were found, try incrementing the index offset once more 
        # (we&#39;ll allow for a single row between the header and the start of the data)
        no_matches = len(matching_rows) == 1 and matching_rows[0].size == 0
        if no_matches:
            ref_indices = ref_indices + 1
            matching_rows = np.where(np.isin(row_indices, ref_indices))

        # check again, and raise Error if still no matches are found
        no_matches = len(matching_rows) == 1 and matching_rows[0].size == 0
        if no_matches:
            aw.HardWarning(&#34;Parser:no_data_found&#34;, label = label)

        matching_rows = all_found[matching_rows]
        return matching_rows

    def _find_column_ends(self, indices):
        &#34;&#34;&#34;
        Determines the end index of a column within the datafile based on the starting indices of
        its header label
        &#34;&#34;&#34;
        data = self._data
        end_indices = np.zeros(indices.shape, dtype = int)
        adx = 0
        for i in indices:
            row, col = i
            idx = 0
            value = 0
            while True:
                try: value = data[row + idx, col]
                except: break
                if value != value: 
                    break
                idx += 1
            finals = np.array([row + idx, col], dtype = int)
            end_indices[adx] += finals
            adx += 1
        return end_indices


class CsvParser(_CORE_Parser):
    &#34;&#34;&#34;
    Handles reading and parsing irregular `csv` files that contain multiple assays.
    It extracts datasets either through regex pattern matching or/and through provided
    decorators within the datafile.
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()

    def pipe(self, filename :str, **kwargs):
        &#34;&#34;&#34;
        A wrapper for read+parse

        Note 
        ----
        This is the suggested use of `CsvParser`. 
        If a directory has been specified into which the datafiles shall be saved, 
        then saving will automatically be done.

        Parameters
        -------
        filename : str
            A filepath to an input csv file.
        **kwargs
            Any additional keyword argument that will be passed to any of the wrapped methods.
        Returns
        -------
        assays : dict
            A dictionary of all the extracted assays from the datafile storing the data as pandas DataFrames.
            Individual assays can also be accessed using the `get` method.
        &#34;&#34;&#34;
        try: 
            self.read(filename, **kwargs)
        except: 
            self.read(filename)
            aw.SoftWarning(&#34;Parser:incompatible_read_kwargs&#34;, func = &#34;pandas.read_csv&#34;)
        
        self.parse(**kwargs)
        assays = self.get()
        
        if self._save_loc is not None: 
            self.save()
        
        return assays

    def read(self, filename : str, **kwargs):
        &#34;&#34;&#34;
        Reads an input csv file. 

        Parameters
        -------
        filename : str
            A filepath to an input csv file.
        **kwargs 
            Any additional keyword arguments to be passed to pandas&#39; `read_csv` function.
        &#34;&#34;&#34;
        self._src = filename

        contents = self._prepare_commas()
        contents = StringIO(contents) # convert to StringIO for pandas to be able to read
        
        delimiter = &#34;;&#34; if self._is_csv2() else &#34;,&#34;
        delimiter = aux.from_kwargs(&#34;sep&#34;, delimiter, kwargs, rm = True)

        # now read the data and convert to numpy array
        try: 
            df = pd.read_csv(contents, header = None, sep = delimiter, **kwargs)
        except: 
            aw.SoftWarning(&#34;Parser:incompatible_read_kwargs&#34;, func = &#34;pandas.read_csv()&#34;)
            df = pd.read_csv(contents, header = None, sep = delimiter)

        df = df.dropna(axis = 0, how = &#34;all&#34;).reset_index(drop=True)
        data = df.to_numpy()

        self._data = data

    def _is_csv2(self):
        &#34;&#34;&#34;
        Tests if csv file is ; delimited (True) or common , (False)
        &#34;&#34;&#34;
        with open(self._src, &#34;r&#34;) as openfile: 
            content = openfile.read()
        if &#34;;&#34; in content: 
            return True
        return False

    def _prepare_commas(self):
        &#34;&#34;&#34;
        This function reads the datafile and adjusts the number of commas 
        within each line to ensure equal commas in the entire file.

        Note
        -------
        Although the method uses the term &#34;commas&#34; it also works with semicolons for csv2

        Returns
        -------
        new_content : str
            A string containing the entire file contents with adjusted commas.
        &#34;&#34;&#34;

        delimiter = &#34;;&#34; if self._is_csv2() else &#34;,&#34;

        # check if quotes are in datafile and adjust comma-patterns to use
        empty_comma_filler = f&#39;{delimiter}&#34;&#34;&#39; if self._has_quotes() else f&#34;{delimiter}&#34;
        comma_sep = f&#39;&#34;{delimiter}&#34;&#39; if self._has_quotes() else f&#34;{delimiter}&#34;
        comma_sep = re.compile(comma_sep)

        with open(self._src, &#34;r&#34;) as f:
            content = f.read()
            lines = content.split(&#34;\n&#34;)
            comma_counts = [len(comma_sep.findall(i)) for i in lines]
            max_commas = max(comma_counts)
            lines = [i + (max_commas - j) * empty_comma_filler for i, j in zip(lines, comma_counts)]
        new_content = &#34;\n&#34;.join(lines)
        return new_content

    def _has_quotes(self):
        &#34;&#34;&#34;
        Checks if cells from the csv input file have quotes around them.
        Essentially it checks if there are any &#34;,&#34; patterns in the file.
        &#34;&#34;&#34;
        delimiter = &#34;;&#34; if self._is_csv2() else &#34;,&#34;
        with open(self._src, &#34;r&#34;) as f:
            content = f.read()
        has_quotes = f&#39;&#34;{delimiter}&#34;&#39; in content
        return has_quotes

class ExcelParser(_CORE_Parser):
    &#34;&#34;&#34;
    Handles reading and parsing `excel` files that may contain multiple assays.
    It extracts datasets either through regex pattern matching or/and through provided
    decorators within the datafile.
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()

    def read(self, filename : str, sheet_name : (str or int) = 0, **kwargs):
        &#34;&#34;&#34;
        Reads an input excel file. 

        Parameters
        -------
        filename : str
            A filepath to an input excel file.
        sheet_name : int or str
            The name of a specific spreadsheet of the file to read.
            If none is provided by default the first sheet will be read.
            Only one single sheet can be read at a time. 
            If an `integer` is provided the sheets will be accessed by their order, otherwise by their name (if a `string` is provided).
        **kwargs
            Any additional keyword arguments to be passed to pandas `read_excel` function.
        &#34;&#34;&#34;
        self._src = filename

        # read data and convert to numpy array
        try: 
            data = pd.read_excel(self._src, sheet_name = sheet_name, header = None, **kwargs)
        except: 
            data = pd.read_excel(self._src, sheet_name = sheet_name, header = None)
            aw.SoftWarning(&#34;Parser:incompatible_read_kwargs&#34;, func = &#34;pandas.read_excel()&#34;)

        data = data.to_numpy()

        self._data = data

    def pipe(self, filename :str, **kwargs):
        &#34;&#34;&#34;
        A wrapper for read+parse

        Note 
        ----
        This is the suggested use of `ExcelParser`. 
        If a directory has been specified into which the datafiles shall be saved, 
        then saving will automatically be done.

        Parameters
        -------
        filename : str
            A filepath to an input excel file.
        **kwargs
            Any additional keyword argument that will be passed to any of the wrapped methods.
        Returns
        -------
        assays : dict
            A dictionary of all the extracted assays from the datafile storing the data as pandas DataFrames.
            Individual assays can also be accessed using the `get` method.
        &#34;&#34;&#34;
        try: 
            self.read(filename, **kwargs)
        except: 
            self.read(filename)
            aw.SoftWarning(&#34;Parser:incompatible_read_kwargs&#34;, func = &#34;pandas.read_excel&#34;)

        self.parse(**kwargs)
        assays = self.get()

        if self._save_loc is not None: 
            self.save()

        return assays

if __name__ == &#34;__main__&#34;:
    
    # parser = CsvParser()
    # parser.assay_pattern(&#34;Rotor-Gene&#34;)
    # parser.save_to(&#34;__csvparser&#34;)
    # mycsv = &#34;./__parser_data/Brilliant III Ultra Fast SYBR Green 2019-01-07 (1).csv&#34;
    # parser.pipe(mycsv)

    # print(&#34;&#34;&#34;\n\n\n ========================= \n All good with CsvParser \n ========================= \n\n\n&#34;&#34;&#34;)

    # parser2 = ExcelParser()
    # parser2.assay_pattern(&#34;Rotor-Gene&#34;)
    # parser2.save_to(&#34;./__excelparser&#34;)
    # myexcel = &#34;./__parser_data/excel 3.9.19.xlsx&#34;
    # parser2.pipe(myexcel, sheet_name = 1)

    # print(&#34;&#34;&#34;\n\n\n ========================= \n All good with ExcelParser \n ========================= \n\n\n&#34;&#34;&#34;)

    # parser3 = ExcelParser()
    # decorated_excel = &#34;./__parser_data/excel 3.9.19_decorated.xlsx&#34;
    # parser3.save_to(&#34;./__decorated_excelparser&#34;)
    # parser3.read(decorated_excel)
    # parser3.assay_pattern(&#34;Rotor-Gene&#34;)
    # parser3.find_by_decorator(decorator = &#34;qpcr:all&#34;)
    # parser3.find_columns()
    # parser3.make_dataframes()
    # parser3.save()
    # # print(parser3.get())

    # print(&#34;&#34;&#34;\n\n\n ========================= \n All good with decorated ExcelParser \n ========================= \n\n\n&#34;&#34;&#34;)

    # parser4 = CsvParser()
    # decorated_csv = &#34;./__parser_data/Brilliant III Ultra Fast SYBR Green 2019-01-07 (1)_decorated.csv&#34;
    # parser4.save_to(&#34;./__decorated_csvparser&#34;)
    # parser4.read(decorated_csv)
    # parser4.assay_pattern(&#34;Rotor-Gene&#34;)
    # parser4.find_by_decorator(decorator = &#34;qpcr:all&#34;)
    # parser4.find_columns()
    # parser4.make_dataframes()
    # parser4.save()
    # # print(parser4.get())

    # print(&#34;&#34;&#34;\n\n\n ========================= \n All good with decorated CsvParser \n ========================= \n\n\n&#34;&#34;&#34;)


    # parser4 = CsvParser()
    # decorated_csv = &#34;./__parser_data/Brilliant III Ultra Fast SYBR Green 2019-01-07 (1)_decorated.csv&#34;
    # parser4.save_to(&#34;./__decorated_csvparser_pipe&#34;)
    # parser4.assay_pattern(&#34;Rotor-Gene&#34;)
    # # parser4.pipe(decorated_csv, decorator = &#34;qpcr:assay&#34;)
    # # print(parser4.get())

    # parser4.pipe(&#34;./__parser_data/manual_decorated.csv&#34;)

    # print(&#34;&#34;&#34;\n\n\n ========================= \n All good with decorated CsvParser using pipe \n ========================= \n\n\n&#34;&#34;&#34;)

    # parser3 = ExcelParser()
    # decorated_excel = &#34;./__parser_data/excel 3.9.19_decorated.xlsx&#34;
    # parser3.save_to(&#34;./__decorated_excelparser_pipe_nodec&#34;)
    # parser3.assay_pattern(&#34;Rotor-Gene&#34;)
    # parser3.pipe(decorated_excel)
    # # print(parser3.get())

    # print(&#34;&#34;&#34;\n\n\n ========================= \n All good with decorated ExcelParser using pipe without dec\n ========================= \n\n\n&#34;&#34;&#34;)


    # same_row_assays = &#34;/Users/NoahHK/Downloads/qPCR cytokines upon treatment_decorated.xlsx&#34;

    # parser5 = ExcelParser()
    # parser5.transpose()
    # parser5.read(same_row_assays, sheet_name = 1)
    # parser5.save_to(&#34;./__transposed_parser&#34;)
    # parser5.assay_pattern(&#34;all&#34;)
    # parser5.labels(  id_label = &#34;Sample Name&#34;, ct_label = &#34;CT&#34;  )

    # print(&#34;&#34;&#34;\n\n\n ========================= \n Transposed excel (FIND)\n ========================= \n\n\n&#34;&#34;&#34;)

    # parser5.find_assays(col = 1)
    # parser5.find_columns()
    # parser5.make_dataframes()
    # r = parser5.get()
    # print(r)

    # # assert parser5._assay_indices is not None, &#34;(find_assays) No assay_indices could be found!!!&#34;
    # parser5.clear()

    # print(&#34;&#34;&#34;\n\n\n ========================= \n Transposed excel (DECO)\n ========================= \n\n\n&#34;&#34;&#34;)


    # parser5.find_by_decorator(&#34;qpcr:all&#34;)
    # # assert parser5._assay_indices is not None, &#34;(find_by_decorator) No assay_indices could be found!!!&#34;
    # parser5.find_columns()
    # parser5.make_dataframes()
    # r = parser5.get()
    
    # parser5.clear()

    # parser5.read(same_row_assays, sheet_name = 1)
    # parser5.parse(decorator = &#34;qpcr:all&#34;)
    # r = parser5.get()
    # print(r)
    # parser5.save()

    bigtable_horiztonal = &#34;/Users/NoahHK/Downloads/Local_cohort_Adenoma_qPCR_rawdata_decorated.xlsx&#34;

    parser_bigtable = ExcelParser()
    parser_bigtable.read(bigtable_horiztonal)

    parser_bigtable.labels( id_label = &#34;tissue_number&#34; )
    parser_bigtable._make_BigTable_range(is_horizontal = True)
    r = parser_bigtable._infer_BigTable_groups( replicates = &#34;3,4&#34;, names = [&#34;GAPDH&#34;, &#34;SORD1&#34;] )
    print(r)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="qpcr.Parsers.Parsers.CsvParser"><code class="flex name class">
<span>class <span class="ident">CsvParser</span></span>
</code></dt>
<dd>
<div class="desc"><p>Handles reading and parsing irregular <code>csv</code> files that contain multiple assays.
It extracts datasets either through regex pattern matching or/and through provided
decorators within the datafile.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CsvParser(_CORE_Parser):
    &#34;&#34;&#34;
    Handles reading and parsing irregular `csv` files that contain multiple assays.
    It extracts datasets either through regex pattern matching or/and through provided
    decorators within the datafile.
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()

    def pipe(self, filename :str, **kwargs):
        &#34;&#34;&#34;
        A wrapper for read+parse

        Note 
        ----
        This is the suggested use of `CsvParser`. 
        If a directory has been specified into which the datafiles shall be saved, 
        then saving will automatically be done.

        Parameters
        -------
        filename : str
            A filepath to an input csv file.
        **kwargs
            Any additional keyword argument that will be passed to any of the wrapped methods.
        Returns
        -------
        assays : dict
            A dictionary of all the extracted assays from the datafile storing the data as pandas DataFrames.
            Individual assays can also be accessed using the `get` method.
        &#34;&#34;&#34;
        try: 
            self.read(filename, **kwargs)
        except: 
            self.read(filename)
            aw.SoftWarning(&#34;Parser:incompatible_read_kwargs&#34;, func = &#34;pandas.read_csv&#34;)
        
        self.parse(**kwargs)
        assays = self.get()
        
        if self._save_loc is not None: 
            self.save()
        
        return assays

    def read(self, filename : str, **kwargs):
        &#34;&#34;&#34;
        Reads an input csv file. 

        Parameters
        -------
        filename : str
            A filepath to an input csv file.
        **kwargs 
            Any additional keyword arguments to be passed to pandas&#39; `read_csv` function.
        &#34;&#34;&#34;
        self._src = filename

        contents = self._prepare_commas()
        contents = StringIO(contents) # convert to StringIO for pandas to be able to read
        
        delimiter = &#34;;&#34; if self._is_csv2() else &#34;,&#34;
        delimiter = aux.from_kwargs(&#34;sep&#34;, delimiter, kwargs, rm = True)

        # now read the data and convert to numpy array
        try: 
            df = pd.read_csv(contents, header = None, sep = delimiter, **kwargs)
        except: 
            aw.SoftWarning(&#34;Parser:incompatible_read_kwargs&#34;, func = &#34;pandas.read_csv()&#34;)
            df = pd.read_csv(contents, header = None, sep = delimiter)

        df = df.dropna(axis = 0, how = &#34;all&#34;).reset_index(drop=True)
        data = df.to_numpy()

        self._data = data

    def _is_csv2(self):
        &#34;&#34;&#34;
        Tests if csv file is ; delimited (True) or common , (False)
        &#34;&#34;&#34;
        with open(self._src, &#34;r&#34;) as openfile: 
            content = openfile.read()
        if &#34;;&#34; in content: 
            return True
        return False

    def _prepare_commas(self):
        &#34;&#34;&#34;
        This function reads the datafile and adjusts the number of commas 
        within each line to ensure equal commas in the entire file.

        Note
        -------
        Although the method uses the term &#34;commas&#34; it also works with semicolons for csv2

        Returns
        -------
        new_content : str
            A string containing the entire file contents with adjusted commas.
        &#34;&#34;&#34;

        delimiter = &#34;;&#34; if self._is_csv2() else &#34;,&#34;

        # check if quotes are in datafile and adjust comma-patterns to use
        empty_comma_filler = f&#39;{delimiter}&#34;&#34;&#39; if self._has_quotes() else f&#34;{delimiter}&#34;
        comma_sep = f&#39;&#34;{delimiter}&#34;&#39; if self._has_quotes() else f&#34;{delimiter}&#34;
        comma_sep = re.compile(comma_sep)

        with open(self._src, &#34;r&#34;) as f:
            content = f.read()
            lines = content.split(&#34;\n&#34;)
            comma_counts = [len(comma_sep.findall(i)) for i in lines]
            max_commas = max(comma_counts)
            lines = [i + (max_commas - j) * empty_comma_filler for i, j in zip(lines, comma_counts)]
        new_content = &#34;\n&#34;.join(lines)
        return new_content

    def _has_quotes(self):
        &#34;&#34;&#34;
        Checks if cells from the csv input file have quotes around them.
        Essentially it checks if there are any &#34;,&#34; patterns in the file.
        &#34;&#34;&#34;
        delimiter = &#34;;&#34; if self._is_csv2() else &#34;,&#34;
        with open(self._src, &#34;r&#34;) as f:
            content = f.read()
        has_quotes = f&#39;&#34;{delimiter}&#34;&#39; in content
        return has_quotes</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="qpcr.Parsers.Parsers._CORE_Parser" href="#qpcr.Parsers.Parsers._CORE_Parser">_CORE_Parser</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qpcr.Parsers.Parsers.CsvParser.pipe"><code class="name flex">
<span>def <span class="ident">pipe</span></span>(<span>self, filename:Â str, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A wrapper for read+parse</p>
<h2 id="note">Note</h2>
<p>This is the suggested use of <code><a title="qpcr.Parsers.Parsers.CsvParser" href="#qpcr.Parsers.Parsers.CsvParser">CsvParser</a></code>.
If a directory has been specified into which the datafiles shall be saved,
then saving will automatically be done.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>A filepath to an input csv file.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Any additional keyword argument that will be passed to any of the wrapped methods.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>assays</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary of all the extracted assays from the datafile storing the data as pandas DataFrames.
Individual assays can also be accessed using the <code>get</code> method.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pipe(self, filename :str, **kwargs):
    &#34;&#34;&#34;
    A wrapper for read+parse

    Note 
    ----
    This is the suggested use of `CsvParser`. 
    If a directory has been specified into which the datafiles shall be saved, 
    then saving will automatically be done.

    Parameters
    -------
    filename : str
        A filepath to an input csv file.
    **kwargs
        Any additional keyword argument that will be passed to any of the wrapped methods.
    Returns
    -------
    assays : dict
        A dictionary of all the extracted assays from the datafile storing the data as pandas DataFrames.
        Individual assays can also be accessed using the `get` method.
    &#34;&#34;&#34;
    try: 
        self.read(filename, **kwargs)
    except: 
        self.read(filename)
        aw.SoftWarning(&#34;Parser:incompatible_read_kwargs&#34;, func = &#34;pandas.read_csv&#34;)
    
    self.parse(**kwargs)
    assays = self.get()
    
    if self._save_loc is not None: 
        self.save()
    
    return assays</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers.CsvParser.read"><code class="name flex">
<span>def <span class="ident">read</span></span>(<span>self, filename:Â str, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads an input csv file. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>A filepath to an input csv file.</dd>
</dl>
<p>**kwargs
Any additional keyword arguments to be passed to pandas' <code>read_csv</code> function.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read(self, filename : str, **kwargs):
    &#34;&#34;&#34;
    Reads an input csv file. 

    Parameters
    -------
    filename : str
        A filepath to an input csv file.
    **kwargs 
        Any additional keyword arguments to be passed to pandas&#39; `read_csv` function.
    &#34;&#34;&#34;
    self._src = filename

    contents = self._prepare_commas()
    contents = StringIO(contents) # convert to StringIO for pandas to be able to read
    
    delimiter = &#34;;&#34; if self._is_csv2() else &#34;,&#34;
    delimiter = aux.from_kwargs(&#34;sep&#34;, delimiter, kwargs, rm = True)

    # now read the data and convert to numpy array
    try: 
        df = pd.read_csv(contents, header = None, sep = delimiter, **kwargs)
    except: 
        aw.SoftWarning(&#34;Parser:incompatible_read_kwargs&#34;, func = &#34;pandas.read_csv()&#34;)
        df = pd.read_csv(contents, header = None, sep = delimiter)

    df = df.dropna(axis = 0, how = &#34;all&#34;).reset_index(drop=True)
    data = df.to_numpy()

    self._data = data</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="qpcr.Parsers.Parsers._CORE_Parser" href="#qpcr.Parsers.Parsers._CORE_Parser">_CORE_Parser</a></b></code>:
<ul class="hlist">
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.assay_pattern" href="#qpcr.Parsers.Parsers._CORE_Parser.assay_pattern">assay_pattern</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.assays" href="#qpcr.Parsers.Parsers._CORE_Parser.assays">assays</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.clear" href="#qpcr.Parsers.Parsers._CORE_Parser.clear">clear</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.find_assays" href="#qpcr.Parsers.Parsers._CORE_Parser.find_assays">find_assays</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.find_by_decorator" href="#qpcr.Parsers.Parsers._CORE_Parser.find_by_decorator">find_by_decorator</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.find_columns" href="#qpcr.Parsers.Parsers._CORE_Parser.find_columns">find_columns</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.get" href="#qpcr.Parsers.Parsers._CORE_Parser.get">get</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.labels" href="#qpcr.Parsers.Parsers._CORE_Parser.labels">labels</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.make_dataframes" href="#qpcr.Parsers.Parsers._CORE_Parser.make_dataframes">make_dataframes</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.max_assay_name_length" href="#qpcr.Parsers.Parsers._CORE_Parser.max_assay_name_length">max_assay_name_length</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.parse" href="#qpcr.Parsers.Parsers._CORE_Parser.parse">parse</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.prune" href="#qpcr.Parsers.Parsers._CORE_Parser.prune">prune</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.save" href="#qpcr.Parsers.Parsers._CORE_Parser.save">save</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.save_to" href="#qpcr.Parsers.Parsers._CORE_Parser.save_to">save_to</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.transpose" href="#qpcr.Parsers.Parsers._CORE_Parser.transpose">transpose</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="qpcr.Parsers.Parsers.ExcelParser"><code class="flex name class">
<span>class <span class="ident">ExcelParser</span></span>
</code></dt>
<dd>
<div class="desc"><p>Handles reading and parsing <code>excel</code> files that may contain multiple assays.
It extracts datasets either through regex pattern matching or/and through provided
decorators within the datafile.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ExcelParser(_CORE_Parser):
    &#34;&#34;&#34;
    Handles reading and parsing `excel` files that may contain multiple assays.
    It extracts datasets either through regex pattern matching or/and through provided
    decorators within the datafile.
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()

    def read(self, filename : str, sheet_name : (str or int) = 0, **kwargs):
        &#34;&#34;&#34;
        Reads an input excel file. 

        Parameters
        -------
        filename : str
            A filepath to an input excel file.
        sheet_name : int or str
            The name of a specific spreadsheet of the file to read.
            If none is provided by default the first sheet will be read.
            Only one single sheet can be read at a time. 
            If an `integer` is provided the sheets will be accessed by their order, otherwise by their name (if a `string` is provided).
        **kwargs
            Any additional keyword arguments to be passed to pandas `read_excel` function.
        &#34;&#34;&#34;
        self._src = filename

        # read data and convert to numpy array
        try: 
            data = pd.read_excel(self._src, sheet_name = sheet_name, header = None, **kwargs)
        except: 
            data = pd.read_excel(self._src, sheet_name = sheet_name, header = None)
            aw.SoftWarning(&#34;Parser:incompatible_read_kwargs&#34;, func = &#34;pandas.read_excel()&#34;)

        data = data.to_numpy()

        self._data = data

    def pipe(self, filename :str, **kwargs):
        &#34;&#34;&#34;
        A wrapper for read+parse

        Note 
        ----
        This is the suggested use of `ExcelParser`. 
        If a directory has been specified into which the datafiles shall be saved, 
        then saving will automatically be done.

        Parameters
        -------
        filename : str
            A filepath to an input excel file.
        **kwargs
            Any additional keyword argument that will be passed to any of the wrapped methods.
        Returns
        -------
        assays : dict
            A dictionary of all the extracted assays from the datafile storing the data as pandas DataFrames.
            Individual assays can also be accessed using the `get` method.
        &#34;&#34;&#34;
        try: 
            self.read(filename, **kwargs)
        except: 
            self.read(filename)
            aw.SoftWarning(&#34;Parser:incompatible_read_kwargs&#34;, func = &#34;pandas.read_excel&#34;)

        self.parse(**kwargs)
        assays = self.get()

        if self._save_loc is not None: 
            self.save()

        return assays</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="qpcr.Parsers.Parsers._CORE_Parser" href="#qpcr.Parsers.Parsers._CORE_Parser">_CORE_Parser</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qpcr.Parsers.Parsers.ExcelParser.pipe"><code class="name flex">
<span>def <span class="ident">pipe</span></span>(<span>self, filename:Â str, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A wrapper for read+parse</p>
<h2 id="note">Note</h2>
<p>This is the suggested use of <code><a title="qpcr.Parsers.Parsers.ExcelParser" href="#qpcr.Parsers.Parsers.ExcelParser">ExcelParser</a></code>.
If a directory has been specified into which the datafiles shall be saved,
then saving will automatically be done.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>A filepath to an input excel file.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Any additional keyword argument that will be passed to any of the wrapped methods.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>assays</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary of all the extracted assays from the datafile storing the data as pandas DataFrames.
Individual assays can also be accessed using the <code>get</code> method.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pipe(self, filename :str, **kwargs):
    &#34;&#34;&#34;
    A wrapper for read+parse

    Note 
    ----
    This is the suggested use of `ExcelParser`. 
    If a directory has been specified into which the datafiles shall be saved, 
    then saving will automatically be done.

    Parameters
    -------
    filename : str
        A filepath to an input excel file.
    **kwargs
        Any additional keyword argument that will be passed to any of the wrapped methods.
    Returns
    -------
    assays : dict
        A dictionary of all the extracted assays from the datafile storing the data as pandas DataFrames.
        Individual assays can also be accessed using the `get` method.
    &#34;&#34;&#34;
    try: 
        self.read(filename, **kwargs)
    except: 
        self.read(filename)
        aw.SoftWarning(&#34;Parser:incompatible_read_kwargs&#34;, func = &#34;pandas.read_excel&#34;)

    self.parse(**kwargs)
    assays = self.get()

    if self._save_loc is not None: 
        self.save()

    return assays</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers.ExcelParser.read"><code class="name flex">
<span>def <span class="ident">read</span></span>(<span>self, filename:Â str, sheet_name:Â strÂ =Â 0, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads an input excel file. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>A filepath to an input excel file.</dd>
<dt><strong><code>sheet_name</code></strong> :&ensp;<code>int</code> or <code>str</code></dt>
<dd>The name of a specific spreadsheet of the file to read.
If none is provided by default the first sheet will be read.
Only one single sheet can be read at a time.
If an <code>integer</code> is provided the sheets will be accessed by their order, otherwise by their name (if a <code>string</code> is provided).</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Any additional keyword arguments to be passed to pandas <code>read_excel</code> function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read(self, filename : str, sheet_name : (str or int) = 0, **kwargs):
    &#34;&#34;&#34;
    Reads an input excel file. 

    Parameters
    -------
    filename : str
        A filepath to an input excel file.
    sheet_name : int or str
        The name of a specific spreadsheet of the file to read.
        If none is provided by default the first sheet will be read.
        Only one single sheet can be read at a time. 
        If an `integer` is provided the sheets will be accessed by their order, otherwise by their name (if a `string` is provided).
    **kwargs
        Any additional keyword arguments to be passed to pandas `read_excel` function.
    &#34;&#34;&#34;
    self._src = filename

    # read data and convert to numpy array
    try: 
        data = pd.read_excel(self._src, sheet_name = sheet_name, header = None, **kwargs)
    except: 
        data = pd.read_excel(self._src, sheet_name = sheet_name, header = None)
        aw.SoftWarning(&#34;Parser:incompatible_read_kwargs&#34;, func = &#34;pandas.read_excel()&#34;)

    data = data.to_numpy()

    self._data = data</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="qpcr.Parsers.Parsers._CORE_Parser" href="#qpcr.Parsers.Parsers._CORE_Parser">_CORE_Parser</a></b></code>:
<ul class="hlist">
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.assay_pattern" href="#qpcr.Parsers.Parsers._CORE_Parser.assay_pattern">assay_pattern</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.assays" href="#qpcr.Parsers.Parsers._CORE_Parser.assays">assays</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.clear" href="#qpcr.Parsers.Parsers._CORE_Parser.clear">clear</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.find_assays" href="#qpcr.Parsers.Parsers._CORE_Parser.find_assays">find_assays</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.find_by_decorator" href="#qpcr.Parsers.Parsers._CORE_Parser.find_by_decorator">find_by_decorator</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.find_columns" href="#qpcr.Parsers.Parsers._CORE_Parser.find_columns">find_columns</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.get" href="#qpcr.Parsers.Parsers._CORE_Parser.get">get</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.labels" href="#qpcr.Parsers.Parsers._CORE_Parser.labels">labels</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.make_dataframes" href="#qpcr.Parsers.Parsers._CORE_Parser.make_dataframes">make_dataframes</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.max_assay_name_length" href="#qpcr.Parsers.Parsers._CORE_Parser.max_assay_name_length">max_assay_name_length</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.parse" href="#qpcr.Parsers.Parsers._CORE_Parser.parse">parse</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.prune" href="#qpcr.Parsers.Parsers._CORE_Parser.prune">prune</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.save" href="#qpcr.Parsers.Parsers._CORE_Parser.save">save</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.save_to" href="#qpcr.Parsers.Parsers._CORE_Parser.save_to">save_to</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.transpose" href="#qpcr.Parsers.Parsers._CORE_Parser.transpose">transpose</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser"><code class="flex name class">
<span>class <span class="ident">_CORE_Parser</span></span>
</code></dt>
<dd>
<div class="desc"><p>This is the functional core for the irregular multi-assay file-reader classes.
It handles the regex searching and numpy indexing of relevant column subsets of the datafiles.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class _CORE_Parser:
    &#34;&#34;&#34;
    This is the functional core for the irregular multi-assay file-reader classes.
    It handles the regex searching and numpy indexing of relevant column subsets of the datafiles.
    &#34;&#34;&#34;
    def __init__(self):
        self._src = None
        self._pattern = None
        self._data = None

        # the found assays, these will be arrays/lists that store the indices, 

        self._assay_indices = None                  # indices of the assay identifiers
        self._assay_names = None                    # names of the assays

        self._assay_names_start_indices = None      # indices of the rep. id headers
        self._assay_names_end_indices = None        # indices of the last entry of the rep. id columns

        self._assay_ct_start_indices = None         # indices of the ct headers
        self._assay_ct_end_indices = None           # indices of the last entry of the ct columns

        # a dictionary to store all assay dataframes
        self._dfs = {}

        # setup the labels for replicate ids and ct value column headers
        self.labels()

        # we must specify a maximum allowed length for the assay names before hand 
        # (since we&#39;re using numpy arrays for storing the names, which require enough open slots to store the characters)
        self._max_assay_name_length = 20
    
        # a folder into which the new assay-split datafiles should be stored
        self._save_loc = None

        # set transpose option in case datasets are stored not on separate row ranges but separate column ranges
        self._transpose = False


        # set up a BigTable data range
        self._bigtable_range = None

        # setup a warning for decorators without patterns warning that it will set to default
        # we do this here in case the MultiSheetReader calls read+parse here multiple times
        # but we don&#39;t need to know that it defaults every time.
        self._WARNING_decorators_but_no_patterns = aw.SoftWarning(&#34;Parser:decorators_but_no_pattern&#34;, once = True)

    def prune(self):
        &#34;&#34;&#34;
        Completely resets the Parser, clearing all data and preset-specifics such as the assay_pattern.
        &#34;&#34;&#34;
        self.__init__()

    def clear(self):
        &#34;&#34;&#34;
        Clears all datasets that were extracted.
        &#34;&#34;&#34;
        self._dfs = {}

        self._assay_indices = None                  # indices of the assay identifiers
        self._assay_names = None                    # names of the assays

        self._assay_names_start_indices = None      # indices of the rep. id headers
        self._assay_names_end_indices = None        # indices of the last entry of the rep. id columns

        self._assay_ct_start_indices = None         # indices of the ct headers
        self._assay_ct_end_indices = None           # indices of the last entry of the ct columns

    def transpose(self):
        &#34;&#34;&#34;
        Inverts the `col` index used by `qpcr.Parsers._CORE_Parser.find_assays` and `qpcr.Parsers._CORE_Parser.find_by_decorator`.
        By default the `col` refers to a column. After using `transpose` it will be interpreted as a `row`.
        Note
        ----
        This is method is dynamic, so repeated calling of `transpose` will keep reverting the interpretation from row to col, back to row, etc.
        &#34;&#34;&#34;
        self._transpose = not self._transpose

    def save_to(self, location : str = None):
        &#34;&#34;&#34;
        Sets the location into which the individual assay datafiles should be saved.
        Parameters
        ----------
        location : str
            The path to a directory where the newly generated assay datafiles shall be saved.
            If this directory does not yet exist, it will be automatically made.
        &#34;&#34;&#34;
        if location is not None: 
            self._save_loc = location
            if not os.path.exists(self._save_loc):
                os.mkdir(self._save_loc)
        return self._save_loc
    
    def get(self, assay : str = None):
        &#34;&#34;&#34;
        Parameters
        ----------
        assay : str
            The name of an assay found in the datafile. Available assays can be assessed using the `self.assays` method.
    
        Returns
        -------
        data : pd.DataFrame or dict
            Either a specific pandas dataframe of one of the assays (if an `assay` name was specified)
            or the entire dictionary of all found dataframes from all assays.
        &#34;&#34;&#34;
        if assay is not None:
            data = self._dfs[assay]
        else: 
            data = self._dfs
        return data

    def save(self):
        &#34;&#34;&#34;
        Saves the individual assays as separate csv files.
        This requires that a saving directory has been set using `self.save_to`.
        The files will simply be named according to the assay name (i.e. `ActinB.csv` for instance).
        &#34;&#34;&#34;
        if self._save_loc is None:
            aw.SoftWarning(&#34;Parser:no_save_loc&#34;)
        else:
            for assay, df in self._dfs.items():
                assay_path = os.path.join(self.save_to(), f&#34;{assay}.csv&#34;)
                df.to_csv(assay_path, index = False)

    def labels(self, id_label : str = &#34;Name&#34;, ct_label : str = &#34;Ct&#34;):
        &#34;&#34;&#34;
        Sets the headers for the relevant data columns for each assay within the datafile.

        Parameters
        ----------
        id_label : str
            The header above the column containing replicate identifiers. 
        
        ct_label : str
            The header above the column containing the replicates&#39; Ct values.
        &#34;&#34;&#34;
        self._id_label = id_label
        self._ct_label = ct_label

    def assays(self):
        &#34;&#34;&#34;
        Returns
        -------
        names : list
            The names of the found assays of the datafile
        &#34;&#34;&#34;        
        return list(self._dfs.keys())

    def assay_pattern(self, pattern : str = None, *flags):
        &#34;&#34;&#34;
        Sets up a regex pattern defining the assay declarations within the datafile.

        Parameters
        ----------
        pattern : str
            A string containing either the key to a predefined pattern from the `assay_patterns` dictionary, 
            or directly regex pattern. 
            If a regex pattern is directly provided, that pattern must contain a capturing group
            for the assay name that can be extracted.
        *flags 
            Any additional flags to pass to `re.compile()` for the regex pattern

        Returns
        -------
        pattern : re.Pattern
            The currently used regex pattern to identify assays within the datafile.
        &#34;&#34;&#34;
        if pattern is not None: 
            # try to get the pattern from the predefined patterns via key
            _pattern = aux.from_kwargs(pattern, None, assay_patterns)
            _pattern = pattern if _pattern is None else _pattern
            self._pattern = re.compile(_pattern, *flags)
        return self._pattern

    def max_assay_name_length(self, length = 20):
        &#34;&#34;&#34;
        Sets the maximum allowed name length (number of characters) assay names.
        
        Parameters
        ----------
        length : int
            The maximum number of characters to store for the assay name. 
            Default is `length = 20` characters.
        &#34;&#34;&#34;
        self._max_assay_name_length = length
    
    def parse(self, **kwargs):
        &#34;&#34;&#34;
        A wrapper for find_assays+find_columns+make_dataframes
        This is the functional core of the Parser&#39;s `pipe` method.

        Parameters
        -------
        **kwargs
            Any additional keyword argument that will be passed to any of the wrapped methods.
        &#34;&#34;&#34;
        decorator = aux.from_kwargs(&#34;decorator&#34;, None, kwargs, rm = True)
        if decorator is not None:
            self.find_by_decorator(decorator = decorator, **kwargs)
        else: 
            self.find_assays(**kwargs)
        
        # ignore if no assays were found (default is false, unless we use multi-assay multi-sheet files)
        ignore_empty = aux.from_kwargs(&#34;ignore_empty&#34;, False, kwargs)
        if ignore_empty:
            try: 
                self.find_columns()
                self.make_dataframes(**kwargs)
            except: 
                pass
        else: 
            self.find_columns()
            self.make_dataframes(**kwargs)

    def find_by_decorator(self, decorator : str, col = 0, **kwargs):
        &#34;&#34;&#34;
        Parses through a column of the datafile and finds all assays that are decorated with a specific decorator.
        Note that this requires that the decorator is in the cell above the assay header. Also, make sure to specify
        an `assay_pattern` to extract the assay name. If no `assay_pattern` is provided, it will simply take the entire cell content.
        
        Parameters
        -----------
        decorator : str
            One of the available `qpcr-decorator`&#39;s for irregular multi-assay files. 
            Available decorators can be assessed via the `qpcr.Parsers.decorators` dictionary keys.
        col : int
            The column in which to look for assay identifiers. 
            By default the first column `col = 0`.
        
        Returns
        -------
        assay_indices : np.ndarray
            The indices (row, col) of all assays found.
        names : np.ndarray
            The extracted names of all assays found.
        &#34;&#34;&#34;
        # ignore if no assays were found (default is false, unless we use multi-assay multi-sheet files)
        ignore_empty = aux.from_kwargs(&#34;ignore_empty&#34;, False, kwargs)
        # get the pattern required (or raise error if invalid decorators are provided)
        if decorator not in decorators.keys():
            aw.HardWarning(&#34;Parser:invalid_decorator&#34;, d = decorator, all_d = list(decorators.keys()))

        decorator_pattern = re.compile( decorators[decorator] )
        decorator_indices, decorator_names = self.find_assays(col = col, pattern = decorator_pattern, **kwargs )

        # check if decorators were identified
        found_indices = decorator_indices.size &gt; 0
        if not found_indices:
            if not ignore_empty: # if none were found either raise error or ignore
                aw.HardWarning(&#34;Parser:no_decorators_found&#34;)
            else: 
                return

        # if no assay_pattern was specified then default to generic &#34;all&#34; to get full cell contents
        if self.assay_pattern() is None:
            self._WARNING_decorators_but_no_patterns.trigger()
            self.assay_pattern(&#34;all&#34;)

        assay_indices = decorator_indices
        # get assay indices as the cells IMMEDIATELY BELOW the decorators
        # we adjust either col or the row indices depending on the transposition
        if self._transpose:
            col = col + 1
        else:
            assay_indices = assay_indices + 1
      
        # get all assay header cells into an array to extract their names
        array = self._prep_header_array(col, assay_indices)

        # adjust avaliable length of stored assay names
        max_length = max(
                            list(  map(len, array)  )
                        )
        self.max_assay_name_length(max_length)

        names = np.array([&#34;-&#34;*self._max_assay_name_length for _ in range(len(array))]) # we need to pre-specify the max allowed length for the assay names by filling an array with some dummy placeholders (&#39;-&#39;)
        idx = 0
        for entry in array:
            # try:
            match = self.assay_pattern().search(entry)
            if match is not None: 
                name = match.group(1)
                names[idx] = name
            # except: 
            #     continue
            idx += 1

        self._assay_indices = assay_indices
        self._assay_names = names
        
        return assay_indices, names

    def find_assays(self, col = 0, **kwargs):
        &#34;&#34;&#34;
        Parses through a column of the datafile and identifies all indices of cells that match the provided `assay_pattern``.
        It stores these values internally and also returns the results as numpy arrays.

        Parameters
        -----------
        col : int
            The column in which to look for assay identifiers. 
            By default the first column `col = 0`.

        Returns
        -------
        indices : np.ndarray
            The indices (row, col) of all assays found.
        names : np.ndarray
            The extracted names of all assays found.
        &#34;&#34;&#34;
        
        custom_pattern = aux.from_kwargs(&#34;pattern&#34;, None, kwargs)
        if self._pattern is None and custom_pattern is None: 
            aw.HardWarning(&#34;Parser:no_pattern_yet&#34;)

        pattern_to_use = self._pattern if custom_pattern is None else custom_pattern

        array = self._prep_header_array(col = col)

        indices = np.zeros(len(array))
        names = np.array([&#34;-&#34;*self._max_assay_name_length for _ in range(len(array))]) # we need to pre-specify the max allowed length for the assay names by filling an array with some dummy placeholders (&#39;-&#39;)
        idx = 0
        for entry in array:
            # try: 
            match = pattern_to_use.search(entry)
            if match is not None: 
                name = match.group(1)
                names[idx] = name
                indices[idx] = 1
            # except: 
            #     continue
            idx += 1
        indices = np.argwhere(indices == 1)

        if indices.size == 0:
            # ignore if no assays were found (default is false, 
            # unless we use multi-assay multi-sheet files)
            ignore_empty = aux.from_kwargs(&#34;ignore_empty&#34;, False, kwargs)
            if not ignore_empty:
                aw.HardWarning(&#34;Parser:no_assays_found&#34;, traceback = False)

        names = names[indices]
        names = names.reshape(len(names))

        self._assay_indices = indices
        self._assay_names = names
        
        return indices, names
    
    def find_columns(self):
        &#34;&#34;&#34;
        Identifies the relevant data column belonging to each assay within the datafile.
        &#34;&#34;&#34;
        # search indices of the starts of id and ct columns
        # these are now the row, col coordinates of each name_column header
        name_col_starts = self._find_column_starts(
                                                    label = self._id_label, 
                                                    ref_indices = self._assay_indices
                                                )
        # these are now the row, col coordinates of each ct_column header
        ct_col_starts = self._find_column_starts(
                                                    label = self._ct_label, 
                                                    ref_indices = self._assay_indices
                                                )

        # now we need to generate know also the end indices of the datacolumns
        name_col_ends = self._find_column_ends(name_col_starts)
        
        # now that we know the end indices for the replicate id column we will adopt the end row indices
        # onto the ct column as well (we don&#39;t parse through the Ct column because it might have missing 
        # Ct values intersperced which would prematurely terminate the parsing...)

        # (1) we transpose to have all row indices easily accessible in the first line
        # (2) we adopt row indices from the transposed name col
        # (3) and transpose back to get our final ct end indices
        ct_col_ends = deepcopy( np.transpose(ct_col_starts) )
        name_col_ends_t = np.transpose(name_col_ends)
        ct_col_ends[0] = name_col_ends_t[0]
        ct_col_ends = np.transpose(ct_col_ends)

        # now store the data
        self._assay_names_start_indices = name_col_starts
        self._assay_names_end_indices = name_col_ends
        self._assay_ct_start_indices = ct_col_starts
        self._assay_ct_end_indices = ct_col_ends

    def make_dataframes(self, allow_nan_ct : bool = True, default_to : float = None, **kwargs):
        &#34;&#34;&#34;
        Generates a set of `pandas DataFrame`s each containing two columns 
        (one for the replicate identifiers, one for the Ct values)
        for subsequent use with the main `qpcr` module.

        Parameters
        ------
        allow_nan_Ct : bool
            Allows Ct values to be NaN within the final dataframe (if `True`, default).
            If no NaN Ct values should be maintained a default value for NaN Ct values must be specified
            using `default_to`.
        default_to : float
            The default value to replace NaN Ct values with. 
            This is ignored if `allow_nan_ct = True`.
        &#34;&#34;&#34;  

        adx = 0
        # print(self._assay_names, self._assay_indices, self._assay_names_start_indices, self._assay_names_end_indices)
        for assay in self._assay_names:
            
            # get the assay&#39;s indices of both replicate id and ct columns
            names_start = self._assay_names_start_indices[adx]
            names_end = self._assay_names_end_indices[adx]
            ct_start = self._assay_ct_start_indices[adx]
            ct_end = self._assay_ct_end_indices[adx]

            # generate the final index slices from the total array
            # of both replicate id (names) and ct columns
            names_range, names_col = self._make_index_range(names_start, names_end, crop_first = True)
            ct_range, ct_col = self._make_index_range(ct_start, ct_end, crop_first = True)

            # get the assay data
            assay_names = self._data[names_range, names_col]
            assay_cts = self._data[ct_range, ct_col]

            # and convert to numeric data
            try: 
                assay_cts = assay_cts.astype(float)
            except ValueError as e:
                assay_cts = np.genfromtxt(  np.array(assay_cts, dtype=str)  )
                bad_value = e.__str__().split(&#34;: &#34;)[1]
                aw.SoftWarning(&#34;Parser:found_non_readable_cts&#34;, assay = assay, bad_value = bad_value)

            # assemble the assay dataframe 
            assay_df = pd.DataFrame(
                                    {
                                        standard_id_header : assay_names, 
                                        standard_ct_header : assay_cts,
                                    }
                                )

            if not allow_nan_ct:
                if not isinstance(default_to, (int, float)): 
                    aw.HardWarning(&#34;Parser:no_ct_nan_default&#34;, d = default_to)
                
                # apply defaulting lambda function
                assay_df[ standard_ct_header ] = assay_df[ standard_ct_header ].apply(
                                                                                        lambda x: x if x == x else default_to
                                                                                    )
            # and store dataframe
            self._dfs.update(
                                { assay : assay_df }
                            )

            adx += 1

    def _make_BigTable_range(self, **kwargs):
        &#34;&#34;&#34;
        Generates a pandas DataFrame of a subsection of an irregular datafile
        containing a &#34;big data table&#34; with multiple assays specified in it. 

        It makes use of the `id_label` specified using `_CORE_Parser.labels` as the
        anchor. The resulting dataframe fill contain all rows from the cell where `id_label``
        as located until the data is empty. 

        If additionally `replicates` are specified in the `kwargs` 
        the starting positions of assay replicates are inferred based on `decorators`. 
        Note, this only works for `horizontal` Big Tables!
        &#34;&#34;&#34;
        is_horizontal = aux.from_kwargs(&#34;is_horizontal&#34;, False, kwargs)
        
        # get the main data
        data = self._data.astype(&#34;str&#34;)
        ref_col_header = self._id_label   
        
        # find big table starting row
        idx = np.argwhere(data == ref_col_header)

        # vet that we actually found the big table
        if idx.size == 0:
            aw.HardWarning(&#34;Parser:no_bigtable_header&#34;, header = ref_col_header)
        idx = idx.reshape(idx.size)
        start, col = idx

        idx = 1
        while True:
            try: 
                entry = data[start+idx, col]
            except: 
                break
            if entry == &#34;nan&#34;:
                break
            idx += 1

        end = start + idx

        # put a -1 offset on the rows if horziontal, as the decorators 
        # are in the row above the actual column headers.
        if is_horizontal:
            start -= 1

        # generate bigtable data range and store
        relevant_data = data[start : end, : ]
        self._bigtable_range = relevant_data  


    def _infer_BigTable_groups(self, **kwargs):
        &#34;&#34;&#34;
        Gets the group ranges from the bigtable datarange.
        Note, this is only used in case of horizontal big tables.
        &#34;&#34;&#34;
        # get the relevant data
        array = self._bigtable_range
        maxrows, allcols = array.shape

        ignore_empty = aux.from_kwargs(&#34;ignore_empty&#34;,False,kwargs)

        # get and vet replicates
        replicates = aux.from_kwargs(&#34;replicates&#34;, None, kwargs, rm = True)
        if replicates is None: 
            aw.HardWarning(&#34;Parser:bigtable_no_replicates&#34;, traceback = False)
        replicates, names = self._vet_replicates(ignore_empty, replicates, array, **kwargs)

        
        rdx = 0 # counter for the replicate groups
        
        data_array = None # this array will store the entire transposed data

        decorator = plain_decorators[&#34;qpcr:group&#34;]

        # now get the assays in question
        # we already vetted if the file is properly 
        # decorated during _vet_replicates
            
        # find decorated starting columns
        # get only column indices            
        indices = np.argwhere(array == decorator)
        indices = indices[ : , 1 ]

        if indices.size == 1:
            indices = [indices]

        # get total slice of bigtable rows
        rows = slice( 1, maxrows )
        
        # iterate over each group
        for col in indices: 
            
            rep = replicates[rdx]

            # get data columns
            cols = slice( col, col + rep )
            
            # get data
            data = array[  rows, cols  ]
            
            # rename data cols if names are provided
            if names is not None: 
                name = names[rdx]
                data[ 0 , : ] = name
            
            # concatenate data into a single array
            if data_array is None: 
                data_array = data
            else:
                data_array = np.concatenate(
                                                ( data_array, data ),
                                                axis = 1,
                                            )
            rdx += 1

        # remove groups from the data array
        groups = data_array[ 0, : ]
        data_array = data_array[ 1:, : ]

        # Actually, right here, instead of having to infer our own replicate 
        # names thare are then just group0 group0 group0 group1 ...
        # We can simply use the sample repeat / tile approach we used to make the
        # group1 etc. replicate names, based on the ACTUAL groups (like the ones we 
        # have just split off from the data -&gt; their first row are already the replicate
        # identifiers we just use those directly... )

        # reshape data into a single column
        data_array = np.concatenate(data_array, axis = 0)

        # now repeat the groups to match the stacked new data column
        groups_tiled = np.tile(groups, data_array.size // groups.size )

        # now get the dataset id column
        id_col = self._id_label
        id_start = np.where(array == id_col)
        id_row, id_col = id_start
        id_rows = slice( int(id_row + 1), maxrows )
        id_col = array[  id_rows, id_col  ]
        
        # repeat dataset ids to match stacked new data column
        ids_tiled = np.repeat(  id_col , groups.size  )

        # assemble all data
        total_data = [ids_tiled, groups_tiled, data_array]
        headers = [ default_dataset_header, standard_id_header, standard_ct_header ]  

        # check for qpcr column and if present, get and adjust shape
        self._BigTable_horizontal_qpcr_col(array, maxrows, groups, total_data, headers)


        # combine the three columns (dataset id, groups, and Ct (actual data_array))
        data_array = np.stack( total_data, axis = 1 )

        # add default names into the first row
        data_array = np.concatenate(
                                        ( 
                                            [headers],
                                            data_array
                                       ),   axis = 0
                                )
        
        # actually return the finished array
        return data_array

    def _BigTable_horizontal_qpcr_col(self, array, maxrows, groups, total_data, headers):
        &#34;&#34;&#34;
        Checks if a &#34;@qpcr&#34; column is present in the data and if so, adjusts its shape and 
        adds it to the data to be assembled for the assays.
        &#34;&#34;&#34;
        column_decorator = plain_decorators[&#34;qpcr:column&#34;]
        qpcr_col = np.where(array == column_decorator)
        qpcr_row, qpcr_col = qpcr_col
        if len(qpcr_col) != 0:
            
            qpcr_rows = slice( int(qpcr_row + 1), maxrows )
            qpcr_col = array[  qpcr_rows, qpcr_col  ]

            qpcr_tiled = np.repeat(  qpcr_col , groups.size  )
            total_data.append(qpcr_tiled)
            headers.append(&#34;@qpcr&#34;)

    def _vet_replicates(self, ignore_empty, replicates, array, **kwargs):
        &#34;&#34;&#34;
        Checks if provided replicates cover all data groups (annoated columns).
        And it also gets the names supposed to be used for the columns.
        &#34;&#34;&#34;
        # get assays for each decorator
        groups = 0
        decorator = plain_decorators[&#34;qpcr:group&#34;]
    
        # find decorated starting columns
        indices = np.argwhere(array == decorator)
        if indices.size == 0 and not ignore_empty:
            aw.HardWarning(&#34;Parser:no_decorators_found&#34;, traceback = False)

        # get only column indices            
        indices = indices[ : , 1 ]
        groups += indices.size

        # check if replicates are an integer, if so transform to 
        # tuple that cover all found assays
        if aux.same_type(replicates, 1): 
            replicates = np.tile([replicates], groups)
        # else, check it it&#39;s a formula that needs to be read out to a tuple.
        elif aux.same_type(replicates, &#34;&#34;):
            replicates = qpcr.Assay()._reps_from_formula(replicates)
        
        # if replicates are already a tuple, make sure they cover all rows
        elif aux.same_type(replicates, ()):
            all_covered = groups == len(replicates)
            if not all_covered:
                aw.HardWarning(&#34;Assay:reps_dont_cover&#34;, n_samples = groups, reps = replicates, traceback = False)
            
        # get names for assays
        group_names = aux.from_kwargs(&#34;names&#34;, None, kwargs)

        # vet that names cover
        if group_names is not None and len(group_names) != len(replicates):
            aw.HardWarning(&#34;Assay:groupnames_dont_colver&#34;, traceback = False, current_groups = f&#34;None, but needs to be {len(replicates)} names.&#34;, new_received = group_names)
        
        # return tranformed replicates
        return replicates, group_names

        

    def _prep_header_array(self, col = None, row = None):
        &#34;&#34;&#34;
        Generates the array in which header entries should be searched for
        &#34;&#34;&#34;
        
        if row is None and col is not None: 
            array = self._data[:, col] if not self._transpose else self._data[col, :]
        elif row is not None and col is None:
            array = self._data[row, :] if not self._transpose else self._data[:, row]
        elif row is not None and col is not None:
            array = self._data[row, col] if not self._transpose else self._data[col, row]
        else:
            aw.HardWarning(&#34;Parser:invalid_range&#34;)

        # re-format to str and reset &#34;nan&#34; to dummy_blank
        array = array.astype(str)
        array[ np.argwhere(array == &#34;nan&#34;) ] = dummy_blank
        array = array.reshape(array.size)
        return array


    def _make_index_range(self, start_indices, end_indices, crop_first = True):
        &#34;&#34;&#34;
        Generates an index range for a data column based on start and stop indices.
        This assumes that the column entry (i.e. entry[1]) is always the same and only the rows are different.
        
        Parameters
        ----------
        start_indices : np.ndarray
            Row, col indices of the header of the data column
        end_indices : np.ndarray
            Row, col indices of the last entry of the data column
        crop_first : bool
            If set to True it will offset the start row indices by +1 to exclude the header.
        &#34;&#34;&#34;

        start = start_indices[0] + 1 if crop_first else start_indices[0]
        end = end_indices[0]

        row_range = slice(  start, end  )
        col = start_indices[1]
        return row_range, col

    def _find_column_starts(self, label, ref_indices):
        &#34;&#34;&#34;
        This function uses the assays&#39; found reference row indices to 
        now search for the coordinates of the labeled cell so we know where a data column starts
        &#34;&#34;&#34;

        # get index to match to ref_indices
        idx_to_match = 0 if not self._transpose else 1

        data = self._data
        all_found = np.argwhere(data == label)
        row_indices = np.transpose(all_found)[ idx_to_match ]

    
        # adjust coordinates +1 as the headers would be in the row below the assay declaration
        # we only have to do this if we use the default setting of assays in the same column
        if not self._transpose:
            ref_indices = ref_indices + 1

        # ref_indices = ref_indices + 1 
        matching_rows = np.where(np.isin(row_indices, ref_indices))
                
        # if no matches were found, try incrementing the index offset once more 
        # (we&#39;ll allow for a single row between the header and the start of the data)
        no_matches = len(matching_rows) == 1 and matching_rows[0].size == 0
        if no_matches:
            ref_indices = ref_indices + 1
            matching_rows = np.where(np.isin(row_indices, ref_indices))

        # check again, and raise Error if still no matches are found
        no_matches = len(matching_rows) == 1 and matching_rows[0].size == 0
        if no_matches:
            aw.HardWarning(&#34;Parser:no_data_found&#34;, label = label)

        matching_rows = all_found[matching_rows]
        return matching_rows

    def _find_column_ends(self, indices):
        &#34;&#34;&#34;
        Determines the end index of a column within the datafile based on the starting indices of
        its header label
        &#34;&#34;&#34;
        data = self._data
        end_indices = np.zeros(indices.shape, dtype = int)
        adx = 0
        for i in indices:
            row, col = i
            idx = 0
            value = 0
            while True:
                try: value = data[row + idx, col]
                except: break
                if value != value: 
                    break
                idx += 1
            finals = np.array([row + idx, col], dtype = int)
            end_indices[adx] += finals
            adx += 1
        return end_indices</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="qpcr.Parsers.Parsers.CsvParser" href="#qpcr.Parsers.Parsers.CsvParser">CsvParser</a></li>
<li><a title="qpcr.Parsers.Parsers.ExcelParser" href="#qpcr.Parsers.Parsers.ExcelParser">ExcelParser</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.assay_pattern"><code class="name flex">
<span>def <span class="ident">assay_pattern</span></span>(<span>self, pattern:Â strÂ =Â None, *flags)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets up a regex pattern defining the assay declarations within the datafile.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>pattern</code></strong> :&ensp;<code>str</code></dt>
<dd>A string containing either the key to a predefined pattern from the <code>assay_patterns</code> dictionary,
or directly regex pattern.
If a regex pattern is directly provided, that pattern must contain a capturing group
for the assay name that can be extracted.</dd>
</dl>
<p>*flags
Any additional flags to pass to <code>re.compile()</code> for the regex pattern</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>pattern</code></strong> :&ensp;<code>re.Pattern</code></dt>
<dd>The currently used regex pattern to identify assays within the datafile.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assay_pattern(self, pattern : str = None, *flags):
    &#34;&#34;&#34;
    Sets up a regex pattern defining the assay declarations within the datafile.

    Parameters
    ----------
    pattern : str
        A string containing either the key to a predefined pattern from the `assay_patterns` dictionary, 
        or directly regex pattern. 
        If a regex pattern is directly provided, that pattern must contain a capturing group
        for the assay name that can be extracted.
    *flags 
        Any additional flags to pass to `re.compile()` for the regex pattern

    Returns
    -------
    pattern : re.Pattern
        The currently used regex pattern to identify assays within the datafile.
    &#34;&#34;&#34;
    if pattern is not None: 
        # try to get the pattern from the predefined patterns via key
        _pattern = aux.from_kwargs(pattern, None, assay_patterns)
        _pattern = pattern if _pattern is None else _pattern
        self._pattern = re.compile(_pattern, *flags)
    return self._pattern</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.assays"><code class="name flex">
<span>def <span class="ident">assays</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>names</code></strong> :&ensp;<code>list</code></dt>
<dd>The names of the found assays of the datafile</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assays(self):
    &#34;&#34;&#34;
    Returns
    -------
    names : list
        The names of the found assays of the datafile
    &#34;&#34;&#34;        
    return list(self._dfs.keys())</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.clear"><code class="name flex">
<span>def <span class="ident">clear</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Clears all datasets that were extracted.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear(self):
    &#34;&#34;&#34;
    Clears all datasets that were extracted.
    &#34;&#34;&#34;
    self._dfs = {}

    self._assay_indices = None                  # indices of the assay identifiers
    self._assay_names = None                    # names of the assays

    self._assay_names_start_indices = None      # indices of the rep. id headers
    self._assay_names_end_indices = None        # indices of the last entry of the rep. id columns

    self._assay_ct_start_indices = None         # indices of the ct headers
    self._assay_ct_end_indices = None           # indices of the last entry of the ct columns</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.find_assays"><code class="name flex">
<span>def <span class="ident">find_assays</span></span>(<span>self, col=0, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Parses through a column of the datafile and identifies all indices of cells that match the provided `assay_pattern``.
It stores these values internally and also returns the results as numpy arrays.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>col</code></strong> :&ensp;<code>int</code></dt>
<dd>The column in which to look for assay identifiers.
By default the first column <code>col = 0</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The indices (row, col) of all assays found.</dd>
<dt><strong><code>names</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The extracted names of all assays found.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_assays(self, col = 0, **kwargs):
    &#34;&#34;&#34;
    Parses through a column of the datafile and identifies all indices of cells that match the provided `assay_pattern``.
    It stores these values internally and also returns the results as numpy arrays.

    Parameters
    -----------
    col : int
        The column in which to look for assay identifiers. 
        By default the first column `col = 0`.

    Returns
    -------
    indices : np.ndarray
        The indices (row, col) of all assays found.
    names : np.ndarray
        The extracted names of all assays found.
    &#34;&#34;&#34;
    
    custom_pattern = aux.from_kwargs(&#34;pattern&#34;, None, kwargs)
    if self._pattern is None and custom_pattern is None: 
        aw.HardWarning(&#34;Parser:no_pattern_yet&#34;)

    pattern_to_use = self._pattern if custom_pattern is None else custom_pattern

    array = self._prep_header_array(col = col)

    indices = np.zeros(len(array))
    names = np.array([&#34;-&#34;*self._max_assay_name_length for _ in range(len(array))]) # we need to pre-specify the max allowed length for the assay names by filling an array with some dummy placeholders (&#39;-&#39;)
    idx = 0
    for entry in array:
        # try: 
        match = pattern_to_use.search(entry)
        if match is not None: 
            name = match.group(1)
            names[idx] = name
            indices[idx] = 1
        # except: 
        #     continue
        idx += 1
    indices = np.argwhere(indices == 1)

    if indices.size == 0:
        # ignore if no assays were found (default is false, 
        # unless we use multi-assay multi-sheet files)
        ignore_empty = aux.from_kwargs(&#34;ignore_empty&#34;, False, kwargs)
        if not ignore_empty:
            aw.HardWarning(&#34;Parser:no_assays_found&#34;, traceback = False)

    names = names[indices]
    names = names.reshape(len(names))

    self._assay_indices = indices
    self._assay_names = names
    
    return indices, names</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.find_by_decorator"><code class="name flex">
<span>def <span class="ident">find_by_decorator</span></span>(<span>self, decorator:Â str, col=0, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Parses through a column of the datafile and finds all assays that are decorated with a specific decorator.
Note that this requires that the decorator is in the cell above the assay header. Also, make sure to specify
an <code>assay_pattern</code> to extract the assay name. If no <code>assay_pattern</code> is provided, it will simply take the entire cell content.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>decorator</code></strong> :&ensp;<code>str</code></dt>
<dd>One of the available <code>qpcr-decorator</code>'s for irregular multi-assay files.
Available decorators can be assessed via the <code>qpcr.Parsers.decorators</code> dictionary keys.</dd>
<dt><strong><code>col</code></strong> :&ensp;<code>int</code></dt>
<dd>The column in which to look for assay identifiers.
By default the first column <code>col = 0</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>assay_indices</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The indices (row, col) of all assays found.</dd>
<dt><strong><code>names</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The extracted names of all assays found.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_by_decorator(self, decorator : str, col = 0, **kwargs):
    &#34;&#34;&#34;
    Parses through a column of the datafile and finds all assays that are decorated with a specific decorator.
    Note that this requires that the decorator is in the cell above the assay header. Also, make sure to specify
    an `assay_pattern` to extract the assay name. If no `assay_pattern` is provided, it will simply take the entire cell content.
    
    Parameters
    -----------
    decorator : str
        One of the available `qpcr-decorator`&#39;s for irregular multi-assay files. 
        Available decorators can be assessed via the `qpcr.Parsers.decorators` dictionary keys.
    col : int
        The column in which to look for assay identifiers. 
        By default the first column `col = 0`.
    
    Returns
    -------
    assay_indices : np.ndarray
        The indices (row, col) of all assays found.
    names : np.ndarray
        The extracted names of all assays found.
    &#34;&#34;&#34;
    # ignore if no assays were found (default is false, unless we use multi-assay multi-sheet files)
    ignore_empty = aux.from_kwargs(&#34;ignore_empty&#34;, False, kwargs)
    # get the pattern required (or raise error if invalid decorators are provided)
    if decorator not in decorators.keys():
        aw.HardWarning(&#34;Parser:invalid_decorator&#34;, d = decorator, all_d = list(decorators.keys()))

    decorator_pattern = re.compile( decorators[decorator] )
    decorator_indices, decorator_names = self.find_assays(col = col, pattern = decorator_pattern, **kwargs )

    # check if decorators were identified
    found_indices = decorator_indices.size &gt; 0
    if not found_indices:
        if not ignore_empty: # if none were found either raise error or ignore
            aw.HardWarning(&#34;Parser:no_decorators_found&#34;)
        else: 
            return

    # if no assay_pattern was specified then default to generic &#34;all&#34; to get full cell contents
    if self.assay_pattern() is None:
        self._WARNING_decorators_but_no_patterns.trigger()
        self.assay_pattern(&#34;all&#34;)

    assay_indices = decorator_indices
    # get assay indices as the cells IMMEDIATELY BELOW the decorators
    # we adjust either col or the row indices depending on the transposition
    if self._transpose:
        col = col + 1
    else:
        assay_indices = assay_indices + 1
  
    # get all assay header cells into an array to extract their names
    array = self._prep_header_array(col, assay_indices)

    # adjust avaliable length of stored assay names
    max_length = max(
                        list(  map(len, array)  )
                    )
    self.max_assay_name_length(max_length)

    names = np.array([&#34;-&#34;*self._max_assay_name_length for _ in range(len(array))]) # we need to pre-specify the max allowed length for the assay names by filling an array with some dummy placeholders (&#39;-&#39;)
    idx = 0
    for entry in array:
        # try:
        match = self.assay_pattern().search(entry)
        if match is not None: 
            name = match.group(1)
            names[idx] = name
        # except: 
        #     continue
        idx += 1

    self._assay_indices = assay_indices
    self._assay_names = names
    
    return assay_indices, names</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.find_columns"><code class="name flex">
<span>def <span class="ident">find_columns</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Identifies the relevant data column belonging to each assay within the datafile.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_columns(self):
    &#34;&#34;&#34;
    Identifies the relevant data column belonging to each assay within the datafile.
    &#34;&#34;&#34;
    # search indices of the starts of id and ct columns
    # these are now the row, col coordinates of each name_column header
    name_col_starts = self._find_column_starts(
                                                label = self._id_label, 
                                                ref_indices = self._assay_indices
                                            )
    # these are now the row, col coordinates of each ct_column header
    ct_col_starts = self._find_column_starts(
                                                label = self._ct_label, 
                                                ref_indices = self._assay_indices
                                            )

    # now we need to generate know also the end indices of the datacolumns
    name_col_ends = self._find_column_ends(name_col_starts)
    
    # now that we know the end indices for the replicate id column we will adopt the end row indices
    # onto the ct column as well (we don&#39;t parse through the Ct column because it might have missing 
    # Ct values intersperced which would prematurely terminate the parsing...)

    # (1) we transpose to have all row indices easily accessible in the first line
    # (2) we adopt row indices from the transposed name col
    # (3) and transpose back to get our final ct end indices
    ct_col_ends = deepcopy( np.transpose(ct_col_starts) )
    name_col_ends_t = np.transpose(name_col_ends)
    ct_col_ends[0] = name_col_ends_t[0]
    ct_col_ends = np.transpose(ct_col_ends)

    # now store the data
    self._assay_names_start_indices = name_col_starts
    self._assay_names_end_indices = name_col_ends
    self._assay_ct_start_indices = ct_col_starts
    self._assay_ct_end_indices = ct_col_ends</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, assay:Â strÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>assay</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of an assay found in the datafile. Available assays can be assessed using the <code>self.assays</code> method.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>dict</code></dt>
<dd>Either a specific pandas dataframe of one of the assays (if an <code>assay</code> name was specified)
or the entire dictionary of all found dataframes from all assays.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self, assay : str = None):
    &#34;&#34;&#34;
    Parameters
    ----------
    assay : str
        The name of an assay found in the datafile. Available assays can be assessed using the `self.assays` method.

    Returns
    -------
    data : pd.DataFrame or dict
        Either a specific pandas dataframe of one of the assays (if an `assay` name was specified)
        or the entire dictionary of all found dataframes from all assays.
    &#34;&#34;&#34;
    if assay is not None:
        data = self._dfs[assay]
    else: 
        data = self._dfs
    return data</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.labels"><code class="name flex">
<span>def <span class="ident">labels</span></span>(<span>self, id_label:Â strÂ =Â 'Name', ct_label:Â strÂ =Â 'Ct')</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the headers for the relevant data columns for each assay within the datafile.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>id_label</code></strong> :&ensp;<code>str</code></dt>
<dd>The header above the column containing replicate identifiers.</dd>
<dt><strong><code>ct_label</code></strong> :&ensp;<code>str</code></dt>
<dd>The header above the column containing the replicates' Ct values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def labels(self, id_label : str = &#34;Name&#34;, ct_label : str = &#34;Ct&#34;):
    &#34;&#34;&#34;
    Sets the headers for the relevant data columns for each assay within the datafile.

    Parameters
    ----------
    id_label : str
        The header above the column containing replicate identifiers. 
    
    ct_label : str
        The header above the column containing the replicates&#39; Ct values.
    &#34;&#34;&#34;
    self._id_label = id_label
    self._ct_label = ct_label</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.make_dataframes"><code class="name flex">
<span>def <span class="ident">make_dataframes</span></span>(<span>self, allow_nan_ct:Â boolÂ =Â True, default_to:Â floatÂ =Â None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates a set of <code>pandas DataFrame</code>s each containing two columns
(one for the replicate identifiers, one for the Ct values)
for subsequent use with the main <code><a title="qpcr" href="../index.html">qpcr</a></code> module.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>allow_nan_Ct</code></strong> :&ensp;<code>bool</code></dt>
<dd>Allows Ct values to be NaN within the final dataframe (if <code>True</code>, default).
If no NaN Ct values should be maintained a default value for NaN Ct values must be specified
using <code>default_to</code>.</dd>
<dt><strong><code>default_to</code></strong> :&ensp;<code>float</code></dt>
<dd>The default value to replace NaN Ct values with.
This is ignored if <code>allow_nan_ct = True</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_dataframes(self, allow_nan_ct : bool = True, default_to : float = None, **kwargs):
    &#34;&#34;&#34;
    Generates a set of `pandas DataFrame`s each containing two columns 
    (one for the replicate identifiers, one for the Ct values)
    for subsequent use with the main `qpcr` module.

    Parameters
    ------
    allow_nan_Ct : bool
        Allows Ct values to be NaN within the final dataframe (if `True`, default).
        If no NaN Ct values should be maintained a default value for NaN Ct values must be specified
        using `default_to`.
    default_to : float
        The default value to replace NaN Ct values with. 
        This is ignored if `allow_nan_ct = True`.
    &#34;&#34;&#34;  

    adx = 0
    # print(self._assay_names, self._assay_indices, self._assay_names_start_indices, self._assay_names_end_indices)
    for assay in self._assay_names:
        
        # get the assay&#39;s indices of both replicate id and ct columns
        names_start = self._assay_names_start_indices[adx]
        names_end = self._assay_names_end_indices[adx]
        ct_start = self._assay_ct_start_indices[adx]
        ct_end = self._assay_ct_end_indices[adx]

        # generate the final index slices from the total array
        # of both replicate id (names) and ct columns
        names_range, names_col = self._make_index_range(names_start, names_end, crop_first = True)
        ct_range, ct_col = self._make_index_range(ct_start, ct_end, crop_first = True)

        # get the assay data
        assay_names = self._data[names_range, names_col]
        assay_cts = self._data[ct_range, ct_col]

        # and convert to numeric data
        try: 
            assay_cts = assay_cts.astype(float)
        except ValueError as e:
            assay_cts = np.genfromtxt(  np.array(assay_cts, dtype=str)  )
            bad_value = e.__str__().split(&#34;: &#34;)[1]
            aw.SoftWarning(&#34;Parser:found_non_readable_cts&#34;, assay = assay, bad_value = bad_value)

        # assemble the assay dataframe 
        assay_df = pd.DataFrame(
                                {
                                    standard_id_header : assay_names, 
                                    standard_ct_header : assay_cts,
                                }
                            )

        if not allow_nan_ct:
            if not isinstance(default_to, (int, float)): 
                aw.HardWarning(&#34;Parser:no_ct_nan_default&#34;, d = default_to)
            
            # apply defaulting lambda function
            assay_df[ standard_ct_header ] = assay_df[ standard_ct_header ].apply(
                                                                                    lambda x: x if x == x else default_to
                                                                                )
        # and store dataframe
        self._dfs.update(
                            { assay : assay_df }
                        )

        adx += 1</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.max_assay_name_length"><code class="name flex">
<span>def <span class="ident">max_assay_name_length</span></span>(<span>self, length=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the maximum allowed name length (number of characters) assay names.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>length</code></strong> :&ensp;<code>int</code></dt>
<dd>The maximum number of characters to store for the assay name.
Default is <code>length = 20</code> characters.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max_assay_name_length(self, length = 20):
    &#34;&#34;&#34;
    Sets the maximum allowed name length (number of characters) assay names.
    
    Parameters
    ----------
    length : int
        The maximum number of characters to store for the assay name. 
        Default is `length = 20` characters.
    &#34;&#34;&#34;
    self._max_assay_name_length = length</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.parse"><code class="name flex">
<span>def <span class="ident">parse</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A wrapper for find_assays+find_columns+make_dataframes
This is the functional core of the Parser's <code>pipe</code> method.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Any additional keyword argument that will be passed to any of the wrapped methods.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse(self, **kwargs):
    &#34;&#34;&#34;
    A wrapper for find_assays+find_columns+make_dataframes
    This is the functional core of the Parser&#39;s `pipe` method.

    Parameters
    -------
    **kwargs
        Any additional keyword argument that will be passed to any of the wrapped methods.
    &#34;&#34;&#34;
    decorator = aux.from_kwargs(&#34;decorator&#34;, None, kwargs, rm = True)
    if decorator is not None:
        self.find_by_decorator(decorator = decorator, **kwargs)
    else: 
        self.find_assays(**kwargs)
    
    # ignore if no assays were found (default is false, unless we use multi-assay multi-sheet files)
    ignore_empty = aux.from_kwargs(&#34;ignore_empty&#34;, False, kwargs)
    if ignore_empty:
        try: 
            self.find_columns()
            self.make_dataframes(**kwargs)
        except: 
            pass
    else: 
        self.find_columns()
        self.make_dataframes(**kwargs)</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.prune"><code class="name flex">
<span>def <span class="ident">prune</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Completely resets the Parser, clearing all data and preset-specifics such as the assay_pattern.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prune(self):
    &#34;&#34;&#34;
    Completely resets the Parser, clearing all data and preset-specifics such as the assay_pattern.
    &#34;&#34;&#34;
    self.__init__()</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves the individual assays as separate csv files.
This requires that a saving directory has been set using <code>self.save_to</code>.
The files will simply be named according to the assay name (i.e. <code>ActinB.csv</code> for instance).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self):
    &#34;&#34;&#34;
    Saves the individual assays as separate csv files.
    This requires that a saving directory has been set using `self.save_to`.
    The files will simply be named according to the assay name (i.e. `ActinB.csv` for instance).
    &#34;&#34;&#34;
    if self._save_loc is None:
        aw.SoftWarning(&#34;Parser:no_save_loc&#34;)
    else:
        for assay, df in self._dfs.items():
            assay_path = os.path.join(self.save_to(), f&#34;{assay}.csv&#34;)
            df.to_csv(assay_path, index = False)</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.save_to"><code class="name flex">
<span>def <span class="ident">save_to</span></span>(<span>self, location:Â strÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the location into which the individual assay datafiles should be saved.
Parameters</p>
<hr>
<dl>
<dt><strong><code>location</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to a directory where the newly generated assay datafiles shall be saved.
If this directory does not yet exist, it will be automatically made.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_to(self, location : str = None):
    &#34;&#34;&#34;
    Sets the location into which the individual assay datafiles should be saved.
    Parameters
    ----------
    location : str
        The path to a directory where the newly generated assay datafiles shall be saved.
        If this directory does not yet exist, it will be automatically made.
    &#34;&#34;&#34;
    if location is not None: 
        self._save_loc = location
        if not os.path.exists(self._save_loc):
            os.mkdir(self._save_loc)
    return self._save_loc</code></pre>
</details>
</dd>
<dt id="qpcr.Parsers.Parsers._CORE_Parser.transpose"><code class="name flex">
<span>def <span class="ident">transpose</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Inverts the <code>col</code> index used by <code>qpcr.Parsers._CORE_Parser.find_assays</code> and <code>qpcr.Parsers._CORE_Parser.find_by_decorator</code>.
By default the <code>col</code> refers to a column. After using <code>transpose</code> it will be interpreted as a <code>row</code>.
Note</p>
<hr>
<p>This is method is dynamic, so repeated calling of <code>transpose</code> will keep reverting the interpretation from row to col, back to row, etc.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transpose(self):
    &#34;&#34;&#34;
    Inverts the `col` index used by `qpcr.Parsers._CORE_Parser.find_assays` and `qpcr.Parsers._CORE_Parser.find_by_decorator`.
    By default the `col` refers to a column. After using `transpose` it will be interpreted as a `row`.
    Note
    ----
    This is method is dynamic, so repeated calling of `transpose` will keep reverting the interpretation from row to col, back to row, etc.
    &#34;&#34;&#34;
    self._transpose = not self._transpose</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">

<a href="https://github.com/NoahHenrikKleinschmidt/qpcr">
<img src="../qpcr_light.svg" width = "100%" >
</a>
<h1>Index</h1>

<div class="toc">
<ul>
<li><a href="#working-with-irregular-files">Working with "irregular files"</a><ul>
<li><a href="#finding-relevant-datasets-through-assay_patterns">"Finding" relevant datasets through assay_patterns</a></li>
</ul>
</li>
<li><a href="#working-with-multi-assay-files">Working with multi-assay files</a><ul>
<li><a href="#getting-all-assays-from-a-multi-assay-file">Getting all assays from a multi-assay file</a><ul>
<li><a href="#making-indivdual-assay-files-from-a-multi-assay-file">Making indivdual assay files from a multi-assay file</a></li>
<li><a href="#using-a-multi-assay-file-directly-for-my-analysis">Using a multi-assay file directly for my analysis</a></li>
<li><a href="#decorators">Decorators</a></li>
<li><a href="#two-things-of-note">Two things of Note:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="qpcr.Parsers" href="index.html">qpcr.Parsers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="qpcr.Parsers.Parsers.CsvParser" href="#qpcr.Parsers.Parsers.CsvParser">CsvParser</a></code></h4>
<ul class="">
<li><code><a title="qpcr.Parsers.Parsers.CsvParser.pipe" href="#qpcr.Parsers.Parsers.CsvParser.pipe">pipe</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers.CsvParser.read" href="#qpcr.Parsers.Parsers.CsvParser.read">read</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="qpcr.Parsers.Parsers.ExcelParser" href="#qpcr.Parsers.Parsers.ExcelParser">ExcelParser</a></code></h4>
<ul class="">
<li><code><a title="qpcr.Parsers.Parsers.ExcelParser.pipe" href="#qpcr.Parsers.Parsers.ExcelParser.pipe">pipe</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers.ExcelParser.read" href="#qpcr.Parsers.Parsers.ExcelParser.read">read</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="qpcr.Parsers.Parsers._CORE_Parser" href="#qpcr.Parsers.Parsers._CORE_Parser">_CORE_Parser</a></code></h4>
<ul class="">
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.assay_pattern" href="#qpcr.Parsers.Parsers._CORE_Parser.assay_pattern">assay_pattern</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.assays" href="#qpcr.Parsers.Parsers._CORE_Parser.assays">assays</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.clear" href="#qpcr.Parsers.Parsers._CORE_Parser.clear">clear</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.find_assays" href="#qpcr.Parsers.Parsers._CORE_Parser.find_assays">find_assays</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.find_by_decorator" href="#qpcr.Parsers.Parsers._CORE_Parser.find_by_decorator">find_by_decorator</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.find_columns" href="#qpcr.Parsers.Parsers._CORE_Parser.find_columns">find_columns</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.get" href="#qpcr.Parsers.Parsers._CORE_Parser.get">get</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.labels" href="#qpcr.Parsers.Parsers._CORE_Parser.labels">labels</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.make_dataframes" href="#qpcr.Parsers.Parsers._CORE_Parser.make_dataframes">make_dataframes</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.max_assay_name_length" href="#qpcr.Parsers.Parsers._CORE_Parser.max_assay_name_length">max_assay_name_length</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.parse" href="#qpcr.Parsers.Parsers._CORE_Parser.parse">parse</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.prune" href="#qpcr.Parsers.Parsers._CORE_Parser.prune">prune</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.save" href="#qpcr.Parsers.Parsers._CORE_Parser.save">save</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.save_to" href="#qpcr.Parsers.Parsers._CORE_Parser.save_to">save_to</a></code></li>
<li><code><a title="qpcr.Parsers.Parsers._CORE_Parser.transpose" href="#qpcr.Parsers.Parsers._CORE_Parser.transpose">transpose</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>